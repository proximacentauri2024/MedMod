{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d36c0e7-ddc6-45d1-bb36-11b2d9715ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import pdb\n",
    "import numpy as np\n",
    "\n",
    "class ImageTextContrastiveLoss(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "    def forward(self,\n",
    "        input_ids=None,\n",
    "        pixel_values=None,\n",
    "        attention_mask=None,\n",
    "        img_labels=None,\n",
    "        text_labels=None,\n",
    "        aug_input_ids=None,\n",
    "        aug_attention_mask=None,\n",
    "        **kwargs,\n",
    "        ):\n",
    "        '''args:\n",
    "        labels: the image corresponds to which classes of diagnoses\n",
    "        text_labels: the text corresponds to which classes of diagnoses\n",
    "        '''\n",
    "        if img_labels is None or text_labels is None:\n",
    "            '''use hard clip loss as the original clip\n",
    "            '''\n",
    "            outputs = self.model(\n",
    "                    input_ids=input_ids,\n",
    "                    pixel_values=pixel_values,\n",
    "                    attention_mask=attention_mask,\n",
    "                    return_loss=True,\n",
    "                    )\n",
    "        else:\n",
    "            '''use soft clip loss\n",
    "            '''\n",
    "            outputs = self.model(\n",
    "                    input_ids=input_ids,\n",
    "                    pixel_values=pixel_values,\n",
    "                    attention_mask=attention_mask,\n",
    "                    return_loss=False,\n",
    "                    )\n",
    "\n",
    "            # get logits\n",
    "            logits = outputs['logits']\n",
    "\n",
    "            # compute soft-labels, -1: negative, 0: uncertain, 1: positive\n",
    "            # in the original data: 1: positive, 0: negative, -1: uncertain, NA: not mentioned\n",
    "            label_sim = torch.matmul(img_labels, text_labels.T)\n",
    "            label_sim = label_sim.to(logits.device)\n",
    "\n",
    "            if aug_input_ids is not None:\n",
    "                aug_text_embeds = self.model.encode_text(aug_input_ids, aug_attention_mask)\n",
    "                img_embeds = outputs['img_embeds']\n",
    "                logits_aug = self.model.compute_logits(img_embeds, aug_text_embeds)\n",
    "                aug_loss_value = self._soft_clip_loss(logits_aug, label_sim)\n",
    "                loss_value = self._soft_clip_loss(logits, label_sim)\n",
    "                outputs['loss_value'] = (aug_loss_value + loss_value) / 2\n",
    "            else:\n",
    "                outputs['loss_value'] = self._soft_clip_loss(logits, label_sim)\n",
    "\n",
    "        return_res = {\n",
    "            'loss_value': outputs['loss_value'],\n",
    "        }\n",
    "        return return_res\n",
    "\n",
    "    def _soft_clip_loss(self, logits_per_img, soft_label):\n",
    "        '''take labels of images and sentences as a softlabel\n",
    "        e.g., image_label = [1, 0, 1, -1], sentence_label = [0, 0, 1, -1]\n",
    "        this pair has similarity as: 1 * 0 + 0 * 0 + 1 * 1 + -1 * -1 = 2.\n",
    "        We will clamp the similarity into [-1,1], and take softmax as a soft-label.\n",
    "        '''\n",
    "        # when using InfoNCE-like loss\n",
    "        image_loss = self._soft_xent_loss(logits_per_img, F.softmax(soft_label,1))\n",
    "        caption_loss = self._soft_xent_loss(logits_per_img.T, F.softmax(soft_label.T,1))\n",
    "        return (image_loss + caption_loss) / 2\n",
    "\n",
    "        # when using multilabel bce loss\n",
    "        # image_loss = self._soft_bce_loss(logits_per_img, soft_label)\n",
    "        # return image_loss\n",
    "\n",
    "    def _soft_xent_loss(self, input, target):\n",
    "        logprobs = torch.nn.functional.log_softmax(input, dim = 1)\n",
    "        return  -(target * logprobs).sum() / input.shape[0]\n",
    "\n",
    "    def _soft_bce_loss(self, input, target):\n",
    "        return nn.functional.binary_cross_entropy_with_logits(input, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4ea494-3d40-48ba-9707-0d4dee703327",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MedCLIPModel(nn.Module):\n",
    "    def __init__(self,\n",
    "        vision_cls=MedCLIPVisionModel,\n",
    "        checkpoint=None,\n",
    "        vision_checkpoint=None,\n",
    "        logit_scale_init_value=0.07,\n",
    "        ) -> None:\n",
    "        super().__init__()\n",
    "        text_proj_bias = False\n",
    "        assert vision_cls in [MedCLIPVisionModel, MedCLIPVisionModelViT], 'vision_cls should be one of [MedCLIPVisionModel, MedCLIPVisionModelViT]'\n",
    "\n",
    "        self.vision_model = vision_cls(checkpoint=vision_checkpoint)\n",
    "        self.text_model = MedCLIPTextModel(proj_bias=False)\n",
    "\n",
    "        # learnable temperature for contrastive loss\n",
    "        self.logit_scale = nn.Parameter(torch.log(torch.tensor(1/logit_scale_init_value)))\n",
    "\n",
    "        if checkpoint is not None:\n",
    "            state_dict = torch.load(os.path.join(checkpoint, constants.WEIGHTS_NAME))\n",
    "            self.load_state_dict(state_dict)\n",
    "            print('load model weight from:', checkpoint)\n",
    "\n",
    "    def from_pretrained(self, input_dir=None):\n",
    "        '''\n",
    "        If input_dir is None, download pretrained weight from google cloud and load.\n",
    "        '''\n",
    "        import wget\n",
    "        import zipfile\n",
    "        pretrained_url = None\n",
    "        if isinstance(self.vision_model, MedCLIPVisionModel):\n",
    "            # resnet\n",
    "            pretrained_url = constants.PRETRAINED_URL_MEDCLIP_RESNET\n",
    "            if input_dir is None:\n",
    "                input_dir = './pretrained/medclip-resnet'\n",
    "        elif isinstance(self.vision_model, MedCLIPVisionModelViT):\n",
    "            # ViT\n",
    "            pretrained_url = constants.PRETRAINED_URL_MEDCLIP_VIT\n",
    "            if input_dir is None:\n",
    "                input_dir = './pretrained/medclip-vit'\n",
    "        else:\n",
    "            raise ValueError(f'We only have pretrained weight for MedCLIP-ViT or MedCLIP-ResNet, get {type(self.vision_model)} instead.')\n",
    "\n",
    "        if not os.path.exists(input_dir):\n",
    "            os.makedirs(input_dir)\n",
    "\n",
    "            # download url link\n",
    "            pretrained_url = requests.get(pretrained_url).text\n",
    "            filename = wget.download(pretrained_url, input_dir)\n",
    "\n",
    "            # unzip\n",
    "            zipf = zipfile.ZipFile(filename)\n",
    "            zipf.extractall(input_dir)\n",
    "            zipf.close()\n",
    "            print('\\n Download pretrained model from:', pretrained_url)\n",
    "        \n",
    "        state_dict = torch.load(os.path.join(input_dir, constants.WEIGHTS_NAME))\n",
    "        self.load_state_dict(state_dict)\n",
    "        print('load model weight from:', input_dir)\n",
    "\n",
    "    def encode_text(self, input_ids=None, attention_mask=None):\n",
    "        input_ids = input_ids.cuda()\n",
    "        if attention_mask is not None:\n",
    "            attention_mask = attention_mask.cuda()\n",
    "        text_embeds = self.text_model(input_ids, attention_mask)\n",
    "        text_embeds = text_embeds / text_embeds.norm(dim=-1, keepdim=True)\n",
    "        return text_embeds\n",
    "\n",
    "    def encode_image(self, pixel_values=None):\n",
    "        # image encoder\n",
    "        vision_output = self.vision_model(pixel_values=pixel_values)\n",
    "        img_embeds = vision_output / vision_output.norm(dim=-1, keepdim=True)\n",
    "        return img_embeds\n",
    "    def forward(self,\n",
    "            input_ids=None,\n",
    "            pixel_values=None,\n",
    "            attention_mask=None,\n",
    "            return_loss=None,\n",
    "            **kwargs,\n",
    "            ):\n",
    "            input_ids = input_ids.cuda()\n",
    "            if attention_mask is not None:\n",
    "                attention_mask = attention_mask.cuda()\n",
    "            pixel_values = pixel_values.cuda()\n",
    "    \n",
    "            img_embeds = self.encode_image(pixel_values)\n",
    "            text_embeds = self.encode_text(input_ids, attention_mask)\n",
    "    \n",
    "            logits_per_image = self.compute_logits(img_embeds, text_embeds)\n",
    "            logits_per_text = logits_per_image.t()\n",
    "    \n",
    "            if return_loss:\n",
    "                loss = self.clip_loss(logits_per_text)\n",
    "            else:\n",
    "                loss = None\n",
    "    \n",
    "            return {'img_embeds':img_embeds, 'text_embeds':text_embeds,\n",
    "                'logits':logits_per_image, 'loss_value':loss, 'logits_per_text':logits_per_text}\n",
    "\n",
    "    def compute_logits(self, img_emb, text_emb):\n",
    "        self.logit_scale.data = torch.clamp(self.logit_scale.data, 0, 4.6052)\n",
    "        logit_scale = self.logit_scale.exp()\n",
    "        logits_per_text = torch.matmul(text_emb, img_emb.t()) * logit_scale\n",
    "        return logits_per_text.t()\n",
    "\n",
    "    def clip_loss(self, similarity: torch.Tensor) -> torch.Tensor:\n",
    "        caption_loss = self.contrastive_loss(similarity)\n",
    "        image_loss = self.contrastive_loss(similarity.T)\n",
    "        return (caption_loss + image_loss) / 2.0\n",
    "\n",
    "    def contrastive_loss(self, logits: torch.Tensor) -> torch.Tensor:\n",
    "        return nn.functional.cross_entropy(logits, torch.arange(len(logits), device=logits.device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0bea825-ed28-46c3-877a-369b563cee42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample phenotyping label \n",
    "# stay is 14851532_episode3_timeseries.csv\n",
    "# stay_id is 30681041\n",
    "# image name is 721e19bf-893cd83c-ea610180-ee56a931-b0b7c146\n",
    "# study_id is 59956491\n",
    "# phenotyping classes\n",
    "# pheno classes: Acute and unspecified renal failure,Acute cerebrovascular disease,Acute myocardial infarction,Cardiac dysrhythmias,Chronic kidney disease,Chronic obstructive pulmonary disease and bronchiectasis,Complications of surgical procedures or medical care,Conduction disorders,Congestive heart failure; nonhypertensive,Coronary atherosclerosis and other heart disease,Diabetes mellitus with complications,Diabetes mellitus without complication,Disorders of lipid metabolism,Essential hypertension,Fluid and electrolyte disorders,Gastrointestinal hemorrhage,Hypertension with complications and secondary hypertension,Other liver diseases,Other lower respiratory disease,Other upper respiratory disease,Pleurisy; pneumothorax; pulmonary collapse,Pneumonia (except that caused by tuberculosis or sexually transmitted disease),Respiratory failure; insufficiency; arrest (adult),Septicemia (except in labor),Shock\n",
    "# image classes: Atelectasis,Cardiomegaly,Consolidation,Edema,Enlarged Cardiomediastinum,Fracture,Lung Lesion,Lung Opacity,No Finding,Pleural Effusion,Pleural Other,Pneumonia,Pneumothorax,Support Devices\n",
    "pheno_label = [0,0,0,0,0,1,1,1,0,0,0,1,1,1,0,0,0,0,0,0,0,0,0,1,0] # 25 classes\n",
    "img_label = [1.0,1.0,0,0,0,0,0,1.0,0,-1.0,0,0,0,1.0] # 14 classes \n",
    "\n",
    "# To get the soft targets \n",
    "def _soft_clip_loss(self, logits_per_img, soft_label):\n",
    "    '''take labels of images and sentences as a softlabel\n",
    "    e.g., image_label = [1, 0, 1, -1], sentence_label = [0, 0, 1, -1]\n",
    "    this pair has similarity as: 1 * 0 + 0 * 0 + 1 * 1 + -1 * -1 = 2.\n",
    "    We will clamp the similarity into [-1,1], and take softmax as a soft-label.\n",
    "    '''\n",
    "    # when using InfoNCE-like loss\n",
    "    image_loss = self._soft_xent_loss(logits_per_img, F.softmax(soft_label,1))\n",
    "    caption_loss = self._soft_xent_loss(logits_per_img.T, F.softmax(soft_label.T,1))\n",
    "    return (image_loss + caption_loss) / 2\n",
    "\n",
    "    # when using multilabel bce loss\n",
    "    # image_loss = self._soft_bce_loss(logits_per_img, soft_label)\n",
    "    # return image_loss\n",
    "\n",
    "def _soft_xent_loss(self, input, target):\n",
    "    logprobs = torch.nn.functional.log_softmax(input, dim = 1)\n",
    "    return  -(target * logprobs).sum() / input.shape[0]\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MML SSL",
   "language": "python",
   "name": "mml-ssl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
