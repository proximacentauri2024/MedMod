{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dbf50228-dcaf-4d95-93a2-83565973d11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from arguments import args_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bbc5d2aa-6bed-4502-95d7-e216a6eac79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import platform\n",
    "import pickle\n",
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "class Discretizer:\n",
    "    def __init__(self, timestep=0.8, store_masks=True, impute_strategy='zero', start_time='zero',\n",
    "                 config_path= 'ehr_utils/resources/discretizer_config.json'):\n",
    "\n",
    "        with open(config_path) as f:\n",
    "            config = json.load(f)\n",
    "            self._id_to_channel = config['id_to_channel']\n",
    "            self._channel_to_id = dict(zip(self._id_to_channel, range(len(self._id_to_channel))))\n",
    "            self._is_categorical_channel = config['is_categorical_channel']\n",
    "            self._possible_values = config['possible_values']\n",
    "            self._normal_values = config['normal_values']\n",
    "\n",
    "        self._header = [\"Hours\"] + self._id_to_channel\n",
    "        self._timestep = timestep\n",
    "        self._store_masks = store_masks\n",
    "        self._start_time = start_time\n",
    "        self._impute_strategy = impute_strategy\n",
    "\n",
    "        # for statistics\n",
    "        self._done_count = 0\n",
    "        self._empty_bins_sum = 0\n",
    "        self._unused_data_sum = 0\n",
    "\n",
    "    def transform(self, X, header=None, end=None):\n",
    "        if header is None:\n",
    "            header = self._header\n",
    "        assert header[0] == \"Hours\"\n",
    "        eps = 1e-6\n",
    "\n",
    "        N_channels = len(self._id_to_channel)\n",
    "        ts = [float(row[0]) for row in X]\n",
    "        for i in range(len(ts) - 1):\n",
    "            assert ts[i] < ts[i+1] + eps\n",
    "\n",
    "        if self._start_time == 'relative':\n",
    "            first_time = ts[0]\n",
    "        elif self._start_time == 'zero':\n",
    "            first_time = 0\n",
    "        else:\n",
    "            raise ValueError(\"start_time is invalid\")\n",
    "\n",
    "        if end is None:\n",
    "            max_hours = max(ts) - first_time\n",
    "        else:\n",
    "            max_hours = end - first_time\n",
    "\n",
    "        N_bins = int(max_hours / self._timestep + 1.0 - eps)\n",
    "\n",
    "        cur_len = 0\n",
    "        begin_pos = [0 for i in range(N_channels)]\n",
    "        end_pos = [0 for i in range(N_channels)]\n",
    "        for i in range(N_channels):\n",
    "            channel = self._id_to_channel[i]\n",
    "            begin_pos[i] = cur_len\n",
    "            if self._is_categorical_channel[channel]:\n",
    "                end_pos[i] = begin_pos[i] + len(self._possible_values[channel])\n",
    "            else:\n",
    "                end_pos[i] = begin_pos[i] + 1\n",
    "            cur_len = end_pos[i]\n",
    "\n",
    "        data = np.zeros(shape=(N_bins, cur_len), dtype=float)\n",
    "        mask = np.zeros(shape=(N_bins, N_channels), dtype=int)\n",
    "        original_value = [[\"\" for j in range(N_channels)] for i in range(N_bins)]\n",
    "        total_data = 0\n",
    "        unused_data = 0\n",
    "\n",
    "        def write(data, bin_id, channel, value, begin_pos):\n",
    "            channel_id = self._channel_to_id[channel]\n",
    "            if self._is_categorical_channel[channel]:\n",
    "                category_id = self._possible_values[channel].index(value)\n",
    "                N_values = len(self._possible_values[channel])\n",
    "                one_hot = np.zeros((N_values,))\n",
    "                one_hot[category_id] = 1\n",
    "                for pos in range(N_values):\n",
    "                    data[bin_id, begin_pos[channel_id] + pos] = one_hot[pos]\n",
    "            else:\n",
    "                data[bin_id, begin_pos[channel_id]] = float(value)\n",
    "\n",
    "        for row in X:\n",
    "            t = float(row[0]) - first_time\n",
    "            if t > max_hours + eps:\n",
    "                continue\n",
    "            bin_id = int(t / self._timestep - eps)\n",
    "            assert 0 <= bin_id < N_bins\n",
    "\n",
    "            for j in range(1, len(row)):\n",
    "                if row[j] == \"\":\n",
    "                    continue\n",
    "                channel = header[j]\n",
    "                channel_id = self._channel_to_id[channel]\n",
    "\n",
    "                total_data += 1\n",
    "                if mask[bin_id][channel_id] == 1:\n",
    "                    unused_data += 1\n",
    "                mask[bin_id][channel_id] = 1\n",
    "\n",
    "                write(data, bin_id, channel, row[j], begin_pos)\n",
    "                original_value[bin_id][channel_id] = row[j]\n",
    "\n",
    "        # impute missing values\n",
    "\n",
    "        if self._impute_strategy not in ['zero', 'normal_value', 'previous', 'next']:\n",
    "            raise ValueError(\"impute strategy is invalid\")\n",
    "\n",
    "        if self._impute_strategy in ['normal_value', 'previous']:\n",
    "            prev_values = [[] for i in range(len(self._id_to_channel))]\n",
    "            for bin_id in range(N_bins):\n",
    "                for channel in self._id_to_channel:\n",
    "                    channel_id = self._channel_to_id[channel]\n",
    "                    if mask[bin_id][channel_id] == 1:\n",
    "                        prev_values[channel_id].append(original_value[bin_id][channel_id])\n",
    "                        continue\n",
    "                    if self._impute_strategy == 'normal_value':\n",
    "                        imputed_value = self._normal_values[channel]\n",
    "                    if self._impute_strategy == 'previous':\n",
    "                        if len(prev_values[channel_id]) == 0:\n",
    "                            imputed_value = self._normal_values[channel]\n",
    "                        else:\n",
    "                            imputed_value = prev_values[channel_id][-1]\n",
    "                    write(data, bin_id, channel, imputed_value, begin_pos)\n",
    "\n",
    "        if self._impute_strategy == 'next':\n",
    "            prev_values = [[] for i in range(len(self._id_to_channel))]\n",
    "            for bin_id in range(N_bins-1, -1, -1):\n",
    "                for channel in self._id_to_channel:\n",
    "                    channel_id = self._channel_to_id[channel]\n",
    "                    if mask[bin_id][channel_id] == 1:\n",
    "                        prev_values[channel_id].append(original_value[bin_id][channel_id])\n",
    "                        continue\n",
    "                    if len(prev_values[channel_id]) == 0:\n",
    "                        imputed_value = self._normal_values[channel]\n",
    "                    else:\n",
    "                        imputed_value = prev_values[channel_id][-1]\n",
    "                    write(data, bin_id, channel, imputed_value, begin_pos)\n",
    "\n",
    "        empty_bins = np.sum([1 - min(1, np.sum(mask[i, :])) for i in range(N_bins)])\n",
    "        self._done_count += 1\n",
    "        self._empty_bins_sum += empty_bins / (N_bins + eps)\n",
    "        self._unused_data_sum += unused_data / (total_data + eps)\n",
    "\n",
    "        if self._store_masks:\n",
    "            data = np.hstack([data, mask.astype(np.float32)])\n",
    "\n",
    "        # create new header\n",
    "        new_header = []\n",
    "        for channel in self._id_to_channel:\n",
    "            if self._is_categorical_channel[channel]:\n",
    "                values = self._possible_values[channel]\n",
    "                for value in values:\n",
    "                    new_header.append(channel + \"->\" + value)\n",
    "            else:\n",
    "                new_header.append(channel)\n",
    "\n",
    "        if self._store_masks:\n",
    "            for i in range(len(self._id_to_channel)):\n",
    "                channel = self._id_to_channel[i]\n",
    "                new_header.append(\"mask->\" + channel)\n",
    "\n",
    "        new_header = \",\".join(new_header)\n",
    "\n",
    "        return (data, new_header)\n",
    "\n",
    "    def print_statistics(self):\n",
    "        print(\"statistics of discretizer:\")\n",
    "        print(\"\\tconverted {} examples\".format(self._done_count))\n",
    "        print(\"\\taverage unused data = {:.2f} percent\".format(100.0 * self._unused_data_sum / self._done_count))\n",
    "        print(\"\\taverage empty  bins = {:.2f} percent\".format(100.0 * self._empty_bins_sum / self._done_count))\n",
    "\n",
    "\n",
    "class Normalizer:\n",
    "    def __init__(self, fields=None):\n",
    "        self._means = None\n",
    "        self._stds = None\n",
    "        self._fields = None\n",
    "        if fields is not None:\n",
    "            self._fields = [col for col in fields]\n",
    "\n",
    "        self._sum_x = None\n",
    "        self._sum_sq_x = None\n",
    "        self._count = 0\n",
    "\n",
    "    def _feed_data(self, x):\n",
    "        x = np.array(x)\n",
    "        self._count += x.shape[0]\n",
    "        if self._sum_x is None:\n",
    "            self._sum_x = np.sum(x, axis=0)\n",
    "            self._sum_sq_x = np.sum(x**2, axis=0)\n",
    "        else:\n",
    "            self._sum_x += np.sum(x, axis=0)\n",
    "            self._sum_sq_x += np.sum(x**2, axis=0)\n",
    "\n",
    "    def _save_params(self, save_file_path):\n",
    "        eps = 1e-7\n",
    "        with open(save_file_path, \"wb\") as save_file:\n",
    "            N = self._count\n",
    "            self._means = 1.0 / N * self._sum_x\n",
    "            self._stds = np.sqrt(1.0/(N - 1) * (self._sum_sq_x - 2.0 * self._sum_x * self._means + N * self._means**2))\n",
    "            self._stds[self._stds < eps] = eps\n",
    "            pickle.dump(obj={'means': self._means,\n",
    "                             'stds': self._stds},\n",
    "                        file=save_file,\n",
    "                        protocol=2)\n",
    "\n",
    "    def load_params(self, load_file_path):\n",
    "        with open(load_file_path, \"rb\") as load_file:\n",
    "            if platform.python_version()[0] == '2':\n",
    "                dct = pickle.load(load_file)\n",
    "            else:\n",
    "                dct = pickle.load(load_file, encoding='latin1')\n",
    "            self._means = dct['means']\n",
    "            self._stds = dct['stds']\n",
    "\n",
    "    def transform(self, X):\n",
    "        if self._fields is None:\n",
    "            fields = range(X.shape[1])\n",
    "        else:\n",
    "            fields = self._fields\n",
    "        ret = 1.0 * X\n",
    "        for col in fields:\n",
    "            ret[:, col] = (X[:, col] - self._means[col]) / self._stds[col]\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "87b6d573-aea1-4613-8c4c-5a5681a8b477",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pandas as pd \n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import glob\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class EHRdataset(Dataset):\n",
    "    def __init__(self, discretizer, normalizer, listfile, dataset_dir, return_names=True, period_length=48.0):\n",
    "        print(\"init EHR\")\n",
    "        self.return_names = return_names\n",
    "        self.discretizer = discretizer\n",
    "        self.normalizer = normalizer\n",
    "        self._period_length = period_length\n",
    "        self._dataset_dir = dataset_dir\n",
    "        listfile_path = listfile\n",
    "        with open(listfile_path, \"r\") as lfile:\n",
    "            self._data = lfile.readlines()\n",
    "        self._listfile_header = self._data[0]\n",
    "        self.CLASSES = self._listfile_header.strip().split(',')[3:]\n",
    "        self._data = self._data[1:]\n",
    "        # print(self._data[12])\n",
    "        self._data = [line.split(',') for line in self._data]\n",
    "        if 'length-of-stay' or 'decompensation' in self._dataset_dir:\n",
    "            self.data_map = [(x, float(t), int(stay_id) ,[float(y)]) for (x, t, stay_id , y) in self._data]\n",
    "            self.names = [x[0] for x in self.data_map]\n",
    "            self.times= [x[1] for x in self.data_map]\n",
    "        else:\n",
    "            self.data_map = {\n",
    "            mas[0]: {\n",
    "                'labels': list(map(float, mas[3:])),\n",
    "                'stay_id': float(mas[2]),\n",
    "                'time': float(mas[1]),\n",
    "                }\n",
    "                for mas in self._data\n",
    "        }\n",
    "            self.names = list(self.data_map.keys())\n",
    "            self.times= None\n",
    "        print(\"times in EHR dataset\", self.times[0:10])\n",
    "        # print(self.data_map[12])\n",
    "        # import pdb; pdb.set_trace()\n",
    "        # self._data = [(line_[0], float(line_[1]), line_[2], float(line_[3])  ) for line_ in self._data]\n",
    "        # self.names = [x[0] for x in self.data_map]\n",
    "        # print(self.names[0:10])\n",
    "    def _read_timeseries(self, ts_filename, time_bound=None):\n",
    "        ret = []\n",
    "        with open(os.path.join(self._dataset_dir, ts_filename), \"r\") as tsfile:\n",
    "            header = tsfile.readline().strip().split(',')\n",
    "            assert header[0] == \"Hours\"\n",
    "            for line in tsfile:\n",
    "                mas = line.strip().split(',')\n",
    "                if time_bound is not None:\n",
    "                    t = float(mas[0])\n",
    "                    if t > time_bound + 1e-6:\n",
    "                        break\n",
    "                ret.append(np.array(mas))\n",
    "        return (np.stack(ret), header)\n",
    "        \n",
    "    def read_by_file_name(self, index, time, time_bound=None):\n",
    "        # print(\"index\", index)\n",
    "        if 'length-of-stay' or 'decompensation' in self._dataset_dir:\n",
    "            entry = next((x for x in self.data_map if x[0] == index and x[1] ==  time), None)\n",
    "            # print(\"entry\", entry)\n",
    "            if entry is None:\n",
    "                raise ValueError(f\"Entry with name {index} not found\")\n",
    "            t = float(entry[1])  # time is the second element in the tuple\n",
    "            stay_id = int(entry[2])  # stay_id is the third element\n",
    "            y = entry[3]  # labels are the fourth element\n",
    "            # print(\"this is entry 3\", y)\n",
    "            (X, header) = self._read_timeseries(index, time_bound=time_bound if time_bound is not None else t)\n",
    "        else:\n",
    "            t = self.data_map[index]['time'] if time_bound is None else time_bound\n",
    "            y = self.data_map[index]['labels']\n",
    "            stay_id = self.data_map[index]['stay_id']\n",
    "            (X, header) = self._read_timeseries(index, time_bound=time_bound)\n",
    "        return {\"X\": X,\n",
    "                \"t\": t,\n",
    "                \"y\": y,\n",
    "                'stay_id': stay_id,\n",
    "                \"header\": header,\n",
    "                \"name\": index}\n",
    "    def get_decomp_los(self, index, time_bound=None):\n",
    "        # name = self._data[index][0]\n",
    "        # time_bound = self._data[index][1]\n",
    "        # ys = self._data[index][3]\n",
    "        # (data, header) = self._read_timeseries(index, time_bound=time_bound)\n",
    "        # data = self.discretizer.transform(data, end=time_bound)[0]\n",
    "        # if (self.normalizer is not None):\n",
    "        #     data = self.normalizer.transform(data)\n",
    "        # ys = np.array(ys, dtype=np.int32) if len(ys) > 1 else np.array(ys, dtype=np.int32)[0]\n",
    "        # return data, ys\n",
    "        # data, ys =\n",
    "        return self.__getitem__(index, time_bound)\n",
    "    def __getitem__(self, tuplee,  time_bound=None):\n",
    "        if 'length-of-stay' or 'decompensation' in self._dataset_dir:\n",
    "            time = tuplee[1]\n",
    "            index = tuplee[0]\n",
    "            # print(\"time:\", time)\n",
    "            # print(\"index:\",index)\n",
    "        else:\n",
    "            if isinstance(index, int):\n",
    "                index = self.names[index]\n",
    "                time = None\n",
    "        ret = self.read_by_file_name(index, time, time_bound)\n",
    "        data = ret[\"X\"]\n",
    "        ts = ret[\"t\"] if ret['t'] > 0.0 else self._period_length\n",
    "        ys = ret[\"y\"]\n",
    "        # print(\"this is ys\" , ys)\n",
    "        names = ret[\"name\"]\n",
    "        data = self.discretizer.transform(data, end=ts)[0]\n",
    "        if (self.normalizer is not None):\n",
    "            data = self.normalizer.transform(data)\n",
    "        if 'length-of-stay' in self._dataset_dir:\n",
    "            ys = np.array(ys, dtype=np.float32) if len(ys) > 1 else np.array(ys, dtype=np.float32)[0]\n",
    "        else:\n",
    "            ys = np.array(ys, dtype=np.int32) if len(ys) > 1 else np.array(ys, dtype=np.int32)[0]\n",
    "        return data, ys\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.names)\n",
    "    def get_datasets(discretizer, normalizer, args):\n",
    "        train_ds = EHRdataset(discretizer, normalizer, f'{args.ehr_data_dir}/{args.task}/train_listfile.csv', os.path.join(args.ehr_data_dir, f'{args.task}/train'))\n",
    "        val_ds = EHRdataset(discretizer, normalizer, f'{args.ehr_data_dir}/{args.task}/val_listfile.csv', os.path.join(args.ehr_data_dir, f'{args.task}/train'))\n",
    "        test_ds = EHRdataset(discretizer, normalizer, f'{args.ehr_data_dir}/{args.task}/test_listfile.csv', os.path.join(args.ehr_data_dir, f'{args.task}/test'))\n",
    "        return train_ds, val_ds, test_ds\n",
    "        \n",
    "    def get_data_loader(discretizer, normalizer, dataset_dir, batch_size):\n",
    "        train_ds, val_ds, test_ds = get_datasets(discretizer, normalizer, dataset_dir)\n",
    "        train_dl = DataLoader(train_ds, batch_size, shuffle=True, collate_fn=my_collate, pin_memory=True, num_workers=16)\n",
    "        val_dl = DataLoader(val_ds, batch_size, shuffle=False, collate_fn=my_collate, pin_memory=True, num_workers=16)\n",
    "        return train_dl, val_dl\n",
    "    def my_collate(batch):\n",
    "        x = [item[0] for item in batch]\n",
    "        x, seq_length = pad_zeros(x)\n",
    "        targets = np.array([item[1] for item in batch])\n",
    "        return [x, targets, seq_length]\n",
    "    def pad_zeros(arr, min_length=None):\n",
    "        dtype = arr[0].dtype\n",
    "        seq_length = [x.shape[0] for x in arr]\n",
    "        max_len = max(seq_length)\n",
    "        ret = [np.concatenate([x, np.zeros((max_len - x.shape[0],) + x.shape[1:], dtype=dtype)], axis=0)\n",
    "               for x in arr]\n",
    "        if (min_length is not None) and ret[0].shape[0] < min_length:\n",
    "            ret = [np.concatenate([x, np.zeros((min_length - x.shape[0],) + x.shape[1:], dtype=dtype)], axis=0)\n",
    "                   for x in ret]\n",
    "        return np.array(ret), seq_length\n",
    "\n",
    "\n",
    "def get_datasets(discretizer, normalizer, args):\n",
    "    train_ds = EHRdataset(discretizer, normalizer, f'{args.ehr_data_dir}/{args.task}/train_listfile.csv', os.path.join(args.ehr_data_dir, f'{args.task}/train'))\n",
    "    val_ds = EHRdataset(discretizer, normalizer, f'{args.ehr_data_dir}/{args.task}/val_listfile.csv', os.path.join(args.ehr_data_dir, f'{args.task}/train'))\n",
    "    test_ds = EHRdataset(discretizer, normalizer, f'{args.ehr_data_dir}/{args.task}/test_listfile.csv', os.path.join(args.ehr_data_dir, f'{args.task}/test'))\n",
    "    return train_ds, val_ds, test_ds\n",
    "\n",
    "def get_data_loader(discretizer, normalizer, dataset_dir, batch_size):\n",
    "    train_ds, val_ds, test_ds = get_datasets(discretizer, normalizer, dataset_dir)\n",
    "    train_dl = DataLoader(train_ds, batch_size, shuffle=True, collate_fn=my_collate, pin_memory=True, num_workers=16)\n",
    "    val_dl = DataLoader(val_ds, batch_size, shuffle=False, collate_fn=my_collate, pin_memory=True, num_workers=16)\n",
    "\n",
    "    return train_dl, val_dl\n",
    "        \n",
    "def my_collate(batch):\n",
    "    x = [item[0] for item in batch]\n",
    "    x, seq_length = pad_zeros(x)\n",
    "    targets = np.array([item[1] for item in batch])\n",
    "    return [x, targets, seq_length]\n",
    "\n",
    "def pad_zeros(arr, min_length=None):\n",
    "\n",
    "    dtype = arr[0].dtype\n",
    "    seq_length = [x.shape[0] for x in arr]\n",
    "    max_len = max(seq_length)\n",
    "    ret = [np.concatenate([x, np.zeros((max_len - x.shape[0],) + x.shape[1:], dtype=dtype)], axis=0)\n",
    "           for x in arr]\n",
    "    if (min_length is not None) and ret[0].shape[0] < min_length:\n",
    "        ret = [np.concatenate([x, np.zeros((min_length - x.shape[0],) + x.shape[1:], dtype=dtype)], axis=0)\n",
    "               for x in ret]\n",
    "    return np.array(ret), seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bc2fbfe6-fc30-4d04-b5e1-4652ab41538f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pandas as pd \n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "# import \n",
    "import glob\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "\n",
    "R_CLASSES  = ['Atelectasis', 'Cardiomegaly', 'Consolidation', 'Edema',\n",
    "       'Enlarged Cardiomediastinum', 'Fracture', 'Lung Lesion',\n",
    "       'Lung Opacity', 'No Finding', 'Pleural Effusion', 'Pleural Other',\n",
    "       'Pneumonia', 'Pneumothorax', 'Support Devices']\n",
    "\n",
    "CLASSES = [\n",
    "       'Acute and unspecified renal failure', 'Acute cerebrovascular disease',\n",
    "       'Acute myocardial infarction', 'Cardiac dysrhythmias',\n",
    "       'Chronic kidney disease',\n",
    "       'Chronic obstructive pulmonary disease and bronchiectasis',\n",
    "       'Complications of surgical procedures or medical care',\n",
    "       'Conduction disorders', 'Congestive heart failure; nonhypertensive',\n",
    "       'Coronary atherosclerosis and other heart disease',\n",
    "       'Diabetes mellitus with complications',\n",
    "       'Diabetes mellitus without complication',\n",
    "       'Disorders of lipid metabolism', 'Essential hypertension',\n",
    "       'Fluid and electrolyte disorders', 'Gastrointestinal hemorrhage',\n",
    "       'Hypertension with complications and secondary hypertension',\n",
    "       'Other liver diseases', 'Other lower respiratory disease',\n",
    "       'Other upper respiratory disease',\n",
    "       'Pleurisy; pneumothorax; pulmonary collapse',\n",
    "       'Pneumonia (except that caused by tuberculosis or sexually transmitted disease)',\n",
    "       'Respiratory failure; insufficiency; arrest (adult)',\n",
    "       'Septicemia (except in labor)', 'Shock'\n",
    "    ]\n",
    "\n",
    "class MIMIC_CXR_EHR(Dataset):\n",
    "    def __init__(self, args, metadata_with_labels, ehr_ds, cxr_ds, split='train'):\n",
    "        \n",
    "        self.CLASSES = CLASSES\n",
    "        if 'radiology' in args.labels_set:\n",
    "            self.CLASSES = R_CLASSES\n",
    "        \n",
    "        self.metadata_with_labels = metadata_with_labels\n",
    "        self.cxr_files_paired = self.metadata_with_labels.dicom_id.values\n",
    "        self.ehr_files_paired = (self.metadata_with_labels['stay'].values)\n",
    "        self.cxr_files_all = cxr_ds.filenames_loaded\n",
    "        self.ehr_files_all = ehr_ds.names\n",
    "        self.ehr_times = ehr_ds.times\n",
    "        self.ehr_files_unpaired = list(set(self.ehr_files_all) - set(self.ehr_files_paired))\n",
    "        self.ehr_ds = ehr_ds\n",
    "        self.cxr_ds = cxr_ds\n",
    "        self.args = args\n",
    "        self.split = split\n",
    "        self.data_ratio = self.args.data_ratio \n",
    "        if split=='test':\n",
    "            self.data_ratio =  1.0\n",
    "        elif split == 'val':\n",
    "            self.data_ratio =  0.0\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        time = None\n",
    "        if self.ehr_times is not None:\n",
    "            time = self.ehr_times[index]\n",
    "        if self.args.data_pairs == 'paired_ehr_cxr':\n",
    "            ehr_data, labels_ehr = self.ehr_ds[(self.ehr_files_paired[index],time)]\n",
    "            cxr_data, labels_cxr = self.cxr_ds[self.cxr_files_paired[index]]\n",
    "            return ehr_data, cxr_data, labels_ehr, labels_cxr\n",
    "        elif self.args.data_pairs == 'paired_ehr':\n",
    "            ehr_data, labels_ehr = self.ehr_ds[(self.ehr_files_paired[index],time)]\n",
    "            cxr_data, labels_cxr = None, None\n",
    "            return ehr_data, cxr_data, labels_ehr, labels_cxr\n",
    "        elif self.args.data_pairs == 'radiology':\n",
    "            ehr_data, labels_ehr = np.zeros((1, 10)), np.zeros(self.args.num_classes)\n",
    "            cxr_data, labels_cxr = self.cxr_ds[self.cxr_files_all[index]]\n",
    "            return ehr_data, cxr_data, labels_ehr, labels_cxr\n",
    "        elif self.args.data_pairs == 'partial_ehr':\n",
    "            # print(\"I am passing this:\", self.ehr_files_all[index])\n",
    "            ehr_data, labels_ehr = self.ehr_ds[(self.ehr_files_all[index],time)]\n",
    "            # print(\"my labels:\",labels_ehr)\n",
    "            cxr_data, labels_cxr = None, None\n",
    "            return ehr_data, cxr_data, labels_ehr, labels_cxr\n",
    "        \n",
    "        elif self.args.data_pairs == 'partial_ehr_cxr':\n",
    "            if index < len(self.ehr_files_paired):\n",
    "                ehr_data, labels_ehr = self.ehr_ds[(self.ehr_files_paired[index],time)]\n",
    "                cxr_data, labels_cxr = self.cxr_ds[self.cxr_files_paired[index]]\n",
    "            else:\n",
    "                index = random.randint(0, len(self.ehr_files_unpaired)-1) \n",
    "                ehr_data, labels_ehr = self.ehr_ds[(self.ehr_files_unpaired[index],time)]\n",
    "                cxr_data, labels_cxr = None, None\n",
    "            return ehr_data, cxr_data, labels_ehr, labels_cxr\n",
    "\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        if 'paired' in self.args.data_pairs:\n",
    "            return len(self.ehr_files_paired)\n",
    "        elif self.args.data_pairs == 'partial_ehr':\n",
    "            return len(self.ehr_files_all)\n",
    "        elif self.args.data_pairs == 'radiology':\n",
    "            return len(self.cxr_files_all)\n",
    "        elif self.args.data_pairs == 'partial_ehr_cxr':\n",
    "            return len(self.ehr_files_paired) + int(self.data_ratio * len(self.ehr_files_unpaired)) \n",
    "        \n",
    "\n",
    "\n",
    "def loadmetadata(args):\n",
    "\n",
    "    data_dir = args.cxr_data_dir\n",
    "    cxr_metadata = pd.read_csv(f'{data_dir}/mimic-cxr-2.0.0-metadata.csv')\n",
    "    icu_stay_metadata = pd.read_csv(f'{args.ehr_data_dir}/root/all_stays.csv')\n",
    "    columns = ['subject_id', 'stay_id', 'intime', 'outtime']\n",
    "\n",
    "    # only common subjects with both icu stay and an xray\n",
    "    cxr_merged_icustays = cxr_metadata.merge(icu_stay_metadata[columns ], how='inner', on='subject_id')\n",
    "    # combine study date time\n",
    "    cxr_merged_icustays['StudyTime'] = cxr_merged_icustays['StudyTime'].apply(lambda x: f'{int(float(x)):06}' )\n",
    "    cxr_merged_icustays['StudyDateTime'] = pd.to_datetime(cxr_merged_icustays['StudyDate'].astype(str) + ' ' + cxr_merged_icustays['StudyTime'].astype(str) ,format=\"%Y%m%d %H%M%S\")\n",
    "    \n",
    "    cxr_merged_icustays.intime=pd.to_datetime(cxr_merged_icustays.intime)\n",
    "    cxr_merged_icustays.outtime=pd.to_datetime(cxr_merged_icustays.outtime)\n",
    "\n",
    "    if args.task == 'decompensation' or args.task == 'length-of-stay':\n",
    "        train_listfile = pd.read_csv(f'/scratch/se1525/mml-ssl/{args.task}/train_listfile.csv')\n",
    "        train_listfile.columns = ['stay' , 'period_length' , 'stay_id' ,'y_true', 'intime' , 'endtime']\n",
    "        test_listfile = pd.read_csv(f'/scratch/se1525/mml-ssl/{args.task}/test_listfile.csv')\n",
    "        test_listfile.columns = ['stay' , 'period_length' , 'stay_id' ,'y_true', 'intime' , 'endtime']\n",
    "        val_listfile = pd.read_csv(f'/scratch/se1525/mml-ssl/{args.task}/val_listfile.csv')\n",
    "        val_listfile.columns = ['stay' , 'period_length' , 'stay_id' ,'y_true', 'intime' , 'endtime']\n",
    "        listfile = train_listfile.append(test_listfile)\n",
    "        listfile = listfile.append(val_listfile)\n",
    "        listfile['subject_id'] = listfile['stay'].apply(lambda x: x.split(\"_\")[0])\n",
    "        #print(listfile.head)\n",
    "\n",
    "        columns2 = ['subject_id', 'endtime']\n",
    "        listfile['subject_id'] = listfile['subject_id'].astype('int64')\n",
    "        cxr_merged_icustays = cxr_merged_icustays.merge(listfile[columns2], how='inner', on='subject_id')\n",
    "        cxr_merged_icustays.endtime=pd.to_datetime(cxr_merged_icustays.endtime)\n",
    "        cxr_merged_icustays_during = cxr_merged_icustays.loc[((cxr_merged_icustays.StudyDateTime>=cxr_merged_icustays.intime)&(cxr_merged_icustays.StudyDateTime<=cxr_merged_icustays.endtime))]\n",
    "\n",
    "    if args.task == 'in-hospital-mortality':\n",
    "        end_time = cxr_merged_icustays.intime + pd.DateOffset(hours=48)\n",
    "        cxr_merged_icustays_during = cxr_merged_icustays.loc[(cxr_merged_icustays.StudyDateTime>=cxr_merged_icustays.intime)&((cxr_merged_icustays.StudyDateTime<=end_time))]\n",
    "\n",
    "    if args.task == 'phenotyping':\n",
    "        end_time = cxr_merged_icustays.outtime\n",
    "        cxr_merged_icustays_during = cxr_merged_icustays.loc[(cxr_merged_icustays.StudyDateTime>=cxr_merged_icustays.intime)&((cxr_merged_icustays.StudyDateTime<=end_time))]\n",
    "\n",
    "    # cxr_merged_icustays_during = cxr_merged_icustays.loc[(cxr_merged_icustays.StudyDateTime>=cxr_merged_icustays.intime)&((cxr_merged_icustays.StudyDateTime<=cxr_merged_icustays.outtime))]\n",
    "    # select cxrs with the ViewPosition == 'AP\n",
    "    cxr_merged_icustays_AP = cxr_merged_icustays_during[cxr_merged_icustays_during['ViewPosition'] == 'AP']\n",
    "\n",
    "    groups = cxr_merged_icustays_AP.groupby('stay_id')\n",
    "\n",
    "    groups_selected = []\n",
    "    for group in groups:\n",
    "        # select the latest cxr for the icu stay\n",
    "        selected = group[1].sort_values('StudyDateTime').tail(1).reset_index()\n",
    "        groups_selected.append(selected)\n",
    "    groups = pd.concat(groups_selected, ignore_index=True)\n",
    "    # import pdb; pdb.set_trace()\n",
    "\n",
    "    # groups['cxr_length'] = (groups['StudyDateTime'] - groups['intime']).astype('timedelta64[h]')\n",
    "    return groups\n",
    "\n",
    "# def \n",
    "def load_cxr_ehr(args, ehr_train_ds, ehr_val_ds, cxr_train_ds, cxr_val_ds, ehr_test_ds, cxr_test_ds):\n",
    "    print(\"at load cxr\")\n",
    "    cxr_merged_icustays = loadmetadata(args) \n",
    "\n",
    "    # if args.task == 'decompensation' or args.task == 'length-of-stay':\n",
    "    #     splits_labels_train = pd.read_csv(f'/scratch/tmp/{args.task}/train_listfile.csv')\n",
    "    #     splits_labels_val = pd.read_csv(f'/scratch/tmp/{args.task}/val_listfile.csv')\n",
    "    #     splits_labels_test = pd.read_csv(f'/scratch/tmp/{args.task}/test_listfile.csv')\n",
    "    # else:\n",
    "    splits_labels_train = pd.read_csv(f'{args.ehr_data_dir}/{args.task}/train_listfile.csv')\n",
    "    splits_labels_val = pd.read_csv(f'{args.ehr_data_dir}/{args.task}/val_listfile.csv')\n",
    "    splits_labels_test = pd.read_csv(f'{args.ehr_data_dir}/{args.task}/test_listfile.csv')\n",
    "    print(\"read splits\")\n",
    "        \n",
    "    train_meta_with_labels = cxr_merged_icustays.merge(splits_labels_train, how='inner', on='stay_id')\n",
    "    val_meta_with_labels = cxr_merged_icustays.merge(splits_labels_val, how='inner', on='stay_id')\n",
    "    test_meta_with_labels = cxr_merged_icustays.merge(splits_labels_test, how='inner', on='stay_id')\n",
    "\n",
    "    print(\"merging done\")\n",
    "    \n",
    "    train_ds = MIMIC_CXR_EHR(args, train_meta_with_labels, ehr_train_ds, cxr_train_ds)\n",
    "    val_ds = MIMIC_CXR_EHR(args, val_meta_with_labels, ehr_val_ds, cxr_val_ds, split='val')\n",
    "    print(\"train_ds\"  , train_ds[0])\n",
    "    test_ds = MIMIC_CXR_EHR(args, test_meta_with_labels, ehr_test_ds, cxr_test_ds, split='test')\n",
    "\n",
    "    print(\"got datasets\")\n",
    "    \n",
    "    train_dl = DataLoader(train_ds, args.batch_size, shuffle=True, collate_fn=my_collate, pin_memory=True, num_workers=16, drop_last=True)\n",
    "    val_dl = DataLoader(val_ds, args.batch_size, shuffle=False, collate_fn=my_collate, pin_memory=True, num_workers=16, drop_last=False)\n",
    "    test_dl = DataLoader(test_ds, args.batch_size, shuffle=False, collate_fn=my_collate, pin_memory=True, num_workers=16, drop_last=False)\n",
    "\n",
    "    print(\"got dataloaders\")\n",
    "\n",
    "    return train_dl, val_dl, test_dl\n",
    "\n",
    "def printPrevalence(merged_file, args):\n",
    "    if args.labels_set == 'pheno':\n",
    "        total_rows = len(merged_file)\n",
    "        print(merged_file[CLASSES].sum()/total_rows)\n",
    "    else:\n",
    "        total_rows = len(merged_file)\n",
    "        print(merged_file['y_true'].value_counts())\n",
    "    # import pdb; pdb.set_trace()\n",
    "\n",
    "def my_collate(batch):\n",
    "    x = [item[0] for item in batch]\n",
    "    pairs = [False if item[1] is None else True for item in batch]\n",
    "    img = torch.stack([torch.zeros(3, 224, 224) if item[1] is None else item[1] for item in batch])\n",
    "    x, seq_length = pad_zeros(x)\n",
    "    targets_ehr = np.array([item[2] for item in batch])\n",
    "    targets_cxr = torch.stack([torch.zeros(14) if item[3] is None else item[3] for item in batch])\n",
    "    return [x, img, targets_ehr, targets_cxr, seq_length, pairs]\n",
    "\n",
    "def pad_zeros(arr, min_length=None):\n",
    "    dtype = arr[0].dtype\n",
    "    seq_length = [x.shape[0] for x in arr]\n",
    "    max_len = max(seq_length)\n",
    "    ret = [np.concatenate([x, np.zeros((max_len - x.shape[0],) + x.shape[1:], dtype=dtype)], axis=0)\n",
    "           for x in arr]\n",
    "    if (min_length is not None) and ret[0].shape[0] < min_length:\n",
    "        ret = [np.concatenate([x, np.zeros((min_length - x.shape[0],) + x.shape[1:], dtype=dtype)], axis=0)\n",
    "               for x in ret]\n",
    "    return np.array(ret), seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d7d662-8931-4c15-996f-4a3ddedc1922",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "50f300b0-94ac-4d70-84e6-16d85452550f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(align=0.0, batch_size=2, beta_1=0.9, crop=224, cxr_data_dir='/scratch/fs999/shamoutlab/data/physionet.org/files/mimic-cxr-jpg/2.0.0', daft_activation='linear', data_pairs='partial_ehr', data_ratio=1.0, depth=1, dim=256, dropout=0.0, ehr_data_dir='/scratch/fs999/shamoutlab/data/mimic-iv-extracted', epochs=2, eval=False, fusion='joint', fusion_type='uni_ehr', imputation='previous', labels_set='pheno', layer_after=4, layers=1, load_state=None, load_state_cxr=None, load_state_ehr=None, lr=0.8, missing_token=None, mmtm_ratio=4, mode='train', network=None, normalizer_state=None, num_classes=10, patience=15, pretrained=False, rec_dropout=0.0, resize=256, resume=False, save_dir='/scratch/se1525/mml-ssl/checkpoints/phenotyping/models', task='decompensation', timestep=1.0, vision_backbone='resnet34', vision_num_classes=14)\n",
      "init EHR\n",
      "times in EHR dataset [27.0, 534.0, 188.0, 24.0, 27.0, 153.0, 128.0, 13.0, 167.0, 15.0]\n",
      "init EHR\n",
      "times in EHR dataset [43.0, 143.0, 184.0, 6.0, 297.0, 10.0, 10.0, 357.0, 13.0, 193.0]\n",
      "init EHR\n",
      "times in EHR dataset [5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0]\n",
      " ehr_train_ds (array([[ 1.        ,  0.        , -0.01233669, ...,  0.        ,\n",
      "         0.        ,  1.        ],\n",
      "       [ 1.        ,  0.        , -0.01233669, ...,  0.        ,\n",
      "         1.        ,  1.        ],\n",
      "       [ 1.        ,  0.        , -0.03036311, ...,  1.        ,\n",
      "         0.        ,  0.        ],\n",
      "       ...,\n",
      "       [ 1.        ,  0.        ,  0.01019634, ...,  0.        ,\n",
      "         0.        ,  0.        ],\n",
      "       [ 1.        ,  0.        ,  0.03272937, ...,  0.        ,\n",
      "         0.        ,  0.        ],\n",
      "       [ 1.        ,  0.        ,  0.12286149, ...,  1.        ,\n",
      "         0.        ,  0.        ]]), 0)\n",
      "at load cxr\n",
      "read splits\n",
      "merging done\n",
      "train_ds (array([[ 1.        ,  0.        , -0.01233669, ...,  0.        ,\n",
      "         0.        ,  1.        ],\n",
      "       [ 1.        ,  0.        , -0.01233669, ...,  0.        ,\n",
      "         1.        ,  1.        ],\n",
      "       [ 1.        ,  0.        , -0.03036311, ...,  1.        ,\n",
      "         0.        ,  0.        ],\n",
      "       ...,\n",
      "       [ 1.        ,  0.        ,  0.01019634, ...,  0.        ,\n",
      "         0.        ,  0.        ],\n",
      "       [ 1.        ,  0.        ,  0.03272937, ...,  0.        ,\n",
      "         0.        ,  0.        ],\n",
      "       [ 1.        ,  0.        ,  0.12286149, ...,  1.        ,\n",
      "         0.        ,  0.        ]]), None, 0, None)\n",
      "got datasets\n",
      "got dataloaders\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import argparse\n",
    "import os\n",
    "import imp\n",
    "import re\n",
    "from trainers.fusion_trainer import FusionTrainer\n",
    "from trainers.mmtm_trainer import MMTMTrainer\n",
    "from trainers.daft_trainer import DAFTTrainer\n",
    "\n",
    "# from ehr_utils.preprocessing import Discretizer, Normalizer\n",
    "# from datasets.ehr_dataset import get_datasets\n",
    "from datasets.cxr_dataset import get_cxr_datasets\n",
    "# from datasets.fusion import load_cxr_ehr\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "from arguments import args_parser\n",
    "\n",
    "parser = args_parser()\n",
    "args = parser.parse_args([ \n",
    "'--vision-backbone', 'resnet34' ,\n",
    "'--resize', '256' , \n",
    "'--task' , 'decompensation' ,\n",
    "'--epochs' , '2' , \n",
    "'--batch_size' , '2' , '--lr' , '0.8' ,\n",
    "'--mode' , 'train' ,\n",
    "'--fusion_type' , 'None' ,\n",
    "'--save_dir' , '/scratch/se1525/mml-ssl/checkpoints/phenotyping/models' ,\n",
    " '--ehr_data_dir', '/scratch/fs999/shamoutlab/data/mimic-iv-extracted',\n",
    "'--data_pairs', 'partial_ehr', \n",
    "'--fusion_type' , 'uni_ehr', \n",
    "'--num_classes' , '10'])\n",
    "\n",
    "\n",
    "# add more arguments here ...\n",
    "# args = parser.parse_args()\n",
    "print(args)\n",
    "\n",
    "if args.missing_token is not None:\n",
    "    from trainers.fusion_tokens_trainer import FusionTokensTrainer as FusionTrainer\n",
    "    \n",
    "path = Path(args.save_dir)\n",
    "path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "seed = 1002\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "def read_timeseries(args):\n",
    "    path = f'{args.ehr_data_dir}/{args.task}/train/14991576_episode3_timeseries.csv'\n",
    "    ret = []\n",
    "    with open(path, \"r\") as tsfile:\n",
    "        header = tsfile.readline().strip().split(',')\n",
    "        assert header[0] == \"Hours\"\n",
    "        for line in tsfile:\n",
    "            mas = line.strip().split(',')\n",
    "            ret.append(np.array(mas))\n",
    "    return np.stack(ret)\n",
    "    \n",
    "\n",
    "discretizer = Discretizer(timestep=float(args.timestep),\n",
    "                          store_masks=True,\n",
    "                          impute_strategy='previous',\n",
    "                          start_time='zero')\n",
    "\n",
    "\n",
    "discretizer_header = discretizer.transform(read_timeseries(args))[1].split(',')\n",
    "cont_channels = [i for (i, x) in enumerate(discretizer_header) if x.find(\"->\") == -1]\n",
    "\n",
    "normalizer = Normalizer(fields=cont_channels)  # choose here which columns to standardize\n",
    "normalizer_state = args.normalizer_state\n",
    "if normalizer_state is None:\n",
    "    normalizer_state = 'normalizers/ph_ts{}.input_str:previous.start_time:zero.normalizer'.format(args.timestep)\n",
    "    normalizer_state = os.path.join(os.path.dirname('/scratch/se1525/mml-ssl/medfuse_baseline/'), normalizer_state)\n",
    "normalizer.load_params(normalizer_state)\n",
    "\n",
    "ehr_train_ds, ehr_val_ds, ehr_test_ds = get_datasets(discretizer, normalizer, args)\n",
    "\n",
    "cxr_train_ds, cxr_val_ds, cxr_test_ds = get_cxr_datasets(args)\n",
    "\n",
    "print(\" ehr_train_ds\" , ehr_train_ds[('16918793_episode1_timeseries.csv', 27.0)])\n",
    "\n",
    "train_dl, val_dl, test_dl = load_cxr_ehr(args, ehr_train_ds, ehr_val_ds, cxr_train_ds, cxr_val_ds, ehr_test_ds, cxr_test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e852bc37-2752-4303-a907-5b7974a33c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Contents of Validation DataLoader:\n",
      "Batch 1:\n",
      "  X data: (2, 143, 76)\n",
      "  Image data: torch.Size([2, 3, 224, 224])\n",
      "  EHR targets: (2,)\n",
      "  CXR targets: torch.Size([2, 14])\n",
      "  Sequence lengths: [43, 143]\n",
      "  Pairs: [False, False]\n",
      "\n",
      "Contents of Test DataLoader:\n",
      "Batch 1:\n",
      "  X data: (2, 6, 76)\n",
      "  Image data: torch.Size([2, 3, 224, 224])\n",
      "  EHR targets: (2,)\n",
      "  CXR targets: torch.Size([2, 14])\n",
      "  Sequence lengths: [5, 6]\n",
      "  Pairs: [False, False]\n"
     ]
    }
   ],
   "source": [
    "def debug_data_loader(dl, name, num_batches=1):\n",
    "    print(f\"\\nContents of {name}:\")\n",
    "    for i, batch in enumerate(dl):\n",
    "        if i >= num_batches: break\n",
    "        print(f\"Batch {i + 1}:\")\n",
    "        x, img, targets_ehr, targets_cxr, seq_length, pairs = batch\n",
    "        print(f\"  X data: {x.shape}\")\n",
    "        print(f\"  Image data: {img.shape}\")\n",
    "        print(f\"  EHR targets: {targets_ehr.shape}\")\n",
    "        print(f\"  CXR targets: {targets_cxr.shape}\")\n",
    "        print(f\"  Sequence lengths: {seq_length}\")\n",
    "        print(f\"  Pairs: {pairs}\")\n",
    "\n",
    "# Optionally print data loader contents for debugging\n",
    "debug_data_loader(val_dl, 'Validation DataLoader')\n",
    "debug_data_loader(test_dl, 'Test DataLoader')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f883840-8531-4b34-9fcf-6515802b7621",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e7caa4-928b-46ca-884b-9eb3b3123a78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MML SSL",
   "language": "python",
   "name": "mml-ssl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
