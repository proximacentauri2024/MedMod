{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5f19b1eb-ea12-40a9-b0c8-a15a80096e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import platform\n",
    "import pickle\n",
    "import json\n",
    "from PIL import Image\n",
    "import glob\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import custom_parser as par\n",
    "\n",
    "parser = par.initiate_parsing()\n",
    "args = parser.parse_args([ '--device' , '$CUDA_VISIBLE_DEVICES',\n",
    "'--vision-backbone', 'resnet34' ,\n",
    "'--resize', '256' , \n",
    "'--task' , 'phenotyping' ,\n",
    "'--job_number' , '${SLURM_JOBID}',\n",
    "'--file_name' , 'SIMCLR-${SLURM_JOBID}' ,\n",
    "'--epochs' , '2' , '--transforms_cxr' , 'simclrv2' , '--temperature' , '0.01' ,\n",
    "'--batch_size' , '30' , '--lr' , '0.8' ,\n",
    "'--num_gpu' , '1' ,\n",
    "'--pretrain_type' , 'simclr' ,\n",
    "'--mode' , 'train' ,\n",
    "'--fusion_type' , 'None' ,\n",
    "'--save_dir' , '/scratch/se1525/mml-ssl/checkpoints/phenotyping/models' ,\n",
    "'--tag' , 'simclr_train'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "59b5f854-7596-4ef3-97c8-17708be58654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Capillary refill rate\n",
      "Diastolic blood pressure\n",
      "Fraction inspired oxygen\n",
      "Glascow coma scale eye opening\n",
      "Glascow coma scale motor response\n",
      "Glascow coma scale total\n",
      "Glascow coma scale verbal response\n",
      "Glucose\n",
      "Heart Rate\n",
      "Height\n",
      "Mean blood pressure\n",
      "Oxygen saturation\n",
      "Respiratory rate\n",
      "Systolic blood pressure\n",
      "Temperature\n",
      "Weight\n",
      "pH\n"
     ]
    }
   ],
   "source": [
    "class Discretizer:\n",
    "    def __init__(self, timestep=0.8, store_masks=True, impute_strategy='zero', start_time='zero',\n",
    "                 config_path= 'ehr_utils/resources/discretizer_config.json'):\n",
    "\n",
    "        with open(config_path) as f:\n",
    "            config = json.load(f)\n",
    "            # print(config)\n",
    "            self._id_to_channel = config['id_to_channel']\n",
    "            self._channel_to_id = dict(zip(self._id_to_channel, range(len(self._id_to_channel))))\n",
    "            self._is_categorical_channel = config['is_categorical_channel']\n",
    "            self._possible_values = config['possible_values']\n",
    "            self._normal_values = config['normal_values']\n",
    "            for channel in self._is_categorical_channel:\n",
    "                    print(channel)\n",
    "            #         print(len(self._possible_values[channel]))\n",
    "\n",
    "        self._header = [\"Hours\"] + self._id_to_channel\n",
    "        self._timestep = timestep\n",
    "        self._store_masks = store_masks\n",
    "        self._start_time = start_time\n",
    "        self._impute_strategy = impute_strategy\n",
    "\n",
    "        # for statistics\n",
    "        self._done_count = 0\n",
    "        self._empty_bins_sum = 0\n",
    "        self._unused_data_sum = 0\n",
    "\n",
    "    def transform(self, X, header=None, end=None):\n",
    "        # print(\"end\" , end)\n",
    "        if header is None:\n",
    "            # print(\"none\")\n",
    "            header = self._header\n",
    "        assert header[0] == \"Hours\"\n",
    "        eps = 1e-6\n",
    "\n",
    "        N_channels = len(self._id_to_channel)\n",
    "        ts = [float(row[0]) for row in X]\n",
    "        for i in range(len(ts) - 1):\n",
    "            assert ts[i] < ts[i+1] + eps\n",
    "\n",
    "        if self._start_time == 'relative':\n",
    "            first_time = ts[0]\n",
    "        elif self._start_time == 'zero':\n",
    "            first_time = 0\n",
    "        else:\n",
    "            raise ValueError(\"start_time is invalid\")\n",
    "\n",
    "        if end is None:\n",
    "            max_hours = max(ts) - first_time\n",
    "        else:\n",
    "            max_hours = end - first_time\n",
    "\n",
    "        N_bins = int(max_hours / self._timestep + 1.0 - eps)\n",
    "        # print(max_hours, self._timestep, N_bins)\n",
    "\n",
    "        cur_len = 0\n",
    "        begin_pos = [0 for i in range(N_channels)]\n",
    "        end_pos = [0 for i in range(N_channels)]\n",
    "        for i in range(N_channels):\n",
    "            channel = self._id_to_channel[i]\n",
    "            begin_pos[i] = cur_len\n",
    "            if self._is_categorical_channel[channel]:\n",
    "                end_pos[i] = begin_pos[i] + len(self._possible_values[channel])\n",
    "            else:\n",
    "                end_pos[i] = begin_pos[i] + 1\n",
    "            cur_len = end_pos[i]\n",
    "\n",
    "        data = np.zeros(shape=(N_bins, cur_len), dtype=float)\n",
    "        mask = np.zeros(shape=(N_bins, N_channels), dtype=int)\n",
    "        original_value = [[\"\" for j in range(N_channels)] for i in range(N_bins)]\n",
    "        total_data = 0\n",
    "        unused_data = 0\n",
    "\n",
    "        def write(data, bin_id, channel, value, begin_pos):\n",
    "            channel_id = self._channel_to_id[channel]\n",
    "            if self._is_categorical_channel[channel]:\n",
    "                # print(\"list: \", self._possible_values[channel], \"val: \", value, \"channel:\", channel)\n",
    "                category_id = self._possible_values[channel].index(value)\n",
    "                N_values = len(self._possible_values[channel])\n",
    "                one_hot = np.zeros((N_values,))\n",
    "                one_hot[category_id] = 1\n",
    "                for pos in range(N_values):\n",
    "                    data[bin_id, begin_pos[channel_id] + pos] = one_hot[pos]\n",
    "            else:\n",
    "                data[bin_id, begin_pos[channel_id]] = float(value)\n",
    "\n",
    "        for row in X:\n",
    "            t = float(row[0]) - first_time\n",
    "            if t > max_hours + eps:\n",
    "                continue\n",
    "            bin_id = int(t / self._timestep - eps)\n",
    "            assert 0 <= bin_id < N_bins\n",
    "\n",
    "            for j in range(1, len(row)):\n",
    "                if row[j] == \"\":\n",
    "                    continue\n",
    "                channel = header[j]\n",
    "                channel_id = self._channel_to_id[channel]\n",
    "\n",
    "                total_data += 1\n",
    "                if mask[bin_id][channel_id] == 1:\n",
    "                    unused_data += 1\n",
    "                mask[bin_id][channel_id] = 1\n",
    "\n",
    "                write(data, bin_id, channel, row[j], begin_pos)\n",
    "                original_value[bin_id][channel_id] = row[j]\n",
    "\n",
    "        # impute missing values\n",
    "\n",
    "        if self._impute_strategy not in ['zero', 'normal_value', 'previous', 'next']:\n",
    "            raise ValueError(\"impute strategy is invalid\")\n",
    "\n",
    "        if self._impute_strategy in ['normal_value', 'previous']:\n",
    "            prev_values = [[] for i in range(len(self._id_to_channel))]\n",
    "            for bin_id in range(N_bins):\n",
    "                for channel in self._id_to_channel:\n",
    "                    channel_id = self._channel_to_id[channel]\n",
    "                    if mask[bin_id][channel_id] == 1:\n",
    "                        prev_values[channel_id].append(original_value[bin_id][channel_id])\n",
    "                        continue\n",
    "                    if self._impute_strategy == 'normal_value':\n",
    "                        imputed_value = self._normal_values[channel]\n",
    "                    if self._impute_strategy == 'previous':\n",
    "                        if len(prev_values[channel_id]) == 0:\n",
    "                            imputed_value = self._normal_values[channel]\n",
    "                        else:\n",
    "                            imputed_value = prev_values[channel_id][-1]\n",
    "                    write(data, bin_id, channel, imputed_value, begin_pos)\n",
    "\n",
    "        if self._impute_strategy == 'next':\n",
    "            prev_values = [[] for i in range(len(self._id_to_channel))]\n",
    "            for bin_id in range(N_bins-1, -1, -1):\n",
    "                for channel in self._id_to_channel:\n",
    "                    channel_id = self._channel_to_id[channel]\n",
    "                    if mask[bin_id][channel_id] == 1:\n",
    "                        prev_values[channel_id].append(original_value[bin_id][channel_id])\n",
    "                        continue\n",
    "                    if len(prev_values[channel_id]) == 0:\n",
    "                        imputed_value = self._normal_values[channel]\n",
    "                    else:\n",
    "                        imputed_value = prev_values[channel_id][-1]\n",
    "                    write(data, bin_id, channel, imputed_value, begin_pos)\n",
    "\n",
    "        empty_bins = np.sum([1 - min(1, np.sum(mask[i, :])) for i in range(N_bins)])\n",
    "        self._done_count += 1\n",
    "        self._empty_bins_sum += empty_bins / (N_bins + eps)\n",
    "        self._unused_data_sum += unused_data / (total_data + eps)\n",
    "\n",
    "        if self._store_masks:\n",
    "            data = np.hstack([data, mask.astype(np.float32)])\n",
    "\n",
    "        # create new header\n",
    "        new_header = []\n",
    "        for channel in self._id_to_channel:\n",
    "            if self._is_categorical_channel[channel]:\n",
    "                values = self._possible_values[channel]\n",
    "                for value in values:\n",
    "                    new_header.append(channel + \"->\" + value)\n",
    "            else:\n",
    "                new_header.append(channel)\n",
    "\n",
    "        if self._store_masks:\n",
    "            for i in range(len(self._id_to_channel)):\n",
    "                channel = self._id_to_channel[i]\n",
    "                new_header.append(\"mask->\" + channel)\n",
    "\n",
    "        new_header = \",\".join(new_header)\n",
    "\n",
    "        return (data, new_header)\n",
    "\n",
    "    def print_statistics(self):\n",
    "        print(\"statistics of discretizer:\")\n",
    "        print(\"\\tconverted {} examples\".format(self._done_count))\n",
    "        print(\"\\taverage unused data = {:.2f} percent\".format(100.0 * self._unused_data_sum / self._done_count))\n",
    "        print(\"\\taverage empty  bins = {:.2f} percent\".format(100.0 * self._empty_bins_sum / self._done_count))\n",
    "        \n",
    "\n",
    "\n",
    "# data preprocessor\n",
    "def read_timeseries(args):\n",
    "    path = f'{args.ehr_data_root}/{args.task}/train/14991576_episode3_timeseries.csv'\n",
    "    ret = []\n",
    "    with open(path, \"r\") as tsfile:\n",
    "        header = tsfile.readline().strip().split(',')\n",
    "        assert header[0] == \"Hours\"\n",
    "        for line in tsfile:\n",
    "            mas = line.strip().split(',')\n",
    "            # print(mas)\n",
    "            ret.append(np.array(mas))\n",
    "    return np.stack(ret)\n",
    "\n",
    "\n",
    "def ehr_funcs_discretizer(args):\n",
    "    \n",
    "    discretizer = Discretizer(timestep=float(args.timestep),\n",
    "                          store_masks=True,\n",
    "                          impute_strategy='previous',\n",
    "                          start_time='zero',\n",
    "                          config_path=f'/scratch/se1525/mml-ssl/ehr_utils/resources/discretizer_config.json')\n",
    "\n",
    "    # print(read_timeseries(args))\n",
    "    discretizer_header = discretizer.transform(read_timeseries(args))[1].split(',')\n",
    "    # print(discretizer_header)\n",
    "    cont_channels = [i for (i, x) in enumerate(discretizer_header) if x.find(\"->\") == -1]\n",
    "    indices_and_values = [(i, x) for (i, x) in enumerate(discretizer_header) if x.find(\"->\") == -1]\n",
    "    # print(len(indices_and_values))\n",
    "    # for i, x in indices_and_values:\n",
    "        # print(\"Index:\", i)\n",
    "        # print(\"Value:\", x)   \n",
    "    return discretizer, cont_channels\n",
    "\n",
    "discretizer, cont_channels = ehr_funcs_discretizer(args)\n",
    "\n",
    "class Normalizer:\n",
    "    def __init__(self, fields=None):\n",
    "        self._means = None\n",
    "        self._stds = None\n",
    "        self._fields = None\n",
    "        if fields is not None:\n",
    "            self._fields = [col for col in fields]\n",
    "\n",
    "        self._sum_x = None\n",
    "        self._sum_sq_x = None\n",
    "        self._count = 0\n",
    "\n",
    "    def _feed_data(self, x):\n",
    "        x = np.array(x)\n",
    "        self._count += x.shape[0]\n",
    "        if self._sum_x is None:\n",
    "            self._sum_x = np.sum(x, axis=0)\n",
    "            self._sum_sq_x = np.sum(x**2, axis=0)\n",
    "        else:\n",
    "            self._sum_x += np.sum(x, axis=0)\n",
    "            self._sum_sq_x += np.sum(x**2, axis=0)\n",
    "\n",
    "    def _save_params(self, save_file_path):\n",
    "        eps = 1e-7\n",
    "        with open(save_file_path, \"wb\") as save_file:\n",
    "            N = self._count\n",
    "            self._means = 1.0 / N * self._sum_x\n",
    "            self._stds = np.sqrt(1.0/(N - 1) * (self._sum_sq_x - 2.0 * self._sum_x * self._means + N * self._means**2))\n",
    "            self._stds[self._stds < eps] = eps\n",
    "            pickle.dump(obj={'means': self._means,\n",
    "                             'stds': self._stds},\n",
    "                        file=save_file,\n",
    "                        protocol=2)\n",
    "\n",
    "    def load_params(self, load_file_path):\n",
    "        with open(load_file_path, \"rb\") as load_file:\n",
    "            if platform.python_version()[0] == '2':\n",
    "                dct = pickle.load(load_file)\n",
    "            else:\n",
    "                dct = pickle.load(load_file, encoding='latin1')\n",
    "            self._means = dct['means']\n",
    "            self._stds = dct['stds']\n",
    "\n",
    "    def transform(self, X):\n",
    "        if self._fields is None:\n",
    "            fields = range(X.shape[1])\n",
    "        else:\n",
    "            fields = self._fields\n",
    "        ret = 1.0 * X\n",
    "        for col in fields:\n",
    "            ret[:, col] = (X[:, col] - self._means[col]) / self._stds[col]\n",
    "        return ret\n",
    "    \n",
    "def ehr_funcs_normalizer(args, cont_channels):\n",
    "    normalizer = Normalizer(fields=cont_channels)  # choose here which columns to standardize\n",
    "    normalizer_state = args.normalizer_state\n",
    "    if normalizer_state is None:\n",
    "        normalizer_state = 'ph_ts{}.input_str:previous.start_time:zero.normalizer'.format(args.timestep)\n",
    "        normalizer_state = os.path.join('/scratch/se1525/mml-ssl/', normalizer_state)\n",
    "    normalizer.load_params(normalizer_state)\n",
    "    \n",
    "    return normalizer\n",
    "\n",
    "normalizer = ehr_funcs_normalizer(args, cont_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6a51b6d1-2355-4417-bd4b-12f9293affe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EHRdataset(Dataset):\n",
    "    def __init__(self, discretizer, normalizer, listfile, dataset_dir, return_names=True, period_length=48.0, transforms=None):\n",
    "        self.return_names = return_names\n",
    "        self.discretizer = discretizer\n",
    "        self.normalizer = normalizer\n",
    "        self._period_length = period_length\n",
    "\n",
    "        self._dataset_dir = dataset_dir\n",
    "        listfile_path = listfile\n",
    "        with open(listfile_path, \"r\") as lfile:\n",
    "            self._data = lfile.readlines()\n",
    "            print(\"Length of list file\" , len(self._data))\n",
    "        self._listfile_header = self._data[0]\n",
    "        self.CLASSES = self._listfile_header.strip().split(',')[3:]\n",
    "        # print(self.CLASSES)\n",
    "        self._data = self._data[1:]\n",
    "        # print(self._data[:1])\n",
    "        self.transforms = transforms\n",
    "        self._data = [line.split(',') for line in self._data]\n",
    "        # print(self._data)\n",
    "        # self._data_map_pheno = [(mas[0], float(mas[1]), list(map(int, mas[2:]))) for mas in self._data]\n",
    "\n",
    "        self.data_map = {\n",
    "            mas[0]: {\n",
    "                'labels': list(map(int, mas[3:])),\n",
    "                'stay_id': float(mas[2]),\n",
    "                'time': float(mas[1]),\n",
    "                }\n",
    "                for mas in self._data\n",
    "        }\n",
    "        # self.data = [(x, float(t), int(stay_id) ,int(y)) for (x, t, stay_id , y) in self._data]\n",
    "        print(\"Length of data_map\" , len(list(self.data_map)))\n",
    "        # print(\"Length of data\" , len(self.data))\n",
    "\n",
    "        \n",
    "        # count = 0\n",
    "        # for value in self._data_map_pheno:\n",
    "        #     print(f\"Value: {value}\")\n",
    "        #     count += 1\n",
    "        #     if count == 5:\n",
    "        #         break\n",
    "             \n",
    "        # print (' ---------------- ')\n",
    "        # self._data_map_decomp = [(x, float(t), int(y)) for (x, t, s, y) in self._data]\n",
    "        # count = 0\n",
    "        # for value in self._data_map_decomp:\n",
    "        #     print(f\"Value: {value}\")\n",
    "        #     count += 1\n",
    "        #     if count == 5:\n",
    "        #         break\n",
    "\n",
    "        self.names = list(self.data_map.keys())\n",
    "    \n",
    "    def _read_timeseries(self, ts_filename, time_bound):\n",
    "        \n",
    "        ret = []\n",
    "        with open(os.path.join(self._dataset_dir, ts_filename), \"r\") as tsfile:\n",
    "            header = tsfile.readline().strip().split(',')\n",
    "            assert header[0] == \"Hours\"\n",
    "            for line in tsfile:\n",
    "                mas = line.strip().split(',')\n",
    "                if time_bound is not None:\n",
    "                    t = float(mas[0])\n",
    "                    if t > time_bound + 1e-6:\n",
    "                        break\n",
    "                ret.append(np.array(mas))\n",
    "        return (np.stack(ret), header)\n",
    "\n",
    "    \n",
    "    def read_by_file_name(self, index):\n",
    "        t = self.data_map[index]['time']\n",
    "        time_bound = t\n",
    "        print(\"time bound\" , time_bound)\n",
    "        print(len(list(self.data_map.keys())))\n",
    "        y = self.data_map[index]['labels']\n",
    "        stay_id = self.data_map[index]['stay_id']\n",
    "        (X, header) = self._read_timeseries(index, time_bound)\n",
    "\n",
    "        return {\"X\": X,\n",
    "                \"t\": t,\n",
    "                \"y\": y,\n",
    "                'stay_id': stay_id,\n",
    "                \"header\": header,\n",
    "                \"name\": index}\n",
    " \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if isinstance(index, int):\n",
    "            index = self.names[index]\n",
    "        ret = self.read_by_file_name(index)\n",
    "        print(\"length after read_by_file_name\" , len(ret[\"X\"]))\n",
    "        # print(\"ret\" , ret)\n",
    "        data = ret[\"X\"]\n",
    "        ts = ret[\"t\"] if ret['t'] > 0.0 else self._period_length\n",
    "        # print(\"ts\" , ts)\n",
    "        \n",
    "\n",
    "        # print(\"data length before: \", len(data[0]))\n",
    "        # print(\"data before: \" , data[0])\n",
    "        data = self.discretizer.transform(data, end=ts)[0]\n",
    "        # print(\"data length after: \", len(data[0]))\n",
    "        # print(\"data after:\" ,  data[0])\n",
    "        if (self.normalizer is not None):\n",
    "            data = self.normalizer.transform(data)\n",
    "               \n",
    "\n",
    "        ys = ret[\"y\"]\n",
    "        names = ret[\"name\"]\n",
    "        ys = np.array(ys, dtype=np.int32) if len(ys) > 1 else np.array(ys, dtype=np.int32)[0]\n",
    "        return data, ys\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "61a1a95c-4d55-4f85-87ad-12fee8d51526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of list file 42629\n",
      "Length of data_map 42628\n",
      "Length of list file 4803\n",
      "Length of data_map 4802\n",
      "Length of list file 11915\n",
      "Length of data_map 11914\n",
      "time bound 25.104444\n",
      "11914\n",
      "length after read_by_file_name 39\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[ 1.        ,  0.        , -0.00332347, ...,  0.        ,\n",
       "          1.        ,  0.        ],\n",
       "        [ 1.        ,  0.        , -0.05289614, ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 1.        ,  0.        , -0.03036311, ...,  1.        ,\n",
       "          1.        ,  0.        ],\n",
       "        ...,\n",
       "        [ 1.        ,  0.        ,  0.01019634, ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 1.        ,  0.        ,  0.01019634, ...,  1.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 1.        ,  0.        ,  0.01019634, ...,  0.        ,\n",
       "          0.        ,  0.        ]]),\n",
       " array([0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0], dtype=int32))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_datasets(discretizer, normalizer, args):\n",
    "    transform = None\n",
    "    # print(f'{args.ehr_data_root}/{args.task}/train_listfile.csv')\n",
    "    # print(os.path.join(args.ehr_data_root, f'{args.task}/train'))\n",
    "    train_ds = EHRdataset(discretizer, normalizer, f'{args.ehr_data_root}/{args.task}/train_listfile.csv', os.path.join(args.ehr_data_root, f'{args.task}/train'), transforms=transform)\n",
    "    val_ds = EHRdataset(discretizer, normalizer, f'{args.ehr_data_root}/{args.task}/val_listfile.csv', os.path.join(args.ehr_data_root, f'{args.task}/train'), transforms = transform)\n",
    "    test_ds = EHRdataset(discretizer, normalizer, f'{args.ehr_data_root}/{args.task}/test_listfile.csv', os.path.join(args.ehr_data_root, f'{args.task}/test'), transforms = transform)\n",
    "    return train_ds, val_ds, test_ds\n",
    "ehr_train_ds, ehr_val_ds, ehr_test_ds = get_datasets(discretizer, normalizer, args)\n",
    "# print(len(ehr_train_ds[1][0]))\n",
    "# print(ehr_train_ds[1])\n",
    "get_item = ehr_test_ds[1]\n",
    "get_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e9db515-7715-44f2-ba4b-8c19a800fce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('/scratch/fs999/shamoutlab/data/mimic-iv-extracted/phenotyping/train_listfile.csv', \"r\") as lfile:\n",
    "#     data = lfile.readlines()\n",
    "# listfile_header = data[0]\n",
    "# CLASSES = listfile_header.strip().split(',')[3:]\n",
    "# data = data[1:]\n",
    "\n",
    "# data = [line.split(',') for line in data]\n",
    "# print(data[1])\n",
    "# data_map = {\n",
    "#     mas[0]: {\n",
    "#         'labels': list(map(float, mas[3:])),\n",
    "#         'stay_id': float(mas[2]),\n",
    "#         'time': float(mas[1]),\n",
    "#         }\n",
    "#         for mas in data\n",
    "# }\n",
    "# print(data_map['16505791_episode1_timeseries.csv'])\n",
    "# def _read_timeseries(ts_filename, time_bound=None):\n",
    "#     ret = []\n",
    "#     with open(os.path.join('/scratch/fs999/shamoutlab/data/mimic-iv-extracted/phenotyping/train', ts_filename), \"r\") as tsfile:\n",
    "#         header = tsfile.readline().strip().split(',')\n",
    "#         assert header[0] == \"Hours\"\n",
    "#         for line in tsfile:\n",
    "#             mas = line.strip().split(',')\n",
    "#             if time_bound is not None:\n",
    "#                 t = float(mas[0])\n",
    "#                 if t > time_bound + 1e-6:\n",
    "#                     break\n",
    "#             ret.append(np.array(mas))\n",
    "#     return (np.stack(ret), header)\n",
    "# def read_by_file_name(data_map, index, time_bound=None):\n",
    "#     t = data_map[index]['time'] if time_bound is None else time_bound\n",
    "#     y = data_map[index]['labels']\n",
    "#     stay_id = data_map[index]['stay_id']\n",
    "#     (X, header) = read_timeseries(index, time_bound=time_bound)\n",
    "\n",
    "#     return {\"X\": X,\n",
    "#             \"t\": t,\n",
    "#             \"y\": y,\n",
    "#             'stay_id': stay_id,\n",
    "#             \"header\": header,\n",
    "#             \"name\": index}\n",
    "\n",
    "# def __getitem__(data_map, index, time_bound=None):\n",
    "#     if isinstance(index, int):\n",
    "#         index = self.names[index]\n",
    "#     ret = self.read_by_file_name(data_map, index, time_bound)\n",
    "#     data = ret[\"X\"]\n",
    "#     ts = ret[\"t\"] if ret['t'] > 0.0 else 48\n",
    "#     ys = ret[\"y\"]\n",
    "#     names = ret[\"name\"]\n",
    "#     data = discretizer.transform(data, end=ts)[0] \n",
    "#     if (normalizer is not None):\n",
    "#         data = self.normalizer.transform(data)\n",
    "#     ys = np.array(ys, dtype=np.int32) if len(ys) > 1 else np.array(ys, dtype=np.int32)[0]\n",
    "#     return data, ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed7e5e0d-3db0-4f88-927a-a6ab981bcd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# listfile = f'{args.ehr_data_root}/length-of-stay/train_listfile.csv'\n",
    "# print(listfile)\n",
    "# with open(listfile, \"r\") as lfile:\n",
    "#     data = lfile.readlines()\n",
    "# listfile_header = data[0]\n",
    "# print(listfile_header)\n",
    "# classes = listfile_header.strip().split(',')[3:]\n",
    "# print(classes)\n",
    "# data = data[1:]\n",
    "\n",
    "# print(data[1:10])\n",
    "# data = [line.split(',') for line in data]\n",
    "# print(data[8][0])\n",
    "# # data_map = {\n",
    "#             mas[0]: {\n",
    "#                 'labels': list(map(float, mas[3:])),\n",
    "#                 'stay_id': float(mas[2]),\n",
    "#                 'time': float(mas[1]),\n",
    "#                 }\n",
    "#                 for mas in data\n",
    "#         }\n",
    "\n",
    "# data_map = {\n",
    "#     x: {\n",
    "#         'time': float(t),\n",
    "#         'stay_id': int(stay_id),\n",
    "#         'labels': float(y)\n",
    "#     }\n",
    "#     for x, t, stay_id, y in data\n",
    "# }\n",
    "# for mas in data[0:10]:\n",
    "#     print(mas)\n",
    "#     print(mas[3])\n",
    "\n",
    "data_map={}\n",
    "# for mas in data:\n",
    "#     data_map[mas[0]]=['labels': mas[3], \n",
    "#                     'stay_id': mas[2],\n",
    "#                     'time': mas[1]}\n",
    "# for mas in data:\n",
    "#     data_map[mas[0]]= mas[3]\n",
    "# data_map = {\n",
    "#             mas[0]: {\n",
    "#                 'labels': mas[3],\n",
    "#                 'stay_id': mas[2],\n",
    "#                 'time': mas[1],\n",
    "#                 }\n",
    "#                 for mas in data\n",
    "#         }\n",
    "# data_map = [(x, float(t), int(stay_id) ,float(y)) for (x, t, stay_id , y) in data]\n",
    "# # print(data_map['10001884_episode1_timeseries.csv'])\n",
    "# print(data_map[8])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1112c7a-cffe-4772-87d2-8d3b7072c97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class EHRdataset(Dataset):\n",
    "#     def __init__(self, discretizer, normalizer, listfile, dataset_dir, return_names=True, period_length=48.0):\n",
    "#         self.return_names = return_names\n",
    "#         self.discretizer = discretizer\n",
    "#         self.normalizer = normalizer\n",
    "#         self._period_length = period_length\n",
    "\n",
    "#         self._dataset_dir = dataset_dir\n",
    "#         listfile_path = listfile\n",
    "#         with open(listfile_path, \"r\") as lfile:\n",
    "#             self._data = lfile.readlines()\n",
    "#         self._listfile_header = self._data[0]\n",
    "#         self.CLASSES = self._listfile_header.strip().split(',')[3:]\n",
    "#         self._data = self._data[1:]\n",
    "#         print(self._data[12])\n",
    "\n",
    "\n",
    "#         self._data = [line.split(',') for line in self._data]\n",
    "#         if 'length-of-stay' or 'decompensation' in self._dataset_dir:\n",
    "#             self.data_map = [(x, float(t), int(stay_id) ,[float(y)]) for (x, t, stay_id , y) in self._data]\n",
    "#             self.names = [x[0] for x in self.data_map]\n",
    "#             self.times= [x[1] for x in self.data_map]\n",
    "#         else:\n",
    "#             self.data_map = {\n",
    "#             mas[0]: {\n",
    "#                 'labels': list(map(float, mas[3:])),\n",
    "#                 'stay_id': float(mas[2]),\n",
    "#                 'time': float(mas[1]),\n",
    "#                 }\n",
    "#                 for mas in self._data\n",
    "#         }\n",
    "#             self.names = list(self.data_map.keys())\n",
    "#         print(self.data_map[12])\n",
    "        \n",
    "\n",
    "#         # import pdb; pdb.set_trace()\n",
    "\n",
    "#         # self._data = [(line_[0], float(line_[1]), line_[2], float(line_[3])  ) for line_ in self._data]\n",
    "\n",
    "\n",
    "\n",
    "#         # self.names = [x[0] for x in self.data_map]\n",
    "#         # print(self.names[0:10])\n",
    "    \n",
    "#     def _read_timeseries(self, ts_filename, time_bound=None):\n",
    "        \n",
    "#         ret = []\n",
    "#         with open(os.path.join(self._dataset_dir, ts_filename), \"r\") as tsfile:\n",
    "#             header = tsfile.readline().strip().split(',')\n",
    "#             assert header[0] == \"Hours\"\n",
    "#             for line in tsfile:\n",
    "#                 mas = line.strip().split(',')\n",
    "#                 if time_bound is not None:\n",
    "#                     t = float(mas[0])\n",
    "#                     if t > time_bound + 1e-6:\n",
    "#                         break\n",
    "#                 ret.append(np.array(mas))\n",
    "#         return (np.stack(ret), header)\n",
    "    \n",
    "#     def read_by_file_name(self, index, time, time_bound=None):\n",
    "#         print(\"index\", index)\n",
    "#         if 'length-of-stay' or 'decompensation' in self._dataset_dir:\n",
    "#             # for x in self.data_map:\n",
    "#             #     if x[0] == index:\n",
    "#             #         entry = x \n",
    "#             #         break\n",
    "#             entry = next((x for x in self.data_map if x[0] == index and x[1] ==  time), None)\n",
    "#             print(\"entry\", entry)\n",
    "#             if entry is None:\n",
    "#                 raise ValueError(f\"Entry with name {index} not found\")\n",
    "            \n",
    "#             t = float(entry[1])  # time is the second element in the tuple\n",
    "#             stay_id = int(entry[2])  # stay_id is the third element\n",
    "#             y = entry[3]  # labels are the fourth element\n",
    "#             print(\"this is entry 3\", y)\n",
    "#             (X, header) = self._read_timeseries(index, time_bound=time_bound if time_bound is not None else t)\n",
    "#         else:     \n",
    "#             t = self.data_map[index]['time'] if time_bound is None else time_bound\n",
    "#             y = self.data_map[index]['labels']\n",
    "#             stay_id = self.data_map[index]['stay_id']\n",
    "#             (X, header) = self._read_timeseries(index, time_bound=time_bound)\n",
    "\n",
    "#         return {\"X\": X,\n",
    "#                 \"t\": t,\n",
    "#                 \"y\": y,\n",
    "#                 'stay_id': stay_id,\n",
    "#                 \"header\": header,\n",
    "#                 \"name\": index}\n",
    "\n",
    "#     def get_decomp_los(self, index, time_bound=None):\n",
    "#         # name = self._data[index][0]\n",
    "#         # time_bound = self._data[index][1]\n",
    "#         # ys = self._data[index][3]\n",
    "\n",
    "#         # (data, header) = self._read_timeseries(index, time_bound=time_bound)\n",
    "#         # data = self.discretizer.transform(data, end=time_bound)[0] \n",
    "#         # if (self.normalizer is not None):\n",
    "#         #     data = self.normalizer.transform(data)\n",
    "#         # ys = np.array(ys, dtype=np.int32) if len(ys) > 1 else np.array(ys, dtype=np.int32)[0]\n",
    "#         # return data, ys\n",
    "\n",
    "#         # data, ys = \n",
    "#         return self.__getitem__(index, time_bound)\n",
    "\n",
    "\n",
    "#     def __getitem__(self, index,  time_bound=None):\n",
    "#         if 'length-of-stay' or 'decompensation' in self._dataset_dir: \n",
    "#             if isinstance(index, int):\n",
    "#                 time = self.times[index]\n",
    "#                 index = self.names[index]\n",
    "#         else:\n",
    "#             if isinstance(index, int):\n",
    "#                 index = self.names[index]\n",
    "#                 time = None\n",
    "                \n",
    "#         ret = self.read_by_file_name(index, time, time_bound)\n",
    "#         data = ret[\"X\"]\n",
    "#         ts = ret[\"t\"] if ret['t'] > 0.0 else self._period_length\n",
    "#         ys = ret[\"y\"]\n",
    "#         print(\"this is ys\" , ys)\n",
    "#         names = ret[\"name\"]\n",
    "#         data = self.discretizer.transform(data, end=ts)[0] \n",
    "#         if (self.normalizer is not None):\n",
    "#             data = self.normalizer.transform(data)\n",
    "#         if 'length-of-stay' in self._dataset_dir:\n",
    "#             ys = np.array(ys, dtype=np.float32) if len(ys) > 1 else np.array(ys, dtype=np.float32)[0]\n",
    "#         else:\n",
    "#             ys = np.array(ys, dtype=np.int32) if len(ys) > 1 else np.array(ys, dtype=np.int32)[0]\n",
    "#         return data, ys\n",
    "\n",
    "    \n",
    "#     def __len__(self):\n",
    "#         return len(self.names)\n",
    "\n",
    "\n",
    "# def get_datasets(discretizer, normalizer, args):\n",
    "#     train_ds = EHRdataset(discretizer, normalizer, f'{args.ehr_data_dir}/{args.task}/train_listfile.csv', os.path.join(args.ehr_data_dir, f'{args.task}/train'))\n",
    "#     val_ds = EHRdataset(discretizer, normalizer, f'{args.ehr_data_dir}/{args.task}/val_listfile.csv', os.path.join(args.ehr_data_dir, f'{args.task}/train'))\n",
    "#     test_ds = EHRdataset(discretizer, normalizer, f'{args.ehr_data_dir}/{args.task}/test_listfile.csv', os.path.join(args.ehr_data_dir, f'{args.task}/test'))\n",
    "#     return train_ds, val_ds, test_ds\n",
    "\n",
    "# def get_data_loader(discretizer, normalizer, dataset_dir, batch_size):\n",
    "#     train_ds, val_ds, test_ds = get_datasets(discretizer, normalizer, dataset_dir)\n",
    "#     train_dl = DataLoader(train_ds, batch_size, shuffle=True, collate_fn=my_collate, pin_memory=True, num_workers=16)\n",
    "#     val_dl = DataLoader(val_ds, batch_size, shuffle=False, collate_fn=my_collate, pin_memory=True, num_workers=16)\n",
    "\n",
    "#     return train_dl, val_dl\n",
    "        \n",
    "# def my_collate(batch):\n",
    "#     x = [item[0] for item in batch]\n",
    "#     x, seq_length = pad_zeros(x)\n",
    "#     targets = np.array([item[1] for item in batch])\n",
    "#     return [x, targets, seq_length]\n",
    "\n",
    "# def pad_zeros(arr, min_length=None):\n",
    "\n",
    "#     dtype = arr[0].dtype\n",
    "#     seq_length = [x.shape[0] for x in arr]\n",
    "#     max_len = max(seq_length)\n",
    "#     ret = [np.concatenate([x, np.zeros((max_len - x.shape[0],) + x.shape[1:], dtype=dtype)], axis=0)\n",
    "#            for x in arr]\n",
    "#     if (min_length is not None) and ret[0].shape[0] < min_length:\n",
    "#         ret = [np.concatenate([x, np.zeros((min_length - x.shape[0],) + x.shape[1:], dtype=dtype)], axis=0)\n",
    "#                for x in ret]\n",
    "#     return np.array(ret), seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "201834a9-83a6-4afc-9476-538679f35c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class EHRdataset(Dataset):\n",
    "#     def __init__(self, discretizer, normalizer, listfile, dataset_dir, return_names=True, period_length=48.0):\n",
    "#         self.return_names = return_names\n",
    "#         self.discretizer = discretizer\n",
    "#         self.normalizer = normalizer\n",
    "#         self._period_length = period_length\n",
    "\n",
    "#         self._dataset_dir = dataset_dir\n",
    "#         listfile_path = listfile\n",
    "#         with open(listfile_path, \"r\") as lfile:\n",
    "#             self._data = lfile.readlines()\n",
    "#         self._listfile_header = self._data[0]\n",
    "#         self.CLASSES = self._listfile_header.strip().split(',')[3:]\n",
    "#         self._data = self._data[1:]\n",
    "#         print(self._data[12])\n",
    "\n",
    "\n",
    "#         self._data = [line.split(',') for line in self._data]\n",
    "#         self.data_map = {\n",
    "#         mas[0]: {\n",
    "#             'labels': list(map(float, mas[3:])),\n",
    "#             'stay_id': float(mas[2]),\n",
    "#             'time': float(mas[1]),\n",
    "#             }\n",
    "#             for mas in self._data\n",
    "#         }\n",
    "#         self.names = list(self.data_map.keys())\n",
    "#         self.names = [[x[0],x[1]] for x in self.data_map]\n",
    "#         print(self.data_map[12])\n",
    "        \n",
    "\n",
    "#         # import pdb; pdb.set_trace()\n",
    "\n",
    "#         # self._data = [(line_[0], float(line_[1]), line_[2], float(line_[3])  ) for line_ in self._data]\n",
    "\n",
    "\n",
    "\n",
    "#         # self.names = [x[0] for x in self.data_map]\n",
    "#         # print(self.names[0:10])\n",
    "    \n",
    "#     def _read_timeseries(self, ts_filename, time_bound=None):\n",
    "        \n",
    "#         ret = []\n",
    "#         with open(os.path.join(self._dataset_dir, ts_filename), \"r\") as tsfile:\n",
    "#             header = tsfile.readline().strip().split(',')\n",
    "#             assert header[0] == \"Hours\"\n",
    "#             for line in tsfile:\n",
    "#                 mas = line.strip().split(',')\n",
    "#                 if time_bound is not None:\n",
    "#                     t = float(mas[0])\n",
    "#                     if t > time_bound + 1e-6:\n",
    "#                         break\n",
    "#                 ret.append(np.array(mas))\n",
    "#         return (np.stack(ret), header)\n",
    "    \n",
    "#     def read_by_file_name(self, index, time, time_bound=None):\n",
    "#         print(\"index\", index)\n",
    "#         if 'length-of-stay' or 'decompensation' in self._dataset_dir:\n",
    "#             # for x in self.data_map:\n",
    "#             #     if x[0] == index:\n",
    "#             #         entry = x \n",
    "#             #         break\n",
    "#             entry = next((x for x in self.data_map if x[0] == index and x[1] ==  time), None)\n",
    "#             print(\"entry\", entry)\n",
    "#             if entry is None:\n",
    "#                 raise ValueError(f\"Entry with name {index} not found\")\n",
    "            \n",
    "#             t = float(entry[1])  # time is the second element in the tuple\n",
    "#             stay_id = int(entry[2])  # stay_id is the third element\n",
    "#             y = entry[3]  # labels are the fourth element\n",
    "#             print(\"this is entry 3\", y)\n",
    "#             (X, header) = self._read_timeseries(index, time_bound=time_bound if time_bound is not None else t)\n",
    "#         else:     \n",
    "#             t = self.data_map[index]['time'] if time_bound is None else time_bound\n",
    "#             y = self.data_map[index]['labels']\n",
    "#             stay_id = self.data_map[index]['stay_id']\n",
    "#             (X, header) = self._read_timeseries(index, time_bound=time_bound)\n",
    "\n",
    "#         return {\"X\": X,\n",
    "#                 \"t\": t,\n",
    "#                 \"y\": y,\n",
    "#                 'stay_id': stay_id,\n",
    "#                 \"header\": header,\n",
    "#                 \"name\": index}\n",
    "\n",
    "#     def get_decomp_los(self, index, time_bound=None):\n",
    "#         # name = self._data[index][0]\n",
    "#         # time_bound = self._data[index][1]\n",
    "#         # ys = self._data[index][3]\n",
    "\n",
    "#         # (data, header) = self._read_timeseries(index, time_bound=time_bound)\n",
    "#         # data = self.discretizer.transform(data, end=time_bound)[0] \n",
    "#         # if (self.normalizer is not None):\n",
    "#         #     data = self.normalizer.transform(data)\n",
    "#         # ys = np.array(ys, dtype=np.int32) if len(ys) > 1 else np.array(ys, dtype=np.int32)[0]\n",
    "#         # return data, ys\n",
    "\n",
    "#         # data, ys = \n",
    "#         return self.__getitem__(index, time_bound)\n",
    "\n",
    "\n",
    "#     def __getitem__(self, index,  time_bound=None):\n",
    "#         if 'length-of-stay' or 'decompensation' in self._dataset_dir: \n",
    "#             if isinstance(index, int):\n",
    "#                 index, time = self.names[index]\n",
    "#         else:\n",
    "#             if isinstance(index, int):\n",
    "#                 index = self.names[index]\n",
    "#                 time = None\n",
    "                \n",
    "#         ret = self.read_by_file_name(index, time, time_bound)\n",
    "#         data = ret[\"X\"]\n",
    "#         ts = ret[\"t\"] if ret['t'] > 0.0 else self._period_length\n",
    "#         ys = ret[\"y\"]\n",
    "#         print(\"this is ys\" , ys)\n",
    "#         names = ret[\"name\"]\n",
    "#         data = self.discretizer.transform(data, end=ts)[0] \n",
    "#         if (self.normalizer is not None):\n",
    "#             data = self.normalizer.transform(data)\n",
    "#         if 'length-of-stay' in self._dataset_dir:\n",
    "#             ys = np.array(ys, dtype=np.float32) if len(ys) > 1 else np.array(ys, dtype=np.float32)[0]\n",
    "#         else:\n",
    "#             ys = np.array(ys, dtype=np.int32) if len(ys) > 1 else np.array(ys, dtype=np.int32)[0]\n",
    "#         return data, ys\n",
    "\n",
    "    \n",
    "#     def __len__(self):\n",
    "#         return len(self.names)\n",
    "\n",
    "\n",
    "# def get_datasets(discretizer, normalizer, args):\n",
    "#     train_ds = EHRdataset(discretizer, normalizer, f'{args.ehr_data_dir}/{args.task}/train_listfile.csv', os.path.join(args.ehr_data_dir, f'{args.task}/train'))\n",
    "#     val_ds = EHRdataset(discretizer, normalizer, f'{args.ehr_data_dir}/{args.task}/val_listfile.csv', os.path.join(args.ehr_data_dir, f'{args.task}/train'))\n",
    "#     test_ds = EHRdataset(discretizer, normalizer, f'{args.ehr_data_dir}/{args.task}/test_listfile.csv', os.path.join(args.ehr_data_dir, f'{args.task}/test'))\n",
    "#     return train_ds, val_ds, test_ds\n",
    "\n",
    "# def get_data_loader(discretizer, normalizer, dataset_dir, batch_size):\n",
    "#     train_ds, val_ds, test_ds = get_datasets(discretizer, normalizer, dataset_dir)\n",
    "#     train_dl = DataLoader(train_ds, batch_size, shuffle=True, collate_fn=my_collate, pin_memory=True, num_workers=16)\n",
    "#     val_dl = DataLoader(val_ds, batch_size, shuffle=False, collate_fn=my_collate, pin_memory=True, num_workers=16)\n",
    "\n",
    "#     return train_dl, val_dl\n",
    "        \n",
    "# def my_collate(batch):\n",
    "#     x = [item[0] for item in batch]\n",
    "#     x, seq_length = pad_zeros(x)\n",
    "#     targets = np.array([item[1] for item in batch])\n",
    "#     return [x, targets, seq_length]\n",
    "\n",
    "# def pad_zeros(arr, min_length=None):\n",
    "\n",
    "#     dtype = arr[0].dtype\n",
    "#     seq_length = [x.shape[0] for x in arr]\n",
    "#     max_len = max(seq_length)\n",
    "#     ret = [np.concatenate([x, np.zeros((max_len - x.shape[0],) + x.shape[1:], dtype=dtype)], axis=0)\n",
    "#            for x in arr]\n",
    "#     if (min_length is not None) and ret[0].shape[0] < min_length:\n",
    "#         ret = [np.concatenate([x, np.zeros((min_length - x.shape[0],) + x.shape[1:], dtype=dtype)], axis=0)\n",
    "#                for x in ret]\n",
    "#     return np.array(ret), seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "238b3f29-e4f5-4819-93f6-3ce9369594ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_datasets(discretizer, normalizer, args):\n",
    "#     transform = None\n",
    "#     # print(f'{args.ehr_data_root}/{args.task}/train_listfile.csv')\n",
    "#     # print(os.path.join(args.ehr_data_root, f'{args.task}/train'))\n",
    "#     train_ds = EHRdataset(discretizer, normalizer, f'{args.ehr_data_root}/{args.task}/train_listfile.csv', os.path.join(args.ehr_data_root, f'{args.task}/train'))\n",
    "#     val_ds = EHRdataset(discretizer, normalizer, f'{args.ehr_data_root}/{args.task}/val_listfile.csv', os.path.join(args.ehr_data_root, f'{args.task}/train'))\n",
    "#     test_ds = EHRdataset(discretizer, normalizer, f'{args.ehr_data_root}/{args.task}/test_listfile.csv', os.path.join(args.ehr_data_root, f'{args.task}/test'))\n",
    "#     return train_ds, val_ds, test_ds\n",
    "# ehr_train_ds, ehr_val_ds, ehr_test_ds = get_datasets(discretizer, normalizer, args)\n",
    "# print(len(ehr_train_ds[1][0]))\n",
    "# print(ehr_train_ds[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3a97a403-feb8-46e0-aa7a-9e5832c5d4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(ehr_test_ds[8])\n",
    "# for i in range(0, 10):  # Print labels for the first 10 entries, or less if the dataset is smaller\n",
    "#     _, labels = ehr_val_ds[i]\n",
    "#     print(f\"Labels for entry {i}: {float(labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "36f65ed5-544b-45e9-8f7b-6401fcf105e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_item = ehr_test_ds[2]\n",
    "# get_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "06486126-6e5a-4a2b-b430-9c4e61406331",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Clip(object):\n",
    "    \"\"\"Transformation to clip image values between 0 and 1.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        return torch.clip(sample, 0, 1)\n",
    "\n",
    "    \n",
    "\n",
    "class RandomCrop(object):\n",
    "    \"Randomly crop an image\"\n",
    "    \n",
    "    def __call__(self, sample):\n",
    "        resize = 256\n",
    "        #print(np.random.uniform(0.4*resize,resize,1))\n",
    "        random_crop_size = int(np.random.uniform(0.6*resize,resize,1))\n",
    "        sample=transforms.RandomCrop(random_crop_size)(sample)\n",
    "        return sample\n",
    "    \n",
    "    \n",
    "class RandomColorDistortion(object):\n",
    "    \"Apply random color distortions to the image\"\n",
    "    \n",
    "    def __call__(self, sample):\n",
    "        resize=256\n",
    "\n",
    "        # Random color distortion\n",
    "        strength = 1.0 # 1.0 imagenet setting and CIFAR uses 0.5\n",
    "        brightness = 0.8 * strength \n",
    "        contrast = 0.8 * strength\n",
    "        saturation = 0.8 * strength\n",
    "        hue = 0.2 * strength\n",
    "        prob = np.random.uniform(0,1,1) \n",
    "        if prob < 0.8:\n",
    "            sample=transforms.ColorJitter(brightness, contrast, saturation, hue)(sample)\n",
    "\n",
    "        # Random Grayscale\n",
    "        sample=transforms.RandomGrayscale(p=0.2)(sample)\n",
    "\n",
    "        # Gaussian blur also based on imagenet but not used for CIFAR\n",
    "        #prob = np.random.uniform(0,1,1)\n",
    "        #if prob < 0.3:\n",
    "        #    sample=transforms.GaussianBlur(kernel_size=resize//10)(sample)\n",
    "        #    sample=transforms.Pad(0)(sample)\n",
    "        return sample \n",
    "    \n",
    "\n",
    "def get_transforms_simclr(args):\n",
    "    normalize = transforms.Normalize([0.5, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    \n",
    "    train_transforms = []\n",
    "    # Resize all images to same size, then randomly crop and resize again\n",
    "    train_transforms.append(transforms.Resize([args.resize, args.resize]))\n",
    "    # Random affine\n",
    "    train_transforms.append(transforms.RandomAffine(degrees=(-45, 45), translate=(0.1,0.1), scale=(0.7, 1.5), shear=(-25, 25)))\n",
    "    # Random crop\n",
    "    train_transforms.append(RandomCrop())\n",
    "    # Resize again\n",
    "    # train_transforms.append(transforms.Resize([args.resize, args.resize], interpolation=3))\n",
    "    train_transforms.append(transforms.Resize([224, 224], interpolation=3))\n",
    "    # Random horizontal flip \n",
    "    train_transforms.append(transforms.RandomHorizontalFlip())\n",
    "    # Random color distortions\n",
    "    train_transforms.append(RandomColorDistortion())\n",
    "    # Convert to tensor\n",
    "    train_transforms.append(transforms.ToTensor())\n",
    "    # Clip values between 0 and 1 and normalize\n",
    "    #train_transforms.append(Clip())\n",
    "    #train_transforms.append(normalize)      \n",
    "\n",
    "    test_transforms = []\n",
    "    # Resize all images to same size, then center crop and resize again\n",
    "    test_transforms.append(transforms.Resize([args.resize, args.resize]))\n",
    "    crop_proportion=0.875\n",
    "    test_transforms.append(transforms.CenterCrop([int(0.875*args.resize), int(0.875*args.resize)]))\n",
    "    # test_transforms.append(transforms.Resize([args.resize, args.resize], interpolation=3))\n",
    "    test_transforms.append(transforms.Resize([224, 224], interpolation=3))\n",
    "    #Convert to tensor\n",
    "    test_transforms.append(transforms.ToTensor())\n",
    "    # Clip values between 0 and 1 and normalize\n",
    "    #test_transforms.append(Clip())\n",
    "    #test_transforms.append(normalize)\n",
    "\n",
    "    return train_transforms, test_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "73854423-e414-41eb-b4b3-a3a3571dd54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MIMICCXR(Dataset):\n",
    "    def __init__(self, paths, args, transform=None, split='train'):\n",
    "        self.data_dir = args.cxr_data_root\n",
    "        self.args = args\n",
    "        self.CLASSES  = ['Atelectasis', 'Cardiomegaly', 'Consolidation', 'Edema',\n",
    "       'Enlarged Cardiomediastinum', 'Fracture', 'Lung Lesion',\n",
    "       'Lung Opacity', 'No Finding', 'Pleural Effusion', 'Pleural Other',\n",
    "       'Pneumonia', 'Pneumothorax', 'Support Devices']\n",
    "        self.filenames_to_path = {path.split('/')[-1].split('.')[0]: path for path in paths}\n",
    "        # for item in self.filenames_to_path.items():\n",
    "        #     print (item)\n",
    "        #     break\n",
    "\n",
    "        metadata = pd.read_csv(f'{self.data_dir}/mimic-cxr-2.0.0-metadata.csv')\n",
    "        # print(metadata.keys())\n",
    "        labels = pd.read_csv(f'{self.data_dir}/mimic-cxr-2.0.0-chexpert.csv')\n",
    "        labels[self.CLASSES] = labels[self.CLASSES].fillna(0)\n",
    "        labels = labels.replace(-1.0, 0.0)\n",
    "        # print(labels.head)\n",
    "        # print(labels.keys())\n",
    "        \n",
    "        splits = pd.read_csv(f'{self.data_dir}/mimic-cxr-ehr-split.csv')\n",
    "\n",
    "\n",
    "        metadata_with_labels = metadata.merge(labels[self.CLASSES+['study_id'] ], how='inner', on='study_id')\n",
    "        # print(metadata_with_labels.keys())\n",
    "\n",
    "\n",
    "        self.filesnames_to_labels = dict(zip(metadata_with_labels['dicom_id'].values, metadata_with_labels[self.CLASSES].values))\n",
    "        # for item in self.filesnames_to_labels.items():\n",
    "        #     print (item)\n",
    "        #     break\n",
    "        self.filenames_loaded = splits.loc[splits.split==split]['dicom_id'].values\n",
    "        # print(self.filenames_loaded[1])\n",
    "        # exclude any files included in the split.csv file but not in the chexpert.csv (i.e. with no labels)\n",
    "        self.filenames_loaded = [filename  for filename in self.filenames_loaded if filename in self.filesnames_to_labels]\n",
    "        # print(self.filenames_loaded[1])\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # print(index)\n",
    "        if isinstance(index, str):\n",
    "            img = Image.open(self.filenames_to_path[index]).convert('RGB')\n",
    "            labels = torch.tensor(self.filesnames_to_labels[index]).float()\n",
    "            if self.transform is not None:\n",
    "                img = self.transform(img)\n",
    "            return img, labels\n",
    "          \n",
    "        \n",
    "        filename = self.filenames_loaded[index]\n",
    "        img = Image.open(self.filenames_to_path[filename]).convert('RGB')\n",
    "        labels = torch.tensor(self.filesnames_to_labels[filename]).float()\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        return img, labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.filenames_loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9a3df713-d0a4-4c95-afbf-15a2b2401c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cxr_datasets(args):\n",
    "    if args.transforms_cxr=='simclrv2':\n",
    "        # print(\"Appling SimCLR image transforms...\")\n",
    "        train_transforms, test_transforms = get_transforms_simclr(args)\n",
    "\n",
    "    data_dir = args.cxr_data_root\n",
    "    # print(data_dir)\n",
    "    filepath = f'{args.cxr_data_root}/new_paths.npy'\n",
    "    if os.path.exists(filepath):\n",
    "        paths = np.load(filepath)\n",
    "        # print(len(paths))\n",
    "    else:\n",
    "        paths = glob.glob(f'{data_dir}/resized/**/*.jpg', recursive = True)\n",
    "        np.save(filepath, paths)\n",
    "    \n",
    "    dataset_train = MIMICCXR(paths, args, split='train', transform=transforms.Compose(train_transforms))\n",
    "    dataset_validate = MIMICCXR(paths, args, split='validate', transform=transforms.Compose(test_transforms),)\n",
    "    dataset_test = MIMICCXR(paths, args, split='test', transform=transforms.Compose(test_transforms),)\n",
    "\n",
    "    return dataset_train , dataset_validate, dataset_test\n",
    "\n",
    "cxr_train_ds, cxr_val_ds, cxr_test_ds = get_cxr_datasets(args)\n",
    "# cxr_train_ds =  get_cxr_datasets(args)\n",
    "# get_item = next(iter((cxr_train_ds)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "40516063-16ed-462c-89d1-f5fc2b0276ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSES = [\n",
    "       'Acute and unspecified renal failure', 'Acute cerebrovascular disease',\n",
    "       'Acute myocardial infarction', 'Cardiac dysrhythmias',\n",
    "       'Chronic kidney disease',\n",
    "       'Chronic obstructive pulmonary disease and bronchiectasis',\n",
    "       'Complications of surgical procedures or medical care',\n",
    "       'Conduction disorders', 'Congestive heart failure; nonhypertensive',\n",
    "       'Coronary atherosclerosis and other heart disease',\n",
    "       'Diabetes mellitus with complications',\n",
    "       'Diabetes mellitus without complication',\n",
    "       'Disorders of lipid metabolism', 'Essential hypertension',\n",
    "       'Fluid and electrolyte disorders', 'Gastrointestinal hemorrhage',\n",
    "       'Hypertension with complications and secondary hypertension',\n",
    "       'Other liver diseases', 'Other lower respiratory disease',\n",
    "       'Other upper respiratory disease',\n",
    "       'Pleurisy; pneumothorax; pulmonary collapse',\n",
    "       'Pneumonia (except that caused by tuberculosis or sexually transmitted disease)',\n",
    "       'Respiratory failure; insufficiency; arrest (adult)',\n",
    "       'Septicemia (except in labor)', 'Shock'\n",
    "    ]\n",
    "\n",
    "class MIMIC_CXR_EHR(Dataset):\n",
    "    def __init__(self, args, metadata_with_labels, ehr_ds, cxr_ds, split='train'):\n",
    "        \n",
    "        self.CLASSES = CLASSES\n",
    "        \n",
    "        self.metadata_with_labels = metadata_with_labels\n",
    "        \n",
    "        self.cxr_files_paired = self.metadata_with_labels.dicom_id.values\n",
    "        self.ehr_files_paired = (self.metadata_with_labels['stay'].values)\n",
    "        self.time_diff = self.metadata_with_labels.time_diff\n",
    "        \n",
    "        self.cxr_files_all = cxr_ds.filenames_loaded\n",
    "        self.ehr_files_all = ehr_ds.names\n",
    "        \n",
    "        self.ehr_files_unpaired = list(set(self.ehr_files_all) - set(self.ehr_files_paired))\n",
    "        \n",
    "        self.ehr_ds = ehr_ds\n",
    "        self.cxr_ds = cxr_ds\n",
    "        \n",
    "        self.args = args\n",
    "        self.split = split\n",
    "        self.data_ratio = self.args.data_ratio if split=='train' else 1.0\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.args.data_pairs == 'paired':\n",
    "            cxr_data, labels_cxr = self.cxr_ds[self.cxr_files_paired[index]]\n",
    "            ehr_data, labels_ehr = self.ehr_ds[self.ehr_files_paired[index]]\n",
    "            time_diff = self.metadata_with_labels.iloc[index].time_diff                      \n",
    "            return ehr_data, cxr_data, labels_ehr, labels_cxr\n",
    "  \n",
    "    def __len__(self):\n",
    "        if self.args.data_pairs == 'paired':\n",
    "            return len(self.ehr_files_paired)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "11efd8f1-1fa4-4a2e-ad85-57e5498fb57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadmetadata(args):\n",
    "\n",
    "    data_dir = args.cxr_data_root\n",
    "    cxr_metadata = pd.read_csv(f'{data_dir}/mimic-cxr-2.0.0-metadata.csv')\n",
    "    print('Number of CXR images=', len(cxr_metadata))\n",
    "    icu_stay_metadata = pd.read_csv(f'{args.ehr_data_root}/root/all_stays.csv')\n",
    "    print('Number of ICU stays=', len(icu_stay_metadata))\n",
    "    columns = ['subject_id', 'stay_id', 'intime', 'outtime']\n",
    "    \n",
    "    # only common subjects with both icu stay and an xray\n",
    "    # Note that inner merge includes rows if a chest X-ray is associated with multiple stays\n",
    "    cxr_merged_icustays = cxr_metadata.merge(icu_stay_metadata[columns], how='inner', on='subject_id')\n",
    "    # print(cxr_merged_icustays.keys())\n",
    "    print('Number of CXR associated with ICU stay based on subject ID=', len(cxr_merged_icustays))\n",
    "    print('Number of unique CXR dicoms=', len(cxr_merged_icustays.dicom_id.unique()))\n",
    "    print('Number of unique CXR study id=', len(cxr_merged_icustays.study_id.unique()))\n",
    "        \n",
    "    # combine study date time\n",
    "    # just changing the format and combining study date and study time to study datetime\n",
    "    cxr_merged_icustays['StudyTime'] = cxr_merged_icustays['StudyTime'].apply(lambda x: f'{int(float(x)):06}' )\n",
    "    cxr_merged_icustays['StudyDateTime'] = pd.to_datetime(cxr_merged_icustays['StudyDate'].astype(str) + ' ' + cxr_merged_icustays['StudyTime'].astype(str) ,format=\"%Y%m%d %H%M%S\")\n",
    "    print(cxr_merged_icustays.head)\n",
    "    # note that study datetime is for cxr images and intime/outtime are for the icu stays\n",
    "    cxr_merged_icustays.intime=pd.to_datetime(cxr_merged_icustays.intime)\n",
    "    cxr_merged_icustays.outtime=pd.to_datetime(cxr_merged_icustays.outtime)\n",
    "    end_time = cxr_merged_icustays.outtime\n",
    "    \n",
    "    cxr_merged_icustays['time_diff'] = cxr_merged_icustays.StudyDateTime-cxr_merged_icustays.intime\n",
    "    cxr_merged_icustays['time_diff'] = cxr_merged_icustays['time_diff'].apply(lambda x: np.round(x.total_seconds()/60/60,3))\n",
    "    \n",
    "    # either only take the latest CXR related with an icu stay or take all CXR images associated with icu stays\n",
    "    # For LE/ FT  (evaluation datasets)\n",
    "    if (args.dataset!='all'):\n",
    "        cxr_merged_icustays_during = cxr_merged_icustays.loc[(cxr_merged_icustays.StudyDateTime>=cxr_merged_icustays.intime)&((cxr_merged_icustays.StudyDateTime<=end_time))]\n",
    "\n",
    "        if args.task == 'decompensation':\n",
    "            print('Only include the last CXR before the current time of prediction')\n",
    "            cxr_merged_icustays_during = cxr_merged_icustays.loc[(cxr_merged_icustays.StudyDateTime<=cxr_merged_icustays.intime)]\n",
    "        \n",
    "        if args.task == 'in-hospital-mortality':\n",
    "            print(\"Excluding chest X-rays beyond 48 hours for in-hospital mortality\")\n",
    "            end_time = cxr_merged_icustays.intime + pd.DateOffset(hours=48)\n",
    "            cxr_merged_icustays_during = cxr_merged_icustays.loc[(cxr_merged_icustays.StudyDateTime>=cxr_merged_icustays.intime)&((cxr_merged_icustays.StudyDateTime<=end_time))]\n",
    "        \n",
    "        # select cxrs with the ViewPosition == 'AP'\n",
    "        cxr_merged_icustays_AP = cxr_merged_icustays_during[cxr_merged_icustays_during['ViewPosition'] == 'AP']\n",
    "        # print(cxr_merged_icustays_AP.loc[cxr_merged_icustays_AP.stay_id==30001947])\n",
    "        # returns the indices of records with the same stay_id\n",
    "        groups = cxr_merged_icustays_AP.groupby('stay_id')\n",
    "        # print(groups.groups)\n",
    "\n",
    "        groups_selected = []\n",
    "        for group in groups:\n",
    "            # select the latest cxr for the icu stay\n",
    "            selected = group[1].sort_values('StudyDateTime').tail(1).reset_index()\n",
    "            groups_selected.append(selected)\n",
    "        groups = pd.concat(groups_selected, ignore_index=True)\n",
    "        # print(groups.head)\n",
    "    \n",
    "    # For SIMCLR pretraining (large dataset)\n",
    "    else:\n",
    "        print(cxr_merged_icustays.ViewPosition.unique())\n",
    "        cxr_merged_icustays_AP = cxr_merged_icustays[cxr_merged_icustays['ViewPosition'] == 'AP']\n",
    "        print(\"Number of CXR associated with ICU stay and in AP view=\", len(cxr_merged_icustays_AP))\n",
    "        groups = cxr_merged_icustays_AP\n",
    "        \n",
    "    print(\"Mean time cxr - intime= \", groups.time_diff.mean())\n",
    "    print(\"Minimum time =\", groups.time_diff.min())\n",
    "    print(\"Maximum time =\", groups.time_diff.max())\n",
    "\n",
    "#     plt.hist(groups.time_diff.apply(lambda x: x.days).astype(\"float64\"))\n",
    "#     plt.xlabel('Time difference in days')\n",
    "#     plt.show()\n",
    "\n",
    "    #print(groups.iloc[0])\n",
    "    return groups\n",
    "\n",
    "def my_collate(batch):\n",
    "    x = [item[0] for item in batch]\n",
    "    pairs = [False if item[1] is None else True for item in batch]\n",
    "    img = torch.stack([torch.zeros(3, 224, 224) if item[1] is None else item[1] for item in batch])\n",
    "    x, seq_length = pad_zeros(x)\n",
    "    targets_ehr = np.array([item[2] for item in batch])\n",
    "    targets_cxr = torch.stack([torch.zeros(14) if item[3] is None else item[3] for item in batch])\n",
    "    return [x, img, targets_ehr, targets_cxr, seq_length, pairs]\n",
    "    \n",
    "\n",
    "def pad_zeros(arr, min_length=None):\n",
    "    dtype = arr[0].dtype\n",
    "    seq_length = [x.shape[0] for x in arr]\n",
    "    max_len = max(seq_length)\n",
    "    ret = [np.concatenate([x, np.zeros((max_len - x.shape[0],) + x.shape[1:], dtype=dtype)], axis=0)\n",
    "           for x in arr]\n",
    "    if (min_length is not None) and ret[0].shape[0] < min_length:\n",
    "        ret = [np.concatenate([x, np.zeros((min_length - x.shape[0],) + x.shape[1:], dtype=dtype)], axis=0)\n",
    "               for x in ret]\n",
    "    return np.array(ret), seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0a4dc7d8-72e9-4a8d-bcd9-2f95cb62191d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CXR images= 377110\n",
      "Number of ICU stays= 59372\n",
      "Number of CXR associated with ICU stay based on subject ID= 368350\n",
      "Number of unique CXR dicoms= 181195\n",
      "Number of unique CXR study id= 122087\n",
      "<bound method NDFrame.head of                                             dicom_id  subject_id  study_id  \\\n",
      "0       02aa804e-bde0afdd-112c0b34-7bc16630-4e384014    10000032  50414267   \n",
      "1       174413ec-4ec4c1f7-34ea26b7-c5f994f8-79ef1962    10000032  50414267   \n",
      "2       2a2277a9-b0ded155-c0de8eb9-c124d10e-82c5caab    10000032  53189527   \n",
      "3       e084de3b-be89b11e-20fe3f9f-9c8d8dfe-4cfd202c    10000032  53189527   \n",
      "4       68b5c4b1-227d0485-9cc38c3f-7b84ab51-4b472714    10000032  53911762   \n",
      "...                                              ...         ...       ...   \n",
      "368345  ee9155f3-944c056b-c76c73d0-3f792f2c-92ae461e    19999442  58497551   \n",
      "368346  16b6c70f-6d36bd77-89d2fef4-9c4b8b0a-79c69135    19999442  58708861   \n",
      "368347  58766883-376a15ce-3b323a28-6af950a0-16b793bd    19999987  55368167   \n",
      "368348  7ba273af-3d290f8d-e28d0ab4-484b7a86-7fc12b08    19999987  58621812   \n",
      "368349  1a1fe7e3-cbac5d93-b339aeda-86bb86b5-4f31e82e    19999987  58971208   \n",
      "\n",
      "       PerformedProcedureStepDescription ViewPosition  Rows  Columns  \\\n",
      "0                     CHEST (PA AND LAT)           PA  3056     2544   \n",
      "1                     CHEST (PA AND LAT)      LATERAL  3056     2544   \n",
      "2                     CHEST (PA AND LAT)           PA  3056     2544   \n",
      "3                     CHEST (PA AND LAT)      LATERAL  3056     2544   \n",
      "4                    CHEST (PORTABLE AP)           AP  2705     2539   \n",
      "...                                  ...          ...   ...      ...   \n",
      "368345               CHEST (PORTABLE AP)           AP  2544     3056   \n",
      "368346               CHEST (PORTABLE AP)           AP  2544     3056   \n",
      "368347               CHEST (PORTABLE AP)           AP  2544     3056   \n",
      "368348               CHEST (PORTABLE AP)           AP  3056     2544   \n",
      "368349               CHEST (PORTABLE AP)           AP  3056     2544   \n",
      "\n",
      "        StudyDate StudyTime ProcedureCodeSequence_CodeMeaning  \\\n",
      "0        21800506    213014                CHEST (PA AND LAT)   \n",
      "1        21800506    213014                CHEST (PA AND LAT)   \n",
      "2        21800626    165500                CHEST (PA AND LAT)   \n",
      "3        21800626    165500                CHEST (PA AND LAT)   \n",
      "4        21800723    080556               CHEST (PORTABLE AP)   \n",
      "...           ...       ...                               ...   \n",
      "368345   21481128    133244               CHEST (PORTABLE AP)   \n",
      "368346   21481119    224703               CHEST (PORTABLE AP)   \n",
      "368347   21451104    051448               CHEST (PORTABLE AP)   \n",
      "368348   21451102    202809               CHEST (PORTABLE AP)   \n",
      "368349   21451103    050507               CHEST (PORTABLE AP)   \n",
      "\n",
      "       ViewCodeSequence_CodeMeaning  \\\n",
      "0                  postero-anterior   \n",
      "1                           lateral   \n",
      "2                  postero-anterior   \n",
      "3                           lateral   \n",
      "4                  antero-posterior   \n",
      "...                             ...   \n",
      "368345             antero-posterior   \n",
      "368346             antero-posterior   \n",
      "368347             antero-posterior   \n",
      "368348             antero-posterior   \n",
      "368349             antero-posterior   \n",
      "\n",
      "       PatientOrientationCodeSequence_CodeMeaning   stay_id  \\\n",
      "0                                           Erect  39553978   \n",
      "1                                           Erect  39553978   \n",
      "2                                           Erect  39553978   \n",
      "3                                           Erect  39553978   \n",
      "4                                             NaN  39553978   \n",
      "...                                           ...       ...   \n",
      "368345                                        NaN  32336619   \n",
      "368346                                      Erect  32336619   \n",
      "368347                                      Erect  36195440   \n",
      "368348                                      Erect  36195440   \n",
      "368349                                  Recumbent  36195440   \n",
      "\n",
      "                     intime              outtime       StudyDateTime  \n",
      "0       2180-07-23 14:00:00  2180-07-23 23:50:47 2180-05-06 21:30:14  \n",
      "1       2180-07-23 14:00:00  2180-07-23 23:50:47 2180-05-06 21:30:14  \n",
      "2       2180-07-23 14:00:00  2180-07-23 23:50:47 2180-06-26 16:55:00  \n",
      "3       2180-07-23 14:00:00  2180-07-23 23:50:47 2180-06-26 16:55:00  \n",
      "4       2180-07-23 14:00:00  2180-07-23 23:50:47 2180-07-23 08:05:56  \n",
      "...                     ...                  ...                 ...  \n",
      "368345  2148-11-19 14:23:43  2148-11-26 13:12:15 2148-11-28 13:32:44  \n",
      "368346  2148-11-19 14:23:43  2148-11-26 13:12:15 2148-11-19 22:47:03  \n",
      "368347  2145-11-02 22:59:00  2145-11-04 21:29:30 2145-11-04 05:14:48  \n",
      "368348  2145-11-02 22:59:00  2145-11-04 21:29:30 2145-11-02 20:28:09  \n",
      "368349  2145-11-02 22:59:00  2145-11-04 21:29:30 2145-11-03 05:05:07  \n",
      "\n",
      "[368350 rows x 16 columns]>\n",
      "Mean time cxr - intime=  68.3979428307123\n",
      "Minimum time = 0.009\n",
      "Maximum time = 2368.942\n",
      "Excluding CXR with missing radiology reports =  7756\n",
      "7756\n",
      "882\n",
      "2166\n"
     ]
    }
   ],
   "source": [
    "def load_cxr_ehr(args, ehr_train_ds, ehr_val_ds, cxr_train_ds, cxr_val_ds, ehr_test_ds, cxr_test_ds):\n",
    "    \n",
    "    # Load cxr and ehr groups\n",
    "    cxr_merged_icustays = loadmetadata(args) \n",
    "    \n",
    "    # Add the labels based on the EHR splits \n",
    "    splits_labels_train = pd.read_csv(f'{args.ehr_data_root}/{args.task}/train_listfile.csv')\n",
    "    splits_labels_val = pd.read_csv(f'{args.ehr_data_root}/{args.task}/val_listfile.csv')\n",
    "    splits_labels_test = pd.read_csv(f'{args.ehr_data_root}/{args.task}/test_listfile.csv')\n",
    "    \n",
    "    # split the groups from cxr_merged_icustays based on the labels from EHR \n",
    "    #TODO: investigate why total size of cxr_merged_icustays drops after the three steps below\n",
    "    train_meta_with_labels = cxr_merged_icustays.merge(splits_labels_train, how='inner', on='stay_id')#change dataset size here\n",
    "    # print(len(train_meta_with_labels))\n",
    "    val_meta_with_labels = cxr_merged_icustays.merge(splits_labels_val, how='inner', on='stay_id')\n",
    "    test_meta_with_labels = cxr_merged_icustays.merge(splits_labels_test, how='inner', on='stay_id')\n",
    "    \n",
    "    # Get rid of chest X-rays that don't have radiology reports\n",
    "    # get the x-ray images and their lables, split them based on the prev split on ehr \n",
    "    metadata = pd.read_csv(f'{args.cxr_data_root}/mimic-cxr-2.0.0-metadata.csv')\n",
    "    labels = pd.read_csv(f'{args.cxr_data_root}/mimic-cxr-2.0.0-chexpert.csv')\n",
    "    metadata_with_labels = metadata.merge(labels[['study_id']], how='inner', on='study_id').drop_duplicates(subset=['dicom_id'])\n",
    "    train_meta_with_labels = train_meta_with_labels.merge(metadata_with_labels[['dicom_id']], how='inner', on='dicom_id')\n",
    "    val_meta_with_labels = val_meta_with_labels.merge(metadata_with_labels[['dicom_id']], how='inner', on='dicom_id')\n",
    "    test_meta_with_labels = test_meta_with_labels.merge(metadata_with_labels[['dicom_id']], how='inner', on='dicom_id')\n",
    "    \n",
    "    print(\"Excluding CXR with missing radiology reports = \",len(train_meta_with_labels))\n",
    "\n",
    "    # Multimodal class\n",
    "    train_ds = MIMIC_CXR_EHR(args, train_meta_with_labels, ehr_train_ds, cxr_train_ds)\n",
    "    print(len(train_ds))\n",
    "    val_ds = MIMIC_CXR_EHR(args, val_meta_with_labels, ehr_val_ds, cxr_val_ds, split='val')\n",
    "    print(len(val_ds))\n",
    "    test_ds = MIMIC_CXR_EHR(args, test_meta_with_labels, ehr_test_ds, cxr_test_ds, split='test')\n",
    "    print(len(test_ds))\n",
    "    \n",
    "    collate = my_collate\n",
    "    \n",
    "    # Multimodal data loader \n",
    "    train_dl = DataLoader(train_ds, args.batch_size, shuffle=True, collate_fn=collate, drop_last=True) #pin_memory=True, num_workers=16,\n",
    "    val_dl = DataLoader(val_ds, args.batch_size, shuffle=False, collate_fn=collate, drop_last=False) #pin_memory=True, num_workers=16,\n",
    "    test_dl = DataLoader(test_ds, args.batch_size, shuffle=False, collate_fn=collate, drop_last=False) # pin_memory=True,num_workers=16,\n",
    "\n",
    "    return train_dl, val_dl, test_dl\n",
    "\n",
    "train_dl, val_dl, test_dl = load_cxr_ehr(args, ehr_train_ds, ehr_val_ds, cxr_train_ds, cxr_val_ds, ehr_test_ds, cxr_test_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c1512b-1b59-44c5-8c60-686932cd08f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ehr_data, cxr_data, ehr_labels, cxr_labels, seq_lengths, pairs = next(iter(train_dl))\n",
    "print(ehr_data.shape)\n",
    "print(seq_lengths)\n",
    "print(cxr_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30be461a-4af7-4742-af6d-0a72c4bb16b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Pytorch \n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint, Callback, TQDMProgressBar\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import math\n",
    "\n",
    "# Import other useful libraries\n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import pickle\n",
    "from flash.core.optimizers import LARS\n",
    "import os\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Import custom libraries/functions\n",
    "from encoders import LSTM, CXRModels\n",
    "import load_tasks as tasks\n",
    "from fusion_models import Fusion\n",
    "class SimCLR(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, args, train_dl):\n",
    "\n",
    "        super().__init__()\n",
    "        assert args.temperature > 0.0, 'The temperature must be a positive float!'\n",
    "        self.warmup_epochs= 10 #int(0.05*max_epochs) (10 as in SimCLR)\n",
    "        self.automatic_optimization = False\n",
    "        \n",
    "        self.num_train_batches=len(train_dl)\n",
    "        self.batch_size=args.batch_size\n",
    "        hidden_dim=args.hidden_dim\n",
    "        self.args=args\n",
    "        self.LABEL_COLUMNS = tasks.load_labels(args.task)\n",
    "        self.task = args.task\n",
    "        \n",
    "        # Load the architecture based on args\n",
    "        self.model = Fusion(args)\n",
    "        self.load_weights()\n",
    "        self.freeze_weights()\n",
    "        \n",
    "               \n",
    "    def load_weights(self):\n",
    "        # loads both encoders for simclr\n",
    "        load_dir_simclr = self.args.save_dir\n",
    "\n",
    "        if self.args.load_state is not None: \n",
    "            # why this condition?\n",
    "            # if 'LC' not in self.args.load_state:\n",
    "            #     if 'mortality' in self.args.save_dir:\n",
    "            #         load_dir_simclr = load_dir_simclr.replace('mortality', 'phenotyping')\n",
    "                    \n",
    "            if 'epoch' in self.args.load_state:\n",
    "                model_dir='/'+self.args.load_state.split('_epoch')[0] + '/'\n",
    "                if self.args.tag == 'eval_epoch':\n",
    "                    checkpoint = torch.load(load_dir_simclr + model_dir + self.args.load_state+\".ckpt\", map_location=\"cpu\")\n",
    "                else:\n",
    "                    checkpoint = torch.load(load_dir_simclr + model_dir + self.args.load_state+\".ckpt\")    \n",
    "            else:\n",
    "                if self.args.tag == 'eval_epoch':\n",
    "                    checkpoint = torch.load(os.path.join(load_dir_simclr, self.args.load_state+\".ckpt\"), map_location=\"cpu\")\n",
    "                else:\n",
    "                    checkpoint = torch.load(os.path.join(load_dir_simclr, self.args.load_state+\".ckpt\"))\n",
    "            own_state = self.model.state_dict()\n",
    "            own_keys = list(own_state.keys())\n",
    "            checkpoint_keys = list(checkpoint['state_dict'].keys())\n",
    "            \n",
    "            print('Total number of checkpoint params = {}'.format(len(checkpoint_keys)))\n",
    "            print('Total number of current model params = {}'.format(len(own_keys)))\n",
    "\n",
    "            count = 0\n",
    "            changed = []\n",
    "            for name in own_keys:\n",
    "                if name not in checkpoint_keys:\n",
    "                    # double check if name exists in a different format\n",
    "                    for x in checkpoint_keys:\n",
    "                        if name in x:\n",
    "                            param=checkpoint['state_dict'][x]\n",
    "                            if isinstance(param, torch.nn.Parameter):\n",
    "                                param=param.data\n",
    "                            own_state[name].copy_(param)\n",
    "                            count+=1\n",
    "                else:\n",
    "                    param=checkpoint['state_dict'][name]\n",
    "                    if isinstance(param, torch.nn.Parameter):\n",
    "                        param=param.data\n",
    "                    own_state[name].copy_(param)\n",
    "                    count+=1\n",
    "            print('Total number params loaded for model weights = {}'.format(count))\n",
    "        \n",
    "    def freeze_weights(self):\n",
    "        if self.args.finetune:\n",
    "            if 'ehr' not in self.args.fusion_type:\n",
    "                print(\"freezing cxr projection head\")\n",
    "                self.freeze(self.model.cxr_model_g)\n",
    "            if 'cxr' not in self.args.fusion_type: \n",
    "                print(\"freezing ehr projection head\")   \n",
    "                self.freeze(self.model.ehr_model_g)\n",
    "        else: \n",
    "            if 'lineareval' in self.args.fusion_type:\n",
    "                print('freezing encoders')\n",
    "                if 'ehr' not in self.args.fusion_type:\n",
    "                    self.freeze(self.model.cxr_model)\n",
    "                    self.freeze(self.model.cxr_model_g)\n",
    "                if 'cxr' not in self.args.fusion_type:\n",
    "                    self.freeze(self.model.ehr_model)\n",
    "                    self.freeze(self.model.ehr_model_g) \n",
    "\n",
    "    def freeze_weights_2(self):\n",
    "        if self.args.finetune:\n",
    "            if 'ehr' not in self.args.fusion_type:\n",
    "                print(\"finetuning\")\n",
    "        else: \n",
    "            if 'lineareval' in self.args.fusion_type:\n",
    "                print('freezing encoders')\n",
    "                if 'ehr' not in self.args.fusion_type and 'cxr' not in self.args.fusion_type:\n",
    "                    self.freeze(self.model.cxr_model)\n",
    "                    self.freeze(self.model.ehr_model)\n",
    "                if 'ehr' not in self.args.fusion_type:\n",
    "                    self.freeze(self.model.cxr_model)\n",
    "                if 'cxr' not in self.args.fusion_type:\n",
    "                    self.freeze(self.model.ehr_model)\n",
    "\n",
    "    def freeze(self, model):\n",
    "        for p in model.parameters():\n",
    "            p.requires_grad = False     \n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        \n",
    "        if self.args.fusion_type == 'None':\n",
    "            # Scaled learning rate in case of multiple GPUs\n",
    "            if self.args.num_gpu > 1:\n",
    "                effective_batchsize = self.args.batch_size*self.args.num_gpu\n",
    "                scaled_lr = self.args.lr*effective_batchsize/self.args.batch_size\n",
    "            else:\n",
    "                scaled_lr = self.args.lr \n",
    "                        \n",
    "            # Optimizer\n",
    "            optimizer = LARS(self.parameters(), lr=scaled_lr, momentum=0.9, weight_decay=self.args.weight_decay)\n",
    "            \n",
    "            # Note that the order of the below affects the initial starting learning rate, hence do not change.\n",
    "            # Main scheduler\n",
    "            mainscheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, 500, verbose=False)\n",
    "            # Learning rate warmup\n",
    "            lambda1= lambda epoch : (epoch+1)/self.warmup_epochs\n",
    "            warmupscheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lambda1, verbose=False)\n",
    "                         \n",
    "            return [optimizer], [mainscheduler, warmupscheduler]\n",
    "        \n",
    "        \n",
    "        else:\n",
    "            optimizer_adam = optim.AdamW(self.parameters(), lr=self.args.lr) #, weight_decay=self.args.weight_decay)\n",
    "            lr_scheduler_adam = optim.lr_scheduler.MultiStepLR(optimizer_adam,milestones=[int(self.args.epochs*0.6),\n",
    "                                                       int(self.args.epochs*0.8)],gamma=0.1)\n",
    "            return [optimizer_adam], [lr_scheduler_adam]\n",
    "                \n",
    "    def logging_status(self, mode):\n",
    "        if mode == 'train':\n",
    "            on_step=True\n",
    "            on_epoch=True\n",
    "        else:\n",
    "            on_step=False # Report for the sake of naming but it's not useful\n",
    "            on_epoch=True\n",
    "        return on_step, on_epoch\n",
    "    \n",
    "#     # TODO: Make this more efficient\n",
    "#     def accuracy_top_k(self, k, temp):\n",
    "#         temp = temp.argsort(dim=1, descending=True)[:, :k]\n",
    "#         batchsize=temp.shape[0]\n",
    "#         b_idx = np.arange(0,batchsize)\n",
    "#         tot=0\n",
    "#         for j in range(0, batchsize):\n",
    "#             tot+= b_idx[j] in temp[j]\n",
    "#         return tot*100/batchsize\n",
    "    \n",
    "    def bce_loss(self, preds, y, mode='train'):\n",
    "        \n",
    "        loss = nn.BCELoss()(preds, y)\n",
    "        \n",
    "        if torch.is_tensor(y):\n",
    "            y = y.detach().cpu().numpy()\n",
    "            \n",
    "        auroc = np.round(roc_auc_score(y, preds.detach().cpu()), 4)\n",
    "        auprc = np.round(average_precision_score(y, preds.detach().cpu()), 4)\n",
    "        \n",
    "        on_step=False\n",
    "        on_epoch=True\n",
    "        #self.log(mode + '_loss', loss, on_step=on_step, on_epoch=on_epoch, logger=True)\n",
    "        self.log(mode + '_auroc', auroc, on_step=on_step, on_epoch=on_epoch) #, logger=True)\n",
    "        self.log(mode + '_auprc', auprc, on_step=on_step, on_epoch=on_epoch) #, logger=True)\n",
    "        \n",
    "        return loss \n",
    "    \n",
    "    \n",
    "    def info_nce_loss(self, feats_ehr, feats_img, mode='train'):\n",
    "        # Calculate cosine similarity matrix\n",
    "        cos_sim = F.cosine_similarity(feats_img[:,None,:], feats_ehr[None,:,:], dim=-1)\n",
    "        #print(cos_sim.size())\n",
    "        cos_sim = cos_sim /  self.args.temperature\n",
    "        self_mask = torch.eye(cos_sim.shape[0], dtype=torch.bool,  device=cos_sim.device)\n",
    "        #print(self_mask.size())\n",
    "        cos_sim_negative = torch.clone(cos_sim)\n",
    "        cos_sim_negative.masked_fill_(self_mask, -9e15)\n",
    "        \n",
    "        # Compute based on img->ehr\n",
    "        nll_1 = cos_sim[self_mask] - torch.logsumexp(cos_sim_negative, dim=1)\n",
    "        #temp_1=torch.reshape(cos_sim, (cos_sim.shape[0],cos_sim.shape[1]))\n",
    "        \n",
    "        # Compute based on ehr->img\n",
    "        nll_2 = cos_sim[self_mask] - torch.logsumexp(cos_sim_negative, dim=0) \n",
    "        #temp_2=torch.reshape(cos_sim_2, (cos_sim_2.shape[0],cos_sim_2.shape[1]))\n",
    "        \n",
    "        # Total loss \n",
    "        loss = -(nll_1 + nll_2).mean()\n",
    "            \n",
    "        # Logging ranking metrics\n",
    "        #self.log(mode+'_loss', loss, on_step=on_step, on_epoch=on_epoch, logger=True)\n",
    "        on_step, on_epoch = self.logging_status(mode)\n",
    "        #self.log(mode+'_acc_top1', self.accuracy_top_k(1, temp_1), on_step=on_step, on_epoch=on_epoch) #, logger=True)\n",
    "        #self.log(mode+'_acc_top5', self.accuracy_top_k(5, temp_1), on_step=on_step, on_epoch=on_epoch) #, logger=True)\n",
    "                     \n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    \n",
    "    def modified_info_nce_loss(self, feats_ehr, feats_img, time_diff, mode='train'):\n",
    "        # Calculate cosine similarity matrix\n",
    "        cos_sim = F.cosine_similarity(feats_img[:,None,:], feats_ehr[None,:,:], dim=-1)\n",
    "        cos_sim = cos_sim /  self.args.temperature\n",
    "        self_mask = torch.eye(cos_sim.shape[0], dtype=torch.bool,  device=cos_sim.device)\n",
    "        cos_sim_negative = torch.clone(cos_sim)\n",
    "        cos_sim_negative.masked_fill_(self_mask, -9e15)\n",
    "        \n",
    "        # Compute the values of beta\n",
    "        k = 1\n",
    "        time_diff = torch.FloatTensor(time_diff)\n",
    "        beta = torch.exp(-k*time_diff).to(self.device)\n",
    "        \n",
    "        # Compute based on img->ehr\n",
    "        nll_1 = cos_sim[self_mask] - torch.logsumexp(cos_sim_negative, dim=1)\n",
    "        nll_1 = beta*nll_1\n",
    "        \n",
    "        # Compute based on ehr->img\n",
    "        nll_2 = cos_sim[self_mask] - torch.logsumexp(cos_sim_negative, dim=0)\n",
    "        nll_2 = beta*nll_2\n",
    "        \n",
    "        # Total loss \n",
    "        loss = -(nll_1 + nll_2).mean()\n",
    "            \n",
    "        # Logging ranking metrics\n",
    "        #self.log(mode+'_loss', loss, on_step=on_step, on_epoch=on_epoch, logger=True)\n",
    "        on_step, on_epoch = self.logging_status(mode)\n",
    "       \n",
    "        return loss \n",
    "    \n",
    "    \n",
    "    def off_diagonal(self,x):\n",
    "        n, m = x.shape\n",
    "        assert n == m\n",
    "        return x.flatten()[:-1].view(n - 1, n + 1)[:, 1:].flatten()\n",
    "\n",
    "    \n",
    "    def vicreg_loss(self, feats_ehr, feats_img, mode='train'):\n",
    "        x = feats_ehr\n",
    "        y = feats_img\n",
    "        repr_loss = F.mse_loss(x, y)\n",
    "\n",
    "        #x = torch.cat(FullGatherLayer.apply(x), dim=0) #\n",
    "        #y = torch.cat(FullGatherLayer.apply(y), dim=0) #\n",
    "        \n",
    "        x = x - x.mean(dim=0)\n",
    "        y = y - y.mean(dim=0)\n",
    "\n",
    "        std_x = torch.sqrt(x.var(dim=0) + 0.0001)\n",
    "        std_y = torch.sqrt(y.var(dim=0) + 0.0001)\n",
    "\n",
    "        std_loss_x = torch.mean(F.relu(1 - std_x)) / 2 \n",
    "        std_loss_y = torch.mean(F.relu(1 - std_y)) / 2\n",
    "        std_loss = torch.mean(F.relu(1 - std_x)) / 2 + torch.mean(F.relu(1 - std_y)) / 2\n",
    "                \n",
    "        cov_x = (x.T @ x) / (self.args.batch_size - 1)\n",
    "        cov_y = (y.T @ y) / (self.args.batch_size - 1)\n",
    "        \n",
    "        num_features = len(cov_x) #TODO as arg\n",
    "                \n",
    "        cov_loss_x = self.off_diagonal(cov_x).pow_(2).sum().div(num_features)\n",
    "        cov_loss_y = self.off_diagonal(cov_y).pow_(2).sum().div(num_features)\n",
    "        cov_loss = self.off_diagonal(cov_x).pow_(2).sum().div(num_features) + self.off_diagonal(cov_y).pow_(2).sum().div(num_features)\n",
    "\n",
    "        loss = (\n",
    "            self.args.sim_coeff * repr_loss\n",
    "            + self.args.std_coeff * std_loss\n",
    "            + self.args.cov_coeff * cov_loss\n",
    "        )\n",
    "        on_step, on_epoch = self.logging_status(mode)\n",
    "        \n",
    "        return loss, std_loss_x, std_loss_y, cov_loss_x, cov_loss_y, repr_loss\n",
    "        \n",
    "        \n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        opt = self.optimizers()\n",
    "        opt.zero_grad()\n",
    "        mode = 'train'\n",
    "        # Forward pass for SimCLR\n",
    "        if ((self.args.fusion_type=='None') & (self.args.beta_infonce == False) & (self.args.vicreg == False)):\n",
    "            ehr, imgs, y_ehr, y_cxr, seq_lengths, pairs = batch\n",
    "            # print(\"ehr\", ehr, \"seq_lengths\" , seq_lengths , \"imgs\", imgs)\n",
    "            ehr = torch.from_numpy(ehr).float()\n",
    "            ehr = ehr.to(self.device)\n",
    "            feats_ehr, feats_img = self.model(ehr, seq_lengths, imgs)\n",
    "            # print(feats_ehr, feats_img)\n",
    "            # Compute and log infoNCE loss\n",
    "            loss = self.info_nce_loss(feats_ehr, feats_img, mode)\n",
    "            self.log(mode+'_loss', loss, on_step=True, on_epoch=True) #, logger=True)\n",
    "            \n",
    "            \n",
    "        elif ((self.args.fusion_type=='None') & (self.args.beta_infonce == True) & (self.args.vicreg == False)):\n",
    "            ehr, imgs, y_ehr, y_cxr, seq_lengths, pairs, time_diff = batch\n",
    "            ehr = torch.from_numpy(ehr).float()\n",
    "            ehr = ehr.to(self.device)\n",
    "            feats_ehr, feats_img = self.model(ehr, seq_lengths, imgs)\n",
    "            \n",
    "            # Compute and log infoNCE loss\n",
    "            loss = self.modified_info_nce_loss(feats_ehr, feats_img, time_diff, mode)\n",
    "            self.log(mode+'_loss', loss, on_step=True, on_epoch=True) #, logger=True)\n",
    "            \n",
    "        elif ((self.args.fusion_type=='None') & (self.args.beta_infonce == False) & (self.args.vicreg == True)):\n",
    "            ehr, imgs, y_ehr, y_cxr, seq_lengths, pairs = batch\n",
    "            ehr = torch.from_numpy(ehr).float()\n",
    "            ehr = ehr.to(self.device)\n",
    "            feats_ehr, feats_img = self.model(ehr, seq_lengths, imgs)\n",
    "            \n",
    "            # Compute and log vicreg loss\n",
    "            loss, std_loss_x, std_loss_y, cov_loss_x, cov_loss_y, repr_loss = self.vicreg_loss(feats_ehr, feats_img, mode)\n",
    "            self.log(mode+'_std_ehr', std_loss_x, on_step=True, on_epoch=True) \n",
    "            self.log(mode+'_std_img', std_loss_y, on_step=True, on_epoch=True)\n",
    "            self.log(mode+'_cov_ehr', cov_loss_x, on_step=True, on_epoch=True) \n",
    "            self.log(mode+'_cov_img', cov_loss_y, on_step=True, on_epoch=True)  \n",
    "            self.log(mode+'_repr_loss', repr_loss, on_step=True, on_epoch=True)  \n",
    "\n",
    "\n",
    "            self.log(mode+'_loss', loss, on_step=True, on_epoch=True) #, logger=True)\n",
    "            \n",
    "                        \n",
    "        else:\n",
    "            \n",
    "            if self.args.finetune:\n",
    "                ehr, imgs, y_ehr, y_cxr, seq_lengths, pairs = batch\n",
    "\n",
    "                ehr = torch.from_numpy(ehr).float()\n",
    "                ehr = ehr.to(self.device)\n",
    "                imgs = imgs.to(self.device)\n",
    "                feats_ehr, feats_img = self.model(ehr, seq_lengths, imgs)\n",
    "                # output = self.model(x=ehr, seq_lengths=seq_lengths, img=imgs)\n",
    "                y_ehr = torch.from_numpy(y_ehr)\n",
    "        \n",
    "            else: # Features are already processed for linear classifier\n",
    "                seq_lengths=None\n",
    "                if 'ehr' in self.args.fusion_type:\n",
    "                    ehr, y_ehr = batch\n",
    "                    ehr = ehr.to(self.device)\n",
    "                    output = self.model(x=ehr,seq_lengths=seq_lengths)\n",
    "                elif 'cxr' in self.args.fusion_type:\n",
    "                    imgs, y_cxr, y_ehr = batch\n",
    "                    imgs = imgs.to(self.device)\n",
    "                    output = self.model(img=imgs)\n",
    "                else:\n",
    "                    ehr, imgs, y_ehr, y_cxr = batch\n",
    "                    ehr = ehr.to(self.device)\n",
    "                    imgs = imgs.to(self.device)\n",
    "                    output = self.model(x=ehr, seq_lengths=seq_lengths, img=imgs)\n",
    "                    # print (\"linear eval\" , output)\n",
    "\n",
    "\n",
    "            y = y_ehr.float()\n",
    "            y = y.to(self.device)\n",
    "\n",
    "            preds = output[self.args.fusion_type].squeeze()\n",
    "            print(preds)\n",
    "            exit()\n",
    "\n",
    "            # Compute and log BCE loss\n",
    "            loss = self.bce_loss(preds, y, mode)\n",
    "            self.log(mode+'_loss', loss, on_step=True, on_epoch=True) #, logger=True)\n",
    "    \n",
    "        # import pdb; pdb.set_trace()\n",
    "        # print(\"loss\" , loss.shape)\n",
    "        # Backpropagate\n",
    "        self.manual_backward(loss)\n",
    "        # Optimizer step\n",
    "        opt.step()\n",
    "        # Learning rate step\n",
    "        if self.args.fusion_type=='None':\n",
    "            mainscheduler, warmupscheduler = self.lr_schedulers()\n",
    "            if (self.trainer.is_last_batch) and (self.trainer.current_epoch < self.warmup_epochs-1):\n",
    "                warmupscheduler.step()\n",
    "            elif (self.trainer.is_last_batch) and (self.trainer.current_epoch >= self.warmup_epochs-1):\n",
    "                mainscheduler.step()\n",
    "                \n",
    "#             if (batch_idx==self.num_train_batches-1) & (self.trainer.current_epoch < self.warmup_epochs-1):\n",
    "#                 warmupscheduler.step()\n",
    "#             elif (batch_idx==self.num_train_batches-1) & (self.trainer.current_epoch >= self.warmup_epochs-1):\n",
    "#                 mainscheduler.step()\n",
    "            \n",
    "\n",
    "            return {'loss': loss, 'feats_ehr': feats_ehr.detach().cpu(), 'feats_img': feats_img.detach().cpu(), 'y_ehr':y_ehr}\n",
    "        else:\n",
    "            return {'loss': loss}\n",
    "\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        mode='val'\n",
    "        # Forward pass for SimCLR\n",
    "        if ((self.args.fusion_type=='None') & (self.args.beta_infonce == False)):\n",
    "            ehr, imgs, y_ehr, y_cxr, seq_lengths, pairs = batch\n",
    "            ehr = torch.from_numpy(ehr).float()\n",
    "            ehr = ehr.to(self.device)\n",
    "            feats_ehr, feats_img = self.model(ehr, seq_lengths, imgs) \n",
    "            \n",
    "            # Compute and log infoNCE loss\n",
    "            loss = self.info_nce_loss(feats_ehr, feats_img, mode)\n",
    "            self.log(mode+'_loss_epoch', loss, on_step=False, on_epoch=True) #, logger=True)\n",
    "            \n",
    "            return {'loss': loss, 'feats_ehr': feats_ehr.detach().cpu(), 'feats_img': feats_img.detach().cpu(), 'y_ehr':y_ehr}\n",
    "        \n",
    "        elif ((self.args.fusion_type=='None') & (self.args.beta_infonce == True)):\n",
    "            ehr, imgs, y_ehr, y_cxr, seq_lengths, pairs, time_diff = batch\n",
    "            ehr = torch.from_numpy(ehr).float()\n",
    "            ehr = ehr.to(self.device)\n",
    "            feats_ehr, feats_img = self.model(ehr, seq_lengths, imgs)\n",
    "            \n",
    "            # Compute and log infoNCE loss\n",
    "            loss = self.modified_info_nce_loss(feats_ehr, feats_img, time_diff, mode)\n",
    "            self.log(mode+'_loss', loss, on_step=False, on_epoch=True) #, logger=True)\n",
    "            return {'loss': loss, 'feats_ehr': feats_ehr.detach().cpu(), 'feats_img': feats_img.detach().cpu(), 'y_ehr':y_ehr}\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            if self.args.finetune:\n",
    "                ehr, imgs, y_ehr, y_cxr, seq_lengths, pairs = batch\n",
    "\n",
    "                ehr = torch.from_numpy(ehr).float()\n",
    "                ehr = ehr.to(self.device)\n",
    "                imgs = imgs.to(self.device)\n",
    "                output = self.model(x=ehr, seq_lengths=seq_lengths, img=imgs)\n",
    "                y_ehr = torch.from_numpy(y_ehr)\n",
    "        \n",
    "            else: # Features are already processed for linear classifier\n",
    "                seq_lengths=None\n",
    "                if 'ehr' in self.args.fusion_type:\n",
    "                    ehr, y_ehr = batch\n",
    "                    ehr = ehr.to(self.device)\n",
    "                    output = self.model(x=ehr,seq_lengths=seq_lengths)\n",
    "                elif 'cxr' in self.args.fusion_type:\n",
    "                    imgs, y_cxr, y_ehr = batch\n",
    "                    imgs = imgs.to(self.device)\n",
    "                    output = self.model(img=imgs)\n",
    "                else:\n",
    "                    ehr, imgs, y_ehr, y_cxr = batch\n",
    "                    ehr = ehr.to(self.device)\n",
    "                    imgs = imgs.to(self.device)\n",
    "                    output = self.model(x=ehr, seq_lengths=seq_lengths, img=imgs)\n",
    "\n",
    "\n",
    "            y = y_ehr.float()\n",
    "            y = y.to(self.device)\n",
    "\n",
    "            preds = output[self.args.fusion_type].squeeze()\n",
    "            # print(preds)\n",
    "            # Compute and log BCE loss\n",
    "            loss = self.bce_loss(preds, y, mode)\n",
    "            # Compute and log BCE loss\n",
    "            #loss = self.bce_loss(batch, mode='val')\n",
    "            self.log(mode+'_loss_epoch', loss, on_step=False, on_epoch=True) #, logger=True)\n",
    "            \n",
    "            return {'loss': loss}\n",
    "        \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        mode='test'\n",
    "        \n",
    "        \n",
    "        # Forward pass for SimCLR\n",
    "        if ((self.args.fusion_type=='None') & (self.args.beta_infonce == False)):\n",
    "            if self.args.beta_infonce == True:\n",
    "                ehr, imgs, y_ehr, y_cxr, seq_lengths, pairs, time_diff = batch\n",
    "            else:\n",
    "                ehr, imgs, y_ehr, y_cxr, seq_lengths, pairs = batch\n",
    "            ehr = torch.from_numpy(ehr).float()\n",
    "            ehr = ehr.to(self.device)\n",
    "            \n",
    "            # At test time of SIMCLR, always return all the layer features\n",
    "            if self.args.mode == 'eval':\n",
    "                feats_ehr_0, feats_ehr_3, feats_img_0, feats_img_3 = self.model(ehr, seq_lengths, imgs) \n",
    "            \n",
    "                # Compute and log infoNCE loss\n",
    "                if self.args.beta_infonce == True:\n",
    "                    loss = self.modified_info_nce_loss(feats_ehr_3, feats_img_3, time_diff, mode)\n",
    "                else:\n",
    "                    loss = self.info_nce_loss(feats_ehr_3, feats_img_3, mode)\n",
    "                self.log(mode+'_loss_epoch', loss, on_step=False, on_epoch=True) #, logger=True)\n",
    "            \n",
    "                return {'loss': loss,   'feats_ehr_0': feats_ehr_0.detach().cpu(), \n",
    "                                        'feats_ehr_3': feats_ehr_3.detach().cpu(), \n",
    "                                        'feats_img_0': feats_img_0.detach().cpu(), \n",
    "                                        'feats_img_3': feats_img_3.detach().cpu(), \n",
    "                                        'y_ehr':y_ehr}        \n",
    "        \n",
    "        else:\n",
    "            if self.args.finetune:\n",
    "                ehr, imgs, y_ehr, y_cxr, seq_lengths, pairs = batch\n",
    "\n",
    "                ehr = torch.from_numpy(ehr).float()\n",
    "                ehr = ehr.to(self.device)\n",
    "                imgs = imgs.to(self.device)\n",
    "                output = self.model(x=ehr, seq_lengths=seq_lengths, img=imgs)\n",
    "                y_ehr = torch.from_numpy(y_ehr)\n",
    "        \n",
    "            else: # Features are already processed for linear classifier\n",
    "                seq_lengths=None\n",
    "                if 'ehr' in self.args.fusion_type:\n",
    "                    ehr, y_ehr = batch\n",
    "                    ehr = ehr.to(self.device)\n",
    "                    output = self.model(x=ehr,seq_lengths=seq_lengths)\n",
    "                elif 'cxr' in self.args.fusion_type:\n",
    "                    imgs, y_cxr, y_ehr = batch\n",
    "                    imgs = imgs.to(self.device)\n",
    "                    output = self.model(img=imgs)\n",
    "                else:\n",
    "                    ehr, imgs, y_ehr, y_cxr = batch\n",
    "                    ehr = ehr.to(self.device)\n",
    "                    imgs = imgs.to(self.device)\n",
    "                    output = self.model(x=ehr, seq_lengths=seq_lengths, img=imgs)\n",
    "\n",
    "\n",
    "            y = y_ehr.float()\n",
    "            y = y.to(self.device)\n",
    "\n",
    "            preds = output[self.args.fusion_type].squeeze()\n",
    "            \n",
    "            # print(y.shape, preds.shape)\n",
    "            # Compute and log BCE loss\n",
    "            loss = self.bce_loss(preds, y, mode)\n",
    "            #loss = self.bce_loss(batch, mode=mode)\n",
    "            return {'loss': loss, 'preds': preds, 'y_ehr': y}\n",
    "                \n",
    "    \n",
    "    def process_features(self, outputs, mode):\n",
    "        y = []\n",
    "        if self.args.mode=='eval':\n",
    "            feats_ehr_0=[]\n",
    "            feats_ehr_3=[]\n",
    "            feats_img_0=[]\n",
    "            feats_img_3=[]\n",
    "        elif mode == 'test':\n",
    "            preds = []\n",
    "        else:\n",
    "            feats_ehr = []\n",
    "            feats_img = []\n",
    "        # Iterate through batches and append\n",
    "        i=0\n",
    "        for output in outputs:\n",
    "            if i ==0:\n",
    "                if mode == 'test':\n",
    "                    preds = output['preds'].detach().cpu()\n",
    "                elif self.args.mode == 'eval':\n",
    "                    feats_ehr_0 = output['feats_ehr_0'].detach().cpu()\n",
    "                    feats_ehr_3 = output['feats_ehr_3'].detach().cpu()\n",
    "                    feats_img_0 = output['feats_img_0'].detach().cpu()\n",
    "                    feats_img_3 = output['feats_img_3'].detach().cpu()\n",
    "                else: \n",
    "                    feats_ehr = output['feats_ehr'].detach().cpu()\n",
    "                    feats_img = output['feats_img'].detach().cpu()\n",
    "                y = output['y_ehr'].tolist()\n",
    "                \n",
    "            else:\n",
    "                if mode == 'test':\n",
    "                    preds = torch.cat((preds, output['preds'].detach().cpu()))\n",
    "                elif self.args.mode == 'eval':\n",
    "                    feats_ehr_0 = torch.cat((feats_ehr_0, output['feats_ehr_0'].detach().cpu()))\n",
    "                    feats_ehr_3 = torch.cat((feats_ehr_3, output['feats_ehr_3'].detach().cpu()))\n",
    "                    feats_img_0 = torch.cat((feats_img_0, output['feats_img_0'].detach().cpu()))\n",
    "                    feats_img_3 = torch.cat((feats_img_3, output['feats_img_3'].detach().cpu()))\n",
    "                else:\n",
    "                    feats_ehr = torch.cat((feats_ehr, output['feats_ehr'].detach().cpu()))\n",
    "                    feats_img = torch.cat((feats_img, output['feats_img'].detach().cpu()))\n",
    "                y.extend(output['y_ehr'].tolist())\n",
    "            i+=1\n",
    "        if mode =='test':\n",
    "            return y, preds\n",
    "        elif self.args.mode=='eval':\n",
    "            return feats_ehr_0, feats_ehr_3, feats_img_0, feats_img_3, y\n",
    "        else:\n",
    "            return feats_ehr, feats_img, y\n",
    "    \n",
    "    def save_features(self, x, descrip, mode):\n",
    "        model_path = self.args.save_dir+'/simclr_lr/'+self.args.file_name\n",
    "        if not os.path.exists(model_path):\n",
    "          os.makedirs(model_path)\n",
    "        \n",
    "        torch.save(x, model_path+'/{}_{}_epoch_{}.pt'.format(mode, descrip, self.current_epoch))\n",
    "    \n",
    "    def training_epoch_end(self, outputs):\n",
    "        mode='train'\n",
    "        if ((self.args.fusion_type=='None') & (self.args.save_features == True)):\n",
    "            feats_ehr, feats_img, y = self.process_features(outputs, mode)\n",
    "            self.save_features(feats_ehr, 'feats_ehr', mode)\n",
    "            self.save_features(feats_img, 'feats_img', mode)      \n",
    "            self.save_features(y, 'y', mode)   \n",
    "        \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        mode='val'\n",
    "        if ((self.args.fusion_type=='None') & (self.args.save_features == True)):\n",
    "            feats_ehr, feats_img, y = self.process_features(outputs, mode)\n",
    "            self.save_features(feats_ehr, 'feats_ehr', mode)\n",
    "            self.save_features(feats_img, 'feats_img', mode)      \n",
    "            self.save_features(y, 'y', mode)\n",
    "            \n",
    "\n",
    "    def test_epoch_end(self, outputs):\n",
    "        if ((self.args.fusion_type=='None') & (self.args.save_features == True)):\n",
    "            mode = self.args.eval_set\n",
    "            feats_ehr_0, feats_ehr_3, feats_img_0, feats_img_3, y = self.process_features(outputs, mode)\n",
    "            self.save_features(feats_ehr_0, 'feats_ehr_0', mode)\n",
    "            self.save_features(feats_ehr_3, 'feats_ehr_3', mode)\n",
    "            self.save_features(feats_img_0, 'feats_img_0', mode)\n",
    "            self.save_features(feats_img_3, 'feats_img_3', mode)      \n",
    "            self.save_features(y, 'y', mode)\n",
    "        else:\n",
    "            if self.task =='phenotyping':\n",
    "                mode = 'test'\n",
    "                y, preds = self.process_features(outputs, mode)\n",
    "\n",
    "                auroc_per_label = np.round(roc_auc_score(y, preds, average=None), 4)\n",
    "                auprc_per_label = np.round(average_precision_score(y, preds, average=None), 4)\n",
    "\n",
    "\n",
    "                auroc_label={}\n",
    "                auprc_label={}\n",
    "                for i, name in enumerate(self.LABEL_COLUMNS):\n",
    "                    auroc_label[name]=auroc_per_label[i].item()\n",
    "                    auprc_label[name]=auprc_per_label[i].item()\n",
    "                    #print(name, auroc_per_label[i], auprc_per_label[i])\n",
    "\n",
    "                self.log('auroc_label', auroc_label)\n",
    "                self.log('auprc_label', auprc_label)\n",
    "            \n",
    "    def calculate_auroc_epoch(self, outputs, mode):\n",
    "        labels = []\n",
    "        predictions = []\n",
    "        auroc_label={}\n",
    "        outputs=outputs[self.args.fusion_type].squeeze()\n",
    "        \n",
    "        for output in outputs:\n",
    "            for out_labels in output[\"labels\"].detach().cpu():\n",
    "                labels.append(out_labels)\n",
    "            for out_predictions in output[\"predictions\"].detach().cpu():\n",
    "                predictions.append(out_predictions)\n",
    "\n",
    "        labels = torch.stack(labels).int()\n",
    "        predictions = torch.stack(predictions)\n",
    "        for i, name in enumerate(self.LABEL_COLUMNS):\n",
    "            class_roc_auc = roc_auc_score(labels[:, i], predictions[:, i])\n",
    "            auroc_label[name]=class_roc_auc\n",
    "            \n",
    "        auroc = roc_auc_score(labels, predictions)\n",
    "        auprc = average_precision_score(labels, predictions)\n",
    "        \n",
    "        return auroc, auprc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "626f7ee5-48c7-4987-b058-4dc206a14496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'args' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-e831d5288417>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load the model, weights (if any), and freeze layers (if any)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading model...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpretrain_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'simclr'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSimCLR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'args' is not defined"
     ]
    }
   ],
   "source": [
    "# Load the model, weights (if any), and freeze layers (if any)\n",
    "print(\"Loading model...\")\n",
    "if args.pretrain_type == 'simclr':\n",
    "    model = SimCLR(args, train_dl)\n",
    "    \n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5637d4ee-ef5f-43d0-a38f-18cc4886ecb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "# Prepare data features for downstream tasks \n",
    "@torch.no_grad()\n",
    "def prepare_data_features(device, model, data_loader, bs, fusion_layer, fusion_type):\n",
    "    print(fusion_layer)\n",
    "    # Prepare model\n",
    "    network = deepcopy(model)\n",
    "    if 'ehr' not in fusion_type:\n",
    "        network.model.cxr_model.vision_backbone.fc = nn.Identity() # Removing projection head g(.) \n",
    "     \n",
    "    if 'cxr' not in fusion_type:\n",
    "        network.model.ehr_model.dense_layer = nn.Identity() # Removing projection head g(.)\n",
    "    \n",
    "    network.eval()\n",
    "    network.to(device)\n",
    "\n",
    "    # Encode all images\n",
    "    feats_ehr, feats_imgs, labels_ehr, labels_imgs = [], [], [], []\n",
    "    \n",
    "    for batch_ehr, batch_imgs, batch_ehr_labels, batch_cxr_labels, seq_lengths, pairs in data_loader:\n",
    "        labels_ehr.append(torch.from_numpy(batch_ehr_labels).detach())\n",
    "        #time_diff.append(torch.from_numpy(np.array(batch_time)).detach())\n",
    "        \n",
    "        if 'cxr' not in fusion_type:\n",
    "            batch_ehr = torch.from_numpy(batch_ehr).float().to(device)\n",
    "            #batch_ehr = batch_ehr.to(device)\n",
    "            batch_ehr_feats = network.model.ehr_model(batch_ehr, seq_lengths)\n",
    "            if fusion_layer == 3:\n",
    "                batch_ehr_feats = network.model.ehr_model_g(batch_ehr_feats)\n",
    "                \n",
    "            print('ehr batch shape', np.shape(batch_ehr_feats))\n",
    "            #batch_ehr_feats=torch.reshape(batch_ehr_feats, (1, np.shape(batch_ehr_feats)[0])) #TODO need this for other code\n",
    "            feats_ehr.append(batch_ehr_feats.detach().cpu()) \n",
    "\n",
    "        if 'ehr' not in fusion_type:\n",
    "            batch_imgs = batch_imgs.to(device)\n",
    "            batch_imgs_feats = network.model.cxr_model(batch_imgs)\n",
    "            if fusion_layer == 3:\n",
    "                batch_imgs_feats = network.model.cxr_model_g(batch_imgs_feats)\n",
    "                \n",
    "            print('cxr batch shape', np.shape(batch_imgs_feats))\n",
    "            feats_imgs.append(batch_imgs_feats.detach().cpu())\n",
    "            labels_imgs.append(batch_cxr_labels)\n",
    "    \n",
    "    labels_ehr = torch.cat(labels_ehr, dim=0)\n",
    "    #time_diff = torch.cat(time_diff, dim=0)\n",
    "    \n",
    "    print('shape ehr', np.shape(feats_ehr))\n",
    "    print('shape imgs', np.shape(feats_imgs))\n",
    "    \n",
    "    print('type ehr', type(feats_ehr))\n",
    "    print('type cxr', type(feats_imgs))\n",
    "    \n",
    "    print(type(feats_ehr[0]))\n",
    "    print(type(feats_ehr[0][0]))\n",
    "    print(feats_ehr[0][0])\n",
    "    \n",
    "    print(type(feats_imgs[0]))\n",
    "    print(type(feats_imgs[0][0]))\n",
    "    print(feats_imgs[0][0])\n",
    "    \n",
    "    if 'cxr' not in fusion_type:\n",
    "        #if len(feats_ehr) == len(labels_ehr):\n",
    "        #    feats_ehr=torch.as_tensor(feats_ehr)\n",
    "        feats_ehr = torch.cat(feats_ehr, dim=0)\n",
    "        \n",
    "\n",
    "    if 'ehr' not in fusion_type:\n",
    "        feats_imgs = torch.cat(feats_imgs, dim=0)\n",
    "        labels_imgs = torch.cat(labels_imgs, dim=0)\n",
    "\n",
    "    if 'cxr' in fusion_type:\n",
    "        return data.DataLoader(data.TensorDataset(feats_imgs, labels_imgs, labels_ehr), batch_size=bs, shuffle=False, drop_last=False)\n",
    "    elif 'ehr' in fusion_type:\n",
    "        return data.DataLoader(data.TensorDataset(feats_ehr, labels_ehr), batch_size=bs, shuffle=False, drop_last=False)\n",
    "    else:\n",
    "        return data.DataLoader(data.TensorDataset(feats_ehr, feats_imgs, labels_ehr, labels_imgs), batch_size=bs, shuffle=False, drop_last=False)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c364e012-606d-43e4-ba24-12066f0f5248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda:0 device...\n",
      "Processing features for linear evaluation...\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Set cuda device\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'  \n",
    "\n",
    "print('Using {} device...'.format(device)) \n",
    "\n",
    "print(\"Processing features for linear evaluation...\")\n",
    "train_dl = prepare_data_features(device, model, train_dl, args.batch_size, args.fusion_layer, args.fusion_type) \n",
    "val_dl = prepare_data_features(device, model, val_dl, args.batch_size, args.fusion_layer, args.fusion_type)\n",
    "test_dl = prepare_data_features(device, model, test_dl, args.batch_size, args.fusion_layer, args.fusion_type)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c09dbde-25bb-4dd9-a17c-6ab450db1e06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc8acfa-c34c-4c94-ba6d-0069757174ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MML SSL",
   "language": "python",
   "name": "mml-ssl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
