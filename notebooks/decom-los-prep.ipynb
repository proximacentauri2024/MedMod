{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "74bd3fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pandas as pd \n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import glob\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import platform\n",
    "import pickle\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5c69a1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discretizer:\n",
    "    def __init__(self, timestep=0.8, store_masks=True, impute_strategy='zero', start_time='zero',\n",
    "                 config_path= '/scratch/se1525/MedFuse/ehr_utils/resources/discretizer_config.json'):\n",
    "\n",
    "        with open(config_path) as f:\n",
    "            config = json.load(f)\n",
    "            self._id_to_channel = config['id_to_channel']\n",
    "            self._channel_to_id = dict(zip(self._id_to_channel, range(len(self._id_to_channel))))\n",
    "            self._is_categorical_channel = config['is_categorical_channel']\n",
    "            self._possible_values = config['possible_values']\n",
    "            self._normal_values = config['normal_values']\n",
    "\n",
    "        self._header = [\"Hours\"] + self._id_to_channel\n",
    "        self._timestep = timestep\n",
    "        self._store_masks = store_masks\n",
    "        self._start_time = start_time\n",
    "        self._impute_strategy = impute_strategy\n",
    "\n",
    "        # for statistics\n",
    "        self._done_count = 0\n",
    "        self._empty_bins_sum = 0\n",
    "        self._unused_data_sum = 0\n",
    "\n",
    "    def transform(self, X, header=None, end=None):\n",
    "        if header is None:\n",
    "            header = self._header\n",
    "        assert header[0] == \"Hours\"\n",
    "        eps = 1e-6\n",
    "\n",
    "        N_channels = len(self._id_to_channel)\n",
    "        ts = [float(row[0]) for row in X]\n",
    "        for i in range(len(ts) - 1):\n",
    "            assert ts[i] < ts[i+1] + eps\n",
    "\n",
    "        if self._start_time == 'relative':\n",
    "            first_time = ts[0]\n",
    "        elif self._start_time == 'zero':\n",
    "            first_time = 0\n",
    "        else:\n",
    "            raise ValueError(\"start_time is invalid\")\n",
    "\n",
    "        if end is None:\n",
    "            max_hours = max(ts) - first_time\n",
    "        else:\n",
    "            max_hours = end - first_time\n",
    "\n",
    "        N_bins = int(max_hours / self._timestep + 1.0 - eps)\n",
    "\n",
    "        cur_len = 0\n",
    "        begin_pos = [0 for i in range(N_channels)]\n",
    "        end_pos = [0 for i in range(N_channels)]\n",
    "        for i in range(N_channels):\n",
    "            channel = self._id_to_channel[i]\n",
    "            begin_pos[i] = cur_len\n",
    "            if self._is_categorical_channel[channel]:\n",
    "                end_pos[i] = begin_pos[i] + len(self._possible_values[channel])\n",
    "            else:\n",
    "                end_pos[i] = begin_pos[i] + 1\n",
    "            cur_len = end_pos[i]\n",
    "\n",
    "        data = np.zeros(shape=(N_bins, cur_len), dtype=float)\n",
    "        mask = np.zeros(shape=(N_bins, N_channels), dtype=int)\n",
    "        original_value = [[\"\" for j in range(N_channels)] for i in range(N_bins)]\n",
    "        total_data = 0\n",
    "        unused_data = 0\n",
    "\n",
    "        def write(data, bin_id, channel, value, begin_pos):\n",
    "            channel_id = self._channel_to_id[channel]\n",
    "            if self._is_categorical_channel[channel]:\n",
    "                category_id = self._possible_values[channel].index(value)\n",
    "                N_values = len(self._possible_values[channel])\n",
    "                one_hot = np.zeros((N_values,))\n",
    "                one_hot[category_id] = 1\n",
    "                for pos in range(N_values):\n",
    "                    data[bin_id, begin_pos[channel_id] + pos] = one_hot[pos]\n",
    "            else:\n",
    "                data[bin_id, begin_pos[channel_id]] = float(value)\n",
    "\n",
    "        for row in X:\n",
    "            t = float(row[0]) - first_time\n",
    "            if t > max_hours + eps:\n",
    "                continue\n",
    "            bin_id = int(t / self._timestep - eps)\n",
    "            assert 0 <= bin_id < N_bins\n",
    "\n",
    "            for j in range(1, len(row)):\n",
    "                if row[j] == \"\":\n",
    "                    continue\n",
    "                channel = header[j]\n",
    "                channel_id = self._channel_to_id[channel]\n",
    "\n",
    "                total_data += 1\n",
    "                if mask[bin_id][channel_id] == 1:\n",
    "                    unused_data += 1\n",
    "                mask[bin_id][channel_id] = 1\n",
    "\n",
    "                write(data, bin_id, channel, row[j], begin_pos)\n",
    "                original_value[bin_id][channel_id] = row[j]\n",
    "\n",
    "        # impute missing values\n",
    "\n",
    "        if self._impute_strategy not in ['zero', 'normal_value', 'previous', 'next']:\n",
    "            raise ValueError(\"impute strategy is invalid\")\n",
    "\n",
    "        if self._impute_strategy in ['normal_value', 'previous']:\n",
    "            prev_values = [[] for i in range(len(self._id_to_channel))]\n",
    "            for bin_id in range(N_bins):\n",
    "                for channel in self._id_to_channel:\n",
    "                    channel_id = self._channel_to_id[channel]\n",
    "                    if mask[bin_id][channel_id] == 1:\n",
    "                        prev_values[channel_id].append(original_value[bin_id][channel_id])\n",
    "                        continue\n",
    "                    if self._impute_strategy == 'normal_value':\n",
    "                        imputed_value = self._normal_values[channel]\n",
    "                    if self._impute_strategy == 'previous':\n",
    "                        if len(prev_values[channel_id]) == 0:\n",
    "                            imputed_value = self._normal_values[channel]\n",
    "                        else:\n",
    "                            imputed_value = prev_values[channel_id][-1]\n",
    "                    write(data, bin_id, channel, imputed_value, begin_pos)\n",
    "\n",
    "        if self._impute_strategy == 'next':\n",
    "            prev_values = [[] for i in range(len(self._id_to_channel))]\n",
    "            for bin_id in range(N_bins-1, -1, -1):\n",
    "                for channel in self._id_to_channel:\n",
    "                    channel_id = self._channel_to_id[channel]\n",
    "                    if mask[bin_id][channel_id] == 1:\n",
    "                        prev_values[channel_id].append(original_value[bin_id][channel_id])\n",
    "                        continue\n",
    "                    if len(prev_values[channel_id]) == 0:\n",
    "                        imputed_value = self._normal_values[channel]\n",
    "                    else:\n",
    "                        imputed_value = prev_values[channel_id][-1]\n",
    "                    write(data, bin_id, channel, imputed_value, begin_pos)\n",
    "\n",
    "        empty_bins = np.sum([1 - min(1, np.sum(mask[i, :])) for i in range(N_bins)])\n",
    "        self._done_count += 1\n",
    "        self._empty_bins_sum += empty_bins / (N_bins + eps)\n",
    "        self._unused_data_sum += unused_data / (total_data + eps)\n",
    "\n",
    "        if self._store_masks:\n",
    "            data = np.hstack([data, mask.astype(np.float32)])\n",
    "\n",
    "        # create new header\n",
    "        new_header = []\n",
    "        for channel in self._id_to_channel:\n",
    "            if self._is_categorical_channel[channel]:\n",
    "                values = self._possible_values[channel]\n",
    "                for value in values:\n",
    "                    new_header.append(channel + \"->\" + value)\n",
    "            else:\n",
    "                new_header.append(channel)\n",
    "\n",
    "        if self._store_masks:\n",
    "            for i in range(len(self._id_to_channel)):\n",
    "                channel = self._id_to_channel[i]\n",
    "                new_header.append(\"mask->\" + channel)\n",
    "\n",
    "        new_header = \",\".join(new_header)\n",
    "\n",
    "        return (data, new_header)\n",
    "\n",
    "    def print_statistics(self):\n",
    "        print(\"statistics of discretizer:\")\n",
    "        print(\"\\tconverted {} examples\".format(self._done_count))\n",
    "        print(\"\\taverage unused data = {:.2f} percent\".format(100.0 * self._unused_data_sum / self._done_count))\n",
    "        print(\"\\taverage empty  bins = {:.2f} percent\".format(100.0 * self._empty_bins_sum / self._done_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "fb8a0343",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalizer:\n",
    "    def __init__(self, fields=None):\n",
    "        self._means = None\n",
    "        self._stds = None\n",
    "        self._fields = None\n",
    "        if fields is not None:\n",
    "            self._fields = [col for col in fields]\n",
    "\n",
    "        self._sum_x = None\n",
    "        self._sum_sq_x = None\n",
    "        self._count = 0\n",
    "\n",
    "    def _feed_data(self, x):\n",
    "        x = np.array(x)\n",
    "        self._count += x.shape[0]\n",
    "        if self._sum_x is None:\n",
    "            self._sum_x = np.sum(x, axis=0)\n",
    "            self._sum_sq_x = np.sum(x**2, axis=0)\n",
    "        else:\n",
    "            self._sum_x += np.sum(x, axis=0)\n",
    "            self._sum_sq_x += np.sum(x**2, axis=0)\n",
    "\n",
    "    def _save_params(self, save_file_path):\n",
    "        eps = 1e-7\n",
    "        with open(save_file_path, \"wb\") as save_file:\n",
    "            N = self._count\n",
    "            self._means = 1.0 / N * self._sum_x\n",
    "            self._stds = np.sqrt(1.0/(N - 1) * (self._sum_sq_x - 2.0 * self._sum_x * self._means + N * self._means**2))\n",
    "            self._stds[self._stds < eps] = eps\n",
    "            pickle.dump(obj={'means': self._means,\n",
    "                             'stds': self._stds},\n",
    "                        file=save_file,\n",
    "                        protocol=2)\n",
    "\n",
    "    def load_params(self, load_file_path):\n",
    "        with open(load_file_path, \"rb\") as load_file:\n",
    "            if platform.python_version()[0] == '2':\n",
    "                dct = pickle.load(load_file)\n",
    "            else:\n",
    "                dct = pickle.load(load_file, encoding='latin1')\n",
    "            self._means = dct['means']\n",
    "            self._stds = dct['stds']\n",
    "\n",
    "    def transform(self, X):\n",
    "        if self._fields is None:\n",
    "            fields = range(X.shape[1])\n",
    "        else:\n",
    "            fields = self._fields\n",
    "        ret = 1.0 * X\n",
    "        for col in fields:\n",
    "            ret[:, col] = (X[:, col] - self._means[col]) / self._stds[col]\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5c3f945a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/scratch/fs999/shamoutlab/data/mimic-iv-extracted/decompensation/test_listfile.csv','r') as file:\n",
    "    data = file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "867df894",
   "metadata": {},
   "outputs": [],
   "source": [
    "header = data[0]\n",
    "# print(header)\n",
    "classes = header.strip().split(',')[3:]\n",
    "# print(classes)\n",
    "data1 = data[1:]\n",
    "# print(data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0d88f76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = [line.split(',') for line in data1]\n",
    "# data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6413a01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_map = {mas[0]: {'labels': list(map(float, mas[3:])),\n",
    "                     'stay_id': float(mas[2]),\n",
    "                     'time': float(mas[1])}\n",
    "                           for mas in data1\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "30bb1146",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('10001884_episode1_timeseries.csv',\n",
       "  {'labels': [1.0], 'stay_id': 37510196.0, 'time': 216.0}),\n",
       " ('10002155_episode1_timeseries.csv',\n",
       "  {'labels': [0.0], 'stay_id': 33685454.0, 'time': 148.0}),\n",
       " ('10002155_episode2_timeseries.csv',\n",
       "  {'labels': [0.0], 'stay_id': 31090461.0, 'time': 93.0}),\n",
       " ('10002155_episode3_timeseries.csv',\n",
       "  {'labels': [1.0], 'stay_id': 32358465.0, 'time': 20.0}),\n",
       " ('10002348_episode1_timeseries.csv',\n",
       "  {'labels': [0.0], 'stay_id': 32610785.0, 'time': 235.0}),\n",
       " ('10002428_episode1_timeseries.csv',\n",
       "  {'labels': [0.0], 'stay_id': 34807493.0, 'time': 48.0}),\n",
       " ('10002930_episode1_timeseries.csv',\n",
       "  {'labels': [0.0], 'stay_id': 37049133.0, 'time': 27.0}),\n",
       " ('10002930_episode2_timeseries.csv',\n",
       "  {'labels': [0.0], 'stay_id': 35629889.0, 'time': 16.0}),\n",
       " ('10003400_episode1_timeseries.csv',\n",
       "  {'labels': [0.0], 'stay_id': 32128372.0, 'time': 309.0}),\n",
       " ('10003400_episode2_timeseries.csv',\n",
       "  {'labels': [0.0], 'stay_id': 34577403.0, 'time': 70.0})]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(data_map.items())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d80bf070",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_timeseries(ts_file, time_bound=48):\n",
    "        \n",
    "    ret = []\n",
    "    with open(ts_file, \"r\") as tsfile:\n",
    "        header = tsfile.readline().strip().split(',')\n",
    "        assert header[0] == \"Hours\"\n",
    "        for line in 000tsfile:\n",
    "            mas = line.strip().split(',')\n",
    "            if time_bound is not None:\n",
    "                t = float(mas[0])\n",
    "                if t > time_bound + 1e-6:\n",
    "                    break\n",
    "            ret.append(np.array(mas))\n",
    "    return (np.stack(ret), header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ce4ba576",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([['0.11666666666666667', '', '', '', '', '', '', '', '', '109', '',\n",
       "         '', '', '30', '', '', '', ''],\n",
       "        ['0.16666666666666666', '', '61.0', '', '', '', '', '', '', '109',\n",
       "         '', '64', '97.0', '29', '74.0', '', '', ''],\n",
       "        ['0.6666666666666666', '', '', '', '', '', '', '', '167.0', '',\n",
       "         '', '', '', '', '', '', '', ''],\n",
       "        ['0.9333333333333333', '', '', '', '', '', '', '', '', '', '', '',\n",
       "         '', '', '', '39.05555555555556', '', ''],\n",
       "        ['1.0833333333333333', '', '', '', 'Spontaneously',\n",
       "         'Obeys Commands', '', 'Oriented', '', '', '', '', '', '', '', '',\n",
       "         '', ''],\n",
       "        ['1.1666666666666667', '', '48.0', '', '', '', '', '', '', '93',\n",
       "         '', '60', '99.0', '26', '93.0', '', '', ''],\n",
       "        ['2.1666666666666665', '', '45.0', '', '', '', '', '', '115.0',\n",
       "         '81', '', '54', '98.0', '22', '83.0', '', '', ''],\n",
       "        ['3.1666666666666665', '', '52.0', '', '', '', '', '', '', '80',\n",
       "         '', '63', '100.0', '22', '99.0', '', '', ''],\n",
       "        ['4.166666666666667', '', '', '', 'To Speech', 'Obeys Commands',\n",
       "         '', 'Oriented', '', '82', '', '', '100.0', '18', '',\n",
       "         '36.55555555555556', '', ''],\n",
       "        ['5.166666666666667', '', '50.0', '', '', '', '', '', '', '78',\n",
       "         '', '60', '100.0', '19', '90.0', '', '', ''],\n",
       "        ['6.166666666666667', '', '51.0', '', '', '', '', '', '', '75',\n",
       "         '', '62', '100.0', '18', '94.0', '', '', ''],\n",
       "        ['7.066666666666666', '', '', '', '', '', '', '', '136.0', '', '',\n",
       "         '', '', '', '', '', '', ''],\n",
       "        ['7.166666666666667', '', '52.0', '', '', '', '', '', '', '79',\n",
       "         '', '62', '100.0', '18', '93.0', '', '', ''],\n",
       "        ['8.166666666666666', '', '49.0', '', 'To Speech',\n",
       "         'Obeys Commands', '', 'Oriented', '', '75', '', '60', '100.0',\n",
       "         '34', '95.0', '36.666666666666664', '', ''],\n",
       "        ['9.166666666666666', '', '45.0', '', '', '', '', '', '', '73',\n",
       "         '', '59', '100.0', '19', '98.0', '', '', ''],\n",
       "        ['10.166666666666666', '', '58.0', '', '', '', '', '', '', '84',\n",
       "         '', '69', '100.0', '24', '106.0', '', '', ''],\n",
       "        ['11.166666666666666', '', '47.0', '', '', '', '', '', '', '80',\n",
       "         '', '61', '100.0', '21', '111.0', '36.888888888888886', '', ''],\n",
       "        ['12.166666666666666', '', '61.0', '', 'Spontaneously',\n",
       "         'Obeys Commands', '', 'Oriented', '84.0', '89', '', '75',\n",
       "         '100.0', '27', '117.0', '36.388888888888886', '', ''],\n",
       "        ['13.166666666666666', '', '55.0', '', '', '', '', '', '', '96',\n",
       "         '', '71', '98.0', '18', '117.0', '', '', ''],\n",
       "        ['14.166666666666666', '', '', '', '', '', '', '', '', '87', '',\n",
       "         '', '100.0', '22', '', '', '', ''],\n",
       "        ['14.466666666666667', '', '', '', '', '', '', '', '', '', '', '',\n",
       "         '', '', '', '', '', '7.36'],\n",
       "        ['15.166666666666666', '', '', '', '', '', '', '', '', '85', '',\n",
       "         '', '100.0', '23', '', '', '', ''],\n",
       "        ['15.2', '', '46.0', '', '', '', '', '', '', '', '', '58', '', '',\n",
       "         '95.0', '', '', '']], dtype='<U19'),\n",
       " ['Hours',\n",
       "  'Capillary refill rate',\n",
       "  'Diastolic blood pressure',\n",
       "  'Fraction inspired oxygen',\n",
       "  'Glascow coma scale eye opening',\n",
       "  'Glascow coma scale motor response',\n",
       "  'Glascow coma scale total',\n",
       "  'Glascow coma scale verbal response',\n",
       "  'Glucose',\n",
       "  'Heart Rate',\n",
       "  'Height',\n",
       "  'Mean blood pressure',\n",
       "  'Oxygen saturation',\n",
       "  'Respiratory rate',\n",
       "  'Systolic blood pressure',\n",
       "  'Temperature',\n",
       "  'Weight',\n",
       "  'pH'])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_timeseries('/scratch/fs999/shamoutlab/data/mimic-iv-extracted/phenotyping/test/14851532_episode3_timeseries.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ab9eea70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_by_file_name(index, time_bound=5):\n",
    "    t = data_map[index.split('/')[-1]]['time'] if time_bound is None else time_bound\n",
    "    y = data_map[index.split('/')[-1]]['labels']\n",
    "    stay_id = data_map[index.split('/')[-1]]['stay_id']\n",
    "    (X, header) = read_timeseries(index, time_bound=time_bound)\n",
    "\n",
    "    return {\"X\": X,\n",
    "            \"t\": t,\n",
    "            \"y\": y,\n",
    "            'stay_id': stay_id,\n",
    "            \"header\": header,\n",
    "            \"name\": index}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a12b1237",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'X': array([['0.11666666666666667', '', '', '', '', '', '', '', '', '109', '',\n",
       "         '', '', '30', '', '', '', ''],\n",
       "        ['0.16666666666666666', '', '61.0', '', '', '', '', '', '', '109',\n",
       "         '', '64', '97.0', '29', '74.0', '', '', ''],\n",
       "        ['0.6666666666666666', '', '', '', '', '', '', '', '167.0', '',\n",
       "         '', '', '', '', '', '', '', ''],\n",
       "        ['0.9333333333333333', '', '', '', '', '', '', '', '', '', '', '',\n",
       "         '', '', '', '39.05555555555556', '', ''],\n",
       "        ['1.0833333333333333', '', '', '', 'Spontaneously',\n",
       "         'Obeys Commands', '', 'Oriented', '', '', '', '', '', '', '', '',\n",
       "         '', ''],\n",
       "        ['1.1666666666666667', '', '48.0', '', '', '', '', '', '', '93',\n",
       "         '', '60', '99.0', '26', '93.0', '', '', ''],\n",
       "        ['2.1666666666666665', '', '45.0', '', '', '', '', '', '115.0',\n",
       "         '81', '', '54', '98.0', '22', '83.0', '', '', ''],\n",
       "        ['3.1666666666666665', '', '52.0', '', '', '', '', '', '', '80',\n",
       "         '', '63', '100.0', '22', '99.0', '', '', ''],\n",
       "        ['4.166666666666667', '', '', '', 'To Speech', 'Obeys Commands',\n",
       "         '', 'Oriented', '', '82', '', '', '100.0', '18', '',\n",
       "         '36.55555555555556', '', '']], dtype='<U19'),\n",
       " 't': 5,\n",
       " 'y': [0.0],\n",
       " 'stay_id': 30681041.0,\n",
       " 'header': ['Hours',\n",
       "  'Capillary refill rate',\n",
       "  'Diastolic blood pressure',\n",
       "  'Fraction inspired oxygen',\n",
       "  'Glascow coma scale eye opening',\n",
       "  'Glascow coma scale motor response',\n",
       "  'Glascow coma scale total',\n",
       "  'Glascow coma scale verbal response',\n",
       "  'Glucose',\n",
       "  'Heart Rate',\n",
       "  'Height',\n",
       "  'Mean blood pressure',\n",
       "  'Oxygen saturation',\n",
       "  'Respiratory rate',\n",
       "  'Systolic blood pressure',\n",
       "  'Temperature',\n",
       "  'Weight',\n",
       "  'pH'],\n",
       " 'name': '/scratch/fs999/shamoutlab/data/mimic-iv-extracted/phenotyping/test/14851532_episode3_timeseries.csv'}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_by_file_name('/scratch/fs999/shamoutlab/data/mimic-iv-extracted/phenotyping/test/14851532_episode3_timeseries.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "32141533",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EHRdataset(Dataset):\n",
    "    def __init__(self, discretizer, normalizer, listfile, dataset_dir, return_names=True, period_length=24.0):\n",
    "        self.return_names = return_names\n",
    "        self.discretizer = discretizer\n",
    "        self.normalizer = normalizer\n",
    "        self._period_length = period_length\n",
    "\n",
    "        self._dataset_dir = dataset_dir\n",
    "        listfile_path = listfile\n",
    "        with open(listfile_path, \"r\") as lfile:\n",
    "            self._data = lfile.readlines()\n",
    "        # column names\n",
    "        self._listfile_header = self._data[0]\n",
    "        # label columns\n",
    "        self.CLASSES = self._listfile_header.strip().split(',')[3:]\n",
    "        # files list with lables \n",
    "        self._data = self._data[1:]\n",
    "        \n",
    "        ### pay attention\n",
    "        self._data = [line.split(',') for line in self._data]\n",
    "        self.data_map = {\n",
    "            mas[0]: {\n",
    "                'labels': list(map(float, mas[3:])),\n",
    "                'stay_id': float(mas[2]),\n",
    "                'time': float(mas[1]),\n",
    "                }\n",
    "                for mas in self._data\n",
    "        }\n",
    "        self.names = list(self.data_map.keys())\n",
    "        ### pay attention\n",
    "    \n",
    "    def _read_timeseries(self, ts_filename, time_bound=None):\n",
    "        \n",
    "        ret = []\n",
    "        with open(os.path.join(self._dataset_dir, ts_filename), \"r\") as tsfile:\n",
    "            header = tsfile.readline().strip().split(',')\n",
    "            assert header[0] == \"Hours\"\n",
    "            for line in tsfile:\n",
    "                mas = line.strip().split(',')\n",
    "                if time_bound is not None:\n",
    "                    t = float(mas[0])\n",
    "                    if t > time_bound + 1e-6:\n",
    "                        break\n",
    "                ret.append(np.array(mas))\n",
    "        return (np.stack(ret), header)\n",
    "    \n",
    "    def read_by_file_name(self, index, time_bound=None):\n",
    "        t = self.data_map[index]['time'] if time_bound is None else time_bound\n",
    "        y = self.data_map[index]['labels']\n",
    "        stay_id = self.data_map[index]['stay_id']\n",
    "        (X, header) = self._read_timeseries(index, time_bound=time_bound)\n",
    "\n",
    "        return {\"X\": X,\n",
    "                \"t\": t,\n",
    "                \"y\": y,\n",
    "                'stay_id': stay_id,\n",
    "                \"header\": header,\n",
    "                \"name\": index}\n",
    "\n",
    "    def get_decomp_los(self, index, time_bound=None):\n",
    "        # name = self._data[index][0]\n",
    "        # time_bound = self._data[index][1]\n",
    "        # ys = self._data[index][3]\n",
    "\n",
    "        # (data, header) = self._read_timeseries(index, time_bound=time_bound)\n",
    "        # data = self.discretizer.transform(data, end=time_bound)[0] \n",
    "        # if (self.normalizer is not None):\n",
    "        #     data = self.normalizer.transform(data)\n",
    "        # ys = np.array(ys, dtype=np.int32) if len(ys) > 1 else np.array(ys, dtype=np.int32)[0]\n",
    "        # return data, ys\n",
    "\n",
    "        # data, ys = \n",
    "        return self.__getitem__(index, time_bound)\n",
    "\n",
    "\n",
    "    def __getitem__(self, index, time_bound=None):\n",
    "        if isinstance(index, int):\n",
    "            index = self.names[index]\n",
    "        ret = self.read_by_file_name(index, time_bound)\n",
    "        data = ret[\"X\"]\n",
    "        ts = ret[\"t\"] if ret['t'] > 0.0 else self._period_length\n",
    "        ys = ret[\"y\"]\n",
    "        names = ret[\"name\"]\n",
    "        data = self.discretizer.transform(X=data, header=None, end=ts)[0] \n",
    "        if (self.normalizer is not None):\n",
    "            data = self.normalizer.transform(X=data)\n",
    "        ys = np.array(ys, dtype=np.int32) if len(ys) > 1 else np.array(ys, dtype=np.int32)[0]\n",
    "        return data, ys\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "55455019",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_collate(batch):\n",
    "    x = [item[0] for item in batch]\n",
    "    x, seq_length = pad_zeros(x)\n",
    "    targets = np.array([item[1] for item in batch])\n",
    "    return [x, targets, seq_length]\n",
    "\n",
    "def pad_zeros(arr, min_length=None):\n",
    "    dtype = arr[0].dtype\n",
    "    seq_length = [x.shape[0] for x in arr]\n",
    "    max_len = max(seq_length)\n",
    "    ret = [np.concatenate([x, np.zeros((max_len - x.shape[0],) + x.shape[1:], dtype=dtype)], axis=0)\n",
    "           for x in arr]\n",
    "    if (min_length is not None) and ret[0].shape[0] < min_length:\n",
    "        ret = [np.concatenate([x, np.zeros((min_length - x.shape[0],) + x.shape[1:], dtype=dtype)], axis=0)\n",
    "               for x in ret]\n",
    "    return np.array(ret), seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "041265ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "listfile1 = '/scratch/fs999/shamoutlab/data/mimic-iv-extracted/in-hospital-mortality/train_listfile.csv'\n",
    "data_dir1 = '/scratch/fs999/shamoutlab/data/mimic-iv-extracted/in-hospital-mortality/train/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "7fedb462",
   "metadata": {},
   "outputs": [],
   "source": [
    "discretizer = Discretizer()\n",
    "normalizer = Normalizer()\n",
    "normalizer.load_params('/scratch/se1525/mml-ssl/ph_ts0.8.input_str:previous.start_time:zero.normalizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "8633acc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = EHRdataset(discretizer,normalizer,listfile1, data_dir1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "1ac27295",
   "metadata": {},
   "outputs": [],
   "source": [
    " test_dataloader = DataLoader(test_set, 32, shuffle=True, collate_fn=my_collate, pin_memory=True, num_workers=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c5dc04c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32,)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(test_dataloader))[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ce72f94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import os\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "656bbbbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reader(object):\n",
    "    def __init__(self, dataset_dir, listfile=None):\n",
    "        self._dataset_dir = dataset_dir\n",
    "        self._current_index = 0\n",
    "        if listfile is None:\n",
    "            listfile_path = os.path.join(dataset_dir, \"listfile.csv\")\n",
    "        else:\n",
    "            listfile_path = listfile\n",
    "        with open(listfile_path, \"r\") as lfile:\n",
    "            self._data = lfile.readlines()\n",
    "        self._listfile_header = self._data[0]\n",
    "        self._data = self._data[1:]\n",
    "\n",
    "    def get_number_of_examples(self):\n",
    "        return len(self._data)\n",
    "\n",
    "    def random_shuffle(self, seed=None):\n",
    "        if seed is not None:\n",
    "            random.seed(seed)\n",
    "        random.shuffle(self._data)\n",
    "\n",
    "    def read_example(self, index):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def read_next(self):\n",
    "        to_read_index = self._current_index\n",
    "        self._current_index += 1\n",
    "        if self._current_index == self.get_number_of_examples():\n",
    "            self._current_index = 0\n",
    "        return self.read_example(to_read_index)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "3f0c0cc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18845"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Reader(dataset_dir=data_dir1,listfile=listfile1).get_number_of_examples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a2c08795",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecompensationReader(Reader):\n",
    "    def __init__(self, dataset_dir, listfile=None):\n",
    "        \"\"\" Reader for decompensation prediction task.\n",
    "        :param dataset_dir: Directory where timeseries files are stored.\n",
    "        :param listfile:    Path to a listfile. If this parameter is left `None` then\n",
    "                            `dataset_dir/listfile.csv` will be used.\n",
    "        \"\"\"\n",
    "        Reader.__init__(self, dataset_dir, listfile)\n",
    "        self._data = [line.split(',') for line in self._data]\n",
    "\n",
    "\n",
    "        self._data = [(x, float(t), int(id), int(y)) for (x, t, id, y) in self._data]\n",
    "\n",
    "    def _read_timeseries(self, ts_filename, time_bound):\n",
    "        ret = []\n",
    "        with open(os.path.join(self._dataset_dir, ts_filename), \"r\") as tsfile:\n",
    "            header = tsfile.readline().strip().split(',')\n",
    "            assert header[0] == \"Hours\"\n",
    "            for line in tsfile:\n",
    "                mas = line.strip().split(',')\n",
    "                t = float(mas[0])\n",
    "                if t > time_bound + 1e-6:\n",
    "                    break\n",
    "                ret.append(np.array(mas))\n",
    "        return (np.stack(ret), header)\n",
    "\n",
    "    def read_example(self, index):\n",
    "        \"\"\" Read the example with given index.\n",
    "\n",
    "        :param index: Index of the line of the listfile to read (counting starts from 0).\n",
    "        :return: Directory with the following keys:\n",
    "            X : np.array\n",
    "                2D array containing all events. Each row corresponds to a moment.\n",
    "                First column is the time and other columns correspond to different\n",
    "                variables.\n",
    "            t : float\n",
    "                Length of the data in hours. Note, in general, it is not equal to the\n",
    "                timestamp of last event.\n",
    "            y : int (0 or 1)\n",
    "                Mortality within next 24 hours.\n",
    "            header : array of strings\n",
    "                Names of the columns. The ordering of the columns is always the same.\n",
    "            name: Name of the sample.\n",
    "        \"\"\"\n",
    "        if index < 0 or index >= len(self._data):\n",
    "            raise ValueError(\"Index must be from 0 (inclusive) to number of examples (exclusive).\")\n",
    "\n",
    "        name = self._data[index][0]\n",
    "        t = self._data[index][1]\n",
    "        id = self._data[index][2]\n",
    "        y = self._data[index][3]\n",
    "        (X, header) = self._read_timeseries(name, t)\n",
    "\n",
    "        return {\"X\": X,\n",
    "                \"t\": t,\n",
    "                \"id\":id,\n",
    "                \"y\": y,\n",
    "                \"header\": header,\n",
    "                \"name\": name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "847e70e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "listfile = '/scratch/fs999/shamoutlab/data/mimic-iv-extracted/decompensation/test_listfile.csv'\n",
    "datadir = '/scratch/fs999/shamoutlab/data/mimic-iv-extracted/decompensation/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "babd0e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = DecompensationReader(datadir,listfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "e124e3ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(239, 18)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reader.read_example(811430)['X'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a721fc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_chunk(data, ts, discretizer, normalizer=None):\n",
    "    data = [discretizer.transform(X, end=t)[0] for (X, t) in zip(data, ts)]\n",
    "    if normalizer is not None:\n",
    "        data = [normalizer.transform(X) for X in data]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "da335d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_chunk(reader, chunk_size):\n",
    "    data = {}\n",
    "    for i in range(chunk_size):\n",
    "        ret = reader.read_next()\n",
    "        for k, v in ret.items():\n",
    "            if k not in data:\n",
    "                data[k] = []\n",
    "            data[k].append(v)\n",
    "    data[\"header\"] = data[\"header\"][0]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "642b4389",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_and_shuffle(data, batch_size):\n",
    "    \"\"\" Sort data by the length and then make batches and shuffle them.\n",
    "        data is tuple (X1, X2, ..., Xn) all of them have the same length.\n",
    "        Usually data = (X, y).\n",
    "    \"\"\"\n",
    "    assert len(data) >= 2\n",
    "    data = list(zip(*data))\n",
    "\n",
    "    random.shuffle(data)\n",
    "\n",
    "    old_size = len(data)\n",
    "    rem = old_size % batch_size\n",
    "    head = data[:old_size - rem]\n",
    "    tail = data[old_size - rem:]\n",
    "    data = []\n",
    "\n",
    "    head.sort(key=(lambda x: x[0].shape[0]))\n",
    "\n",
    "    mas = [head[i: i+batch_size] for i in range(0, len(head), batch_size)]\n",
    "    random.shuffle(mas)\n",
    "\n",
    "    for x in mas:\n",
    "        data += x\n",
    "    data += tail\n",
    "\n",
    "    data = list(zip(*data))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "693e2338",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_zeros(arr, min_length=None):\n",
    "    \"\"\"\n",
    "    `arr` is an array of `np.array`s\n",
    "\n",
    "    The function appends zeros to every `np.array` in `arr`\n",
    "    to equalize their first axis lenghts.\n",
    "    \"\"\"\n",
    "    dtype = arr[0].dtype\n",
    "    max_len = max([x.shape[0] for x in arr])\n",
    "    ret = [np.concatenate([x, np.zeros((max_len - x.shape[0],) + x.shape[1:], dtype=dtype)], axis=0)\n",
    "           for x in arr]\n",
    "    if (min_length is not None) and ret[0].shape[0] < min_length:\n",
    "        ret = [np.concatenate([x, np.zeros((min_length - x.shape[0],) + x.shape[1:], dtype=dtype)], axis=0)\n",
    "               for x in ret]\n",
    "    return np.array(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "298165a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchGen(object):\n",
    "\n",
    "    def __init__(self, reader, discretizer, normalizer,\n",
    "                 batch_size, steps, shuffle, return_names=False):\n",
    "        self.reader = reader\n",
    "        self.discretizer = discretizer\n",
    "        self.normalizer = normalizer\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.return_names = return_names\n",
    "\n",
    "        if steps is None:\n",
    "            self.n_examples = reader.get_number_of_examples()\n",
    "            self.steps = (self.n_examples + batch_size - 1) // batch_size\n",
    "        else:\n",
    "            self.n_examples = steps * batch_size\n",
    "            self.steps = steps\n",
    "\n",
    "        self.chunk_size = min(1024, self.steps) * batch_size\n",
    "        self.lock = threading.Lock()\n",
    "        self.generator = self._generator()\n",
    "\n",
    "    def _generator(self):\n",
    "        B = self.batch_size\n",
    "        while True:\n",
    "            if self.shuffle:\n",
    "                self.reader.random_shuffle()\n",
    "            remaining = self.n_examples\n",
    "            while remaining > 0:\n",
    "                current_size = min(self.chunk_size, remaining)\n",
    "                remaining -= current_size\n",
    "\n",
    "                ret = read_chunk(self.reader, current_size)\n",
    "                Xs = ret[\"X\"]\n",
    "                ts = ret[\"t\"]\n",
    "                ys = ret[\"y\"]\n",
    "                names = ret[\"name\"]\n",
    "                \n",
    "                #Xs = preprocess_chunk(Xs, ts, self.discretizer, self.normalizer)\n",
    "                (Xs, ys, ts, names) = sort_and_shuffle([Xs, ys, ts, names], B)\n",
    "\n",
    "                for i in range(0, current_size, B):\n",
    "                    X = pad_zeros(Xs[i:i + B])\n",
    "                    y = np.array(ys[i:i + B])\n",
    "                    batch_names = names[i:i+B]\n",
    "                    batch_ts = ts[i:i+B]\n",
    "                    batch_data = (X, y)\n",
    "                    if not self.return_names:\n",
    "                        yield batch_data\n",
    "                    else:\n",
    "                        yield {\"data\": batch_data, \"names\": batch_names, \"ts\": batch_ts}\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self.generator\n",
    "\n",
    "    def next(self):\n",
    "        with self.lock:\n",
    "            return next(self.generator)\n",
    "\n",
    "    def __next__(self):\n",
    "        return self.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "320426f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "405720"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(811440+2-1)//2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "2225beaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_loader = BatchGen(reader,discretizer,normalizer,batch_size=1,steps=None,shuffle=True,return_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "fe02c5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = next(iter(batch_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "71cec1f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': (array([[['0.3', '', '', ..., '', '', ''],\n",
       "          ['0.48333333333333334', '', '85.0', ..., '', '', ''],\n",
       "          ['0.5', '', '', ..., '', '', ''],\n",
       "          ...,\n",
       "          ['27.683333333333334', '', '', ..., '', '', ''],\n",
       "          ['28.466666666666665', '', '', ..., '', '', ''],\n",
       "          ['28.483333333333334', '', '60.0', ..., '', '', '']]],\n",
       "        dtype='<U19'),\n",
       "  array([0])),\n",
       " 'names': ('15048951_episode3_timeseries.csv',),\n",
       " 'ts': (29.0,)}"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "eb18fd8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 69, 18)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['data'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b289d1e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d8bfcf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MML SSL",
   "language": "python",
   "name": "mml-ssl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
