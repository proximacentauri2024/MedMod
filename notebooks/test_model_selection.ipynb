{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cda0bea-d9c8-48d4-a163-157b8db6e849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse arguments\n",
    "args = parser.parse_args([\n",
    "    '--device', '$CUDA_VISIBLE_DEVICES',\n",
    "    '--vision-backbone', 'resnet34',\n",
    "    '--epochs', '100',\n",
    "    '--batch_size', '256',\n",
    "    '--lr', '0.01',\n",
    "    '--job_number', '${SLURM_JOBID}',\n",
    "    '--load_state', 'SIMCLR-7538352',\n",
    "    '--file_name', 'SIMCLR-7538352-select-LC',\n",
    "    '--width', '1',\n",
    "    '--pretrain_type', 'simclr',\n",
    "    '--fusion_type', 'lineareval',\n",
    "    '--mode', 'train',\n",
    "    '--fusion_layer', '0',\n",
    "    '--save_dir', '/scratch/se1525/mml-ssl/checkpoints/phenotyping/models',\n",
    "    '--tag', 'model_selection'\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94be944-ec4c-4afb-bb17-146286722cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2022 Farah E. Shamout\n",
    "#\n",
    "# TODO: licsense\n",
    "# ==============================================================================\n",
    "\"\"\"This script defines the SimCLR model and performs training and evaluation.\"\"\"\n",
    "\n",
    "\n",
    "data_dir = '/scratch/fs999/shamoutlab/data/mimic-iv-extracted/'\n",
    "img_dir = '/scratch/fs999/shamoutlab/data/physionet.org/files//mimic-cxr-jpg/2.0.0'\n",
    "code_dir = '/scratch/se1525/mml-ssl'\n",
    "task = 'phenotyping'\n",
    "\n",
    "# Import libraries\n",
    "import sys\n",
    "#sys.path.append(f'{code_dir_medfuse}')\n",
    "import numpy as np\n",
    "import argparse\n",
    "import os\n",
    "import importlib as imp\n",
    "import re\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import neptune.new as neptune\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import time\n",
    "import pickle as pkl\n",
    "\n",
    "# ##Â Visualization\n",
    "# import matplotlib.pyplot as plt\n",
    "# from matplotlib.pyplot import figure\n",
    "# from tqdm.notebook import tqdm\n",
    "# import matplotlib\n",
    "# matplotlib.use('Agg')\n",
    "\n",
    "# Import Pytorch \n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "\n",
    "\n",
    "## Performance metrics\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "# Import custom functions\n",
    "import parser as par\n",
    "import data_utils as prep\n",
    "# from fusion_trainer_farah import FusionTrainer\n",
    "# from mmtm_trainer import MMTMTrainer\n",
    "# from daft_trainer import DAFTTrainer\n",
    "\n",
    "\n",
    "# Import functions from MedFuse\n",
    "from datasets.ehr_dataset import get_datasets\n",
    "from datasets.cxr_dataset import get_cxr_datasets\n",
    "from datasets.fusion import load_cxr_ehr\n",
    "from ehr_preprocess import ehr_funcs\n",
    "from simclr_trainer_gpu import SimCLR, train, test, prepare_data_features#, LogisticRegression, train_logreg\n",
    "import load_tasks as tasks\n",
    "\n",
    "#sys.path.append('/home/shamoutlab/.local/bin')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def initiate_logger(tags):\n",
    "    logger = pl_loggers.NeptuneLogger(project=\"shaza-workspace/mml-ssl\",\n",
    "    api_key=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiI4NDU3ZDlmMi01OGEyLTQzMTAtODJmYS01Mjc5N2U4ZjgyMTAifQ==\", tags=tags, log_model_checkpoints=False)\n",
    "    return logger\n",
    "\n",
    "seed = 1002\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "\n",
    "def load_weights(model, path):\n",
    "        checkpoint = torch.load(path)\n",
    "        own_state = model.state_dict()\n",
    "        own_keys = list(model.state_dict().keys())\n",
    "        checkpoint_keys = list(checkpoint['state_dict'].keys())\n",
    "        \n",
    "        #print('Total number of checkpoint params = {}'.format(len(checkpoint_keys)))\n",
    "        #print('Total number of current model params = {}'.format(len(own_keys)))\n",
    "\n",
    "        count = 0\n",
    "        changed = []\n",
    "        for name in own_keys:\n",
    "            if name not in checkpoint_keys:\n",
    "                # double check if name exists in a different format\n",
    "                for x in checkpoint_keys:\n",
    "                    if name in x:\n",
    "                        param=checkpoint['state_dict'][x]\n",
    "                        if isinstance(param, torch.nn.Parameter):\n",
    "                            param=param.data\n",
    "                        own_state[name].copy_(param)\n",
    "                        count+=1\n",
    "            else:\n",
    "                param=checkpoint['state_dict'][name]\n",
    "                if isinstance(param, torch.nn.Parameter):\n",
    "                    param=param.data\n",
    "                own_state[name].copy_(param)\n",
    "                count+=1\n",
    "        #print('Total number params loaded for model weights = {}'.format(count))\n",
    "        \n",
    "        \n",
    "        return model\n",
    "    \n",
    "    \n",
    "def best_model(num_epochs, results):\n",
    "    auroc_train = []\n",
    "    auprc_train = []\n",
    "\n",
    "    auroc_val = []\n",
    "    auprc_val = []\n",
    "    for i in range(0, num_epochs):\n",
    "        auroc_train.append(results[i]['train_auroc'])\n",
    "        auprc_train.append(results[i]['train_auprc'])\n",
    "    \n",
    "        auroc_val.append(results[i]['val_auroc'])\n",
    "        auprc_val.append(results[i]['val_auprc'])\n",
    "    \n",
    "    max_val_auroc = max(auroc_val) \n",
    "    max_val_auprc = max(auprc_val) \n",
    "\n",
    "    print(max_val_auroc)\n",
    "    print(max_val_auprc)\n",
    "\n",
    "    max_index = auroc_val.index(max_val_auroc)\n",
    "    return max_index\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Measure time\n",
    "    startTime = time.time()\n",
    "    \n",
    "    \n",
    "    # Parse arguments\n",
    "    parser = par.initiate_parsing()\n",
    "    args = parser.parse_args()\n",
    "    job_number = args.job_number\n",
    "    task = args.task\n",
    "    \n",
    "    path = Path(args.save_dir)\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Set cuda device\n",
    "    if torch.cuda.is_available():\n",
    "        device = 'cuda'    \n",
    "    else:\n",
    "        device = 'cpu'\n",
    "    print('Using {} device...'.format(device))        \n",
    "   \n",
    "    # Load datasets and initiate dataloaders\n",
    "    print('Loading datasets...')\n",
    "    discretizer, normalizer = ehr_funcs(args)\n",
    "    ehr_train_ds, ehr_val_ds, ehr_test_ds = get_datasets(discretizer, normalizer, args)\n",
    "    cxr_train_ds, cxr_val_ds, cxr_test_ds = get_cxr_datasets(args)\n",
    "    train_dl, val_dl, test_dl = load_cxr_ehr(args, ehr_train_ds, ehr_val_ds, cxr_train_ds, cxr_val_ds, ehr_test_ds, cxr_test_ds)\n",
    "    \n",
    "    # Store arguments after loading datasets\n",
    "    with open(f\"{args.save_dir}/args/args_{job_number}.txt\", 'w') as results_file:\n",
    "        print(\"Storing arguments...\")\n",
    "        for arg in vars(args): \n",
    "            print(f\"  {arg:<40}: {getattr(args, arg)}\")\n",
    "            results_file.write(f\"  {arg:<40}: {getattr(args, arg)}\\n\")\n",
    "\n",
    "\n",
    "    ## Get model paths\n",
    "    load_dir_simclr = args.save_dir\n",
    "    if 'mortality' in args.save_dir:\n",
    "        load_dir_simclr = load_dir_simclr.replace('mortality', 'phenotyping')\n",
    "    \n",
    "    print(load_dir_simclr+\"/{}/*\".format(args.load_state))\n",
    "    paths_models = glob.glob(load_dir_simclr+\"/{}/*\".format(args.load_state))\n",
    "    print('\\nNumber of epochs =',len(paths_models))\n",
    "    num_epochs = len(paths_models)\n",
    "    max_epoch = num_epochs-1\n",
    "    print('Max epoch idx =', max_epoch)\n",
    "    \n",
    "    \n",
    "    # Create a directory to save results of current training settings\n",
    "    args.file_name = args.file_name + '-lr{}-e{}-bs{}'.format(args.lr, args.epochs, args.batch_size)\n",
    "    \n",
    "    results_path = args.save_dir+'/'+args.file_name+'/'\n",
    "    if not os.path.exists(results_path):\n",
    "        os.makedirs(results_path)\n",
    "    results_file_path = results_path+'results_label.csv' \n",
    "    \n",
    "    print(results_file_path)\n",
    "    \n",
    "    if os.path.isfile(results_file_path):\n",
    "        saved_results = pd.read_csv(results_file_path)\n",
    "        print(saved_results.head())\n",
    "        print(\"Last epoch evaluated =\", saved_results.epoch.max())\n",
    "        start_epoch = saved_results.epoch.max()+1\n",
    "    else:\n",
    "        print(\"No results for those settings and this model\")\n",
    "        start_epoch = 0\n",
    "        \n",
    "    \n",
    "    results = {}\n",
    "    load_state=args.load_state\n",
    "    \n",
    "#     start_epoch = 188\n",
    "#     num_epochs = 189\n",
    "    \n",
    "    for i in range(start_epoch, num_epochs): #num_epochs\n",
    "        print(\"------------------------------------------------------\")\n",
    "        print(\"Epoch num=\", i)\n",
    "        print(\"------------------------------------------------------\")\n",
    "            \n",
    "        \n",
    "        # Load the model weights and freeze encoders\n",
    "        print(\"Loading model...\")\n",
    "        if i <10:\n",
    "            epoch_num = '0'+str(i)\n",
    "        else:\n",
    "            epoch_num = str(i)\n",
    "        args.load_state = load_state+'_epoch_'+epoch_num\n",
    "        if args.pretrain_type == 'simclr':\n",
    "            model = SimCLR(args, train_dl)\n",
    "        \n",
    "        #print('Printing model architecture')\n",
    "        #print(model)\n",
    "        \n",
    "        # For efficiency, load features once so that we do not perform  a forward pass every time\n",
    "        if ('lineareval' in args.fusion_type) & (not args.finetune):\n",
    "            print(\"Processing features for linear evaluation...\")\n",
    "            train_dl_pr = prepare_data_features(device, model, train_dl, args.batch_size, args.fusion_layer, args.fusion_type) \n",
    "            val_dl_pr = prepare_data_features(device, model, val_dl, args.batch_size, args.fusion_layer, args.fusion_type)\n",
    "            test_dl_pr = prepare_data_features(device, model, test_dl, args.batch_size, args.fusion_layer, args.fusion_type)\n",
    "    \n",
    "        # Delete parts of the model for memory purposes\n",
    "        del model.model.cxr_model\n",
    "        del model.model.cxr_model_g\n",
    "        del model.model.ehr_model\n",
    "        del model.model.ehr_model_g\n",
    "    \n",
    "        # [1] Train LR model and store best checkpoint of the LR model based on validation AUROC \n",
    "        print('==> training')        \n",
    "        print(len(train_dl))\n",
    "        trainer = train(model, args, train_dl_pr, val_dl_pr, load_state_prefix=args.load_state_simclr)\n",
    "        \n",
    "        print(\"Best model score = \", trainer.checkpoint_callback.best_model_score)\n",
    "        print(\"Best model path = \", trainer.checkpoint_callback.best_model_path)\n",
    "        \n",
    "        # [2] Load best check point and store its training set and validation set AUROC for the respective SIMCLR epoch\n",
    "        model = load_weights(model, trainer.checkpoint_callback.best_model_path)\n",
    "                \n",
    "        results[i] = {}\n",
    "        \n",
    "        print(\"Evaluate on training set\")\n",
    "        trainer.test(model, train_dl_pr)\n",
    "        results[i]['train_auroc'] = trainer.logged_metrics['test_auroc'].item()\n",
    "        results[i]['train_auprc'] = trainer.logged_metrics['test_auprc'].item()\n",
    "        \n",
    "        if task != 'in-hospital-mortality':\n",
    "            results[i]['train_auroc_label'] = trainer.logged_metrics['auroc_label']\n",
    "            results[i]['train_auprc_label'] = trainer.logged_metrics['auprc_label']\n",
    "        \n",
    "        print(\"Evaluate on validation set\")\n",
    "        trainer.test(model, val_dl_pr)\n",
    "        results[i]['val_auroc'] = trainer.logged_metrics['test_auroc'].item()\n",
    "        results[i]['val_auprc'] = trainer.logged_metrics['test_auprc'].item()\n",
    "        \n",
    "        if task != 'in-hospital-mortality':\n",
    "            results[i]['val_auroc_label'] = trainer.logged_metrics['auroc_label']\n",
    "            results[i]['val_auprc_label'] = trainer.logged_metrics['auprc_label']\n",
    "        \n",
    "        print(\"Evaluate on test set\")\n",
    "        trainer.test(model, test_dl_pr)\n",
    "        results[i]['test_auroc'] = trainer.logged_metrics['test_auroc'].item()\n",
    "        results[i]['test_auprc'] = trainer.logged_metrics['test_auprc'].item()\n",
    "    \n",
    "        if task != 'in-hospital-mortality':\n",
    "            results[i]['test_auroc_label'] = trainer.logged_metrics['auroc_label']\n",
    "            results[i]['test_auprc_label'] = trainer.logged_metrics['auprc_label']\n",
    "        \n",
    "        # After evaluation is completed, free up the memory\n",
    "        del model\n",
    "\n",
    "        #Â Previously below were outside for loop \n",
    "        # Convert dictionary to pandas dataframe\n",
    "        new_results = pd.DataFrame.from_dict(results, orient='index').reset_index().rename(columns={'index':'epoch'})\n",
    "\n",
    "        # Append to existing results if they exist\n",
    "        if os.path.isfile(results_file_path):\n",
    "            saved_results = saved_results.append(new_results)\n",
    "        else:\n",
    "            saved_results = new_results\n",
    "            \n",
    "        saved_results = saved_results.drop_duplicates(\"epoch\")\n",
    "        saved_results.to_csv(results_file_path)\n",
    "    \n",
    "    # Compare across all epochs and choose the best model based on validation auroc\n",
    "    best_results = saved_results.loc[saved_results['val_auroc'].idxmax()]\n",
    "    print('Final results for best epoch={}:'.format(best_results.epoch), best_results)\n",
    "    \n",
    "    executionTime = (time.time() - startTime)\n",
    "    print('Execution time in minutes: ' + str(executionTime/60))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MML SSL",
   "language": "python",
   "name": "mml-ssl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
