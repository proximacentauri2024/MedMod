{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "07d8b27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import torchvision\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from torch import nn\n",
    "import torch.nn.functional  as F\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from datetime import timedelta\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c99c6f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EHRModel(nn.Module):\n",
    "\n",
    "    def __init__(self, \n",
    "                 hidden_dim: int =256, \n",
    "                 input_dim: int =76,  \n",
    "                 batch_first: bool = True, \n",
    "                 dropout: float = 0.0, \n",
    "                 layers: int = 1,\n",
    "                 projection_dim: int = 512):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layers = layers\n",
    "        for layer in range(layers):\n",
    "            setattr(self, f'layer{layer}', nn.LSTM(\n",
    "                input_dim, hidden_dim,\n",
    "                batch_first=batch_first,\n",
    "                dropout = dropout)\n",
    "            )\n",
    "            input_dim = hidden_dim\n",
    "\n",
    "        self.do = nn.Dropout(dropout)\n",
    "        self.feats_dim = hidden_dim\n",
    "        self.projection_layer = nn.Linear(hidden_dim, projection_dim)\n",
    "        self.initialize_weights()\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        for model in self.modules():\n",
    "\n",
    "            if type(model) in [nn.Linear]:\n",
    "                nn.init.xavier_uniform_(model.weight)\n",
    "                nn.init.zeros_(model.bias)\n",
    "            elif type(model) in [nn.LSTM, nn.RNN, nn.GRU]:\n",
    "                nn.init.orthogonal_(model.weight_hh_l0)\n",
    "                nn.init.xavier_uniform_(model.weight_ih_l0)\n",
    "                nn.init.zeros_(model.bias_hh_l0)\n",
    "                nn.init.zeros_(model.bias_ih_l0)\n",
    "\n",
    "    def forward(self, x, seq_lengths):\n",
    "        x = torch.nn.utils.rnn.pack_padded_sequence(x, seq_lengths, batch_first=True, enforce_sorted=False)\n",
    "        for layer in range(self.layers):\n",
    "             x, (ht, _) = getattr(self, f'layer{layer}')(x)\n",
    "        feats = ht.squeeze()\n",
    "        out = self.do(feats)\n",
    "        out = self.projection_layer(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77808954",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0452,  0.0201,  0.0941,  ...,  0.0908, -0.0756,  0.1801],\n",
       "        [-0.0902,  0.0859,  0.2003,  ..., -0.1018,  0.0363,  0.0034]],\n",
       "       grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EHRModel()(torch.randn(2,113,76),[33,69])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ad516b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CXRModel(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 backbone: str = 'resnet34',\n",
    "                 projection_dim: int = 512):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.vision_backbone = getattr(torchvision.models, backbone)(pretrained=False)\n",
    "        self.vision_backbone.fc = nn.Linear(self.vision_backbone.fc.in_features,projection_dim)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        visual_feats = self.vision_backbone(x)\n",
    "        return  visual_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27d46390",
   "metadata": {},
   "outputs": [],
   "source": [
    "cxr = CXRModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1512f74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2145,  0.5096,  0.0987,  ..., -0.6843, -1.3869,  0.7160],\n",
       "        [ 0.3406,  0.7467,  0.3305,  ..., -0.6176, -1.4298,  0.6900]],\n",
       "       grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cxr(torch.randn(2,3,224,224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "061b28f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ALIGN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 hidden_dim: int =256, \n",
    "                 input_dim: int =76, \n",
    "                 batch_first: bool = True, \n",
    "                 dropout: float = 0.0, \n",
    "                 layers: int = 1,\n",
    "                 backbone: str = 'resnet34',\n",
    "                 projection_dim: int = 512):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.cxr_encoder = CXRModel(backbone=backbone,\n",
    "                                     projection_dim=projection_dim)\n",
    "        \n",
    "        self.ehr_encoder = EHRModel(hidden_dim=hidden_dim,\n",
    "                                    input_dim=input_dim,\n",
    "                                    batch_first=batch_first,\n",
    "                                    dropout=dropout,\n",
    "                                    layers=layers,\n",
    "                                    projection_dim=projection_dim)\n",
    "        \n",
    "    def forward(self,\n",
    "               cxr: torch.Tensor,\n",
    "               ehr: torch.Tensor,\n",
    "               seq_lengths: list):\n",
    "        \n",
    "        cxr_projections = self.cxr_encoder(cxr)\n",
    "        ehr_projections = self.ehr_encoder(ehr,seq_lengths)\n",
    "        \n",
    "        return {'cxr': cxr_projections, \n",
    "                'ehr': ehr_projections}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e489b3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeds = ALIGN()(torch.randn(5,3,224,224),torch.randn(5,113,76),[33,69,77,36,96])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56029849",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cxr': tensor([[ 1.7096, -0.2067, -0.8512,  ...,  1.1037,  0.5145,  0.5686],\n",
       "         [ 1.7030, -0.4490, -0.6695,  ...,  1.1321,  0.4178,  0.3128],\n",
       "         [ 1.5802, -0.4385, -0.7142,  ...,  1.2737,  0.3301,  0.2907],\n",
       "         [ 1.5846, -0.4202, -0.4880,  ...,  0.9678,  0.3520,  0.2787],\n",
       "         [ 1.9196, -0.3036, -0.6729,  ...,  1.0678,  0.4076,  0.3792]],\n",
       "        grad_fn=<AddmmBackward>),\n",
       " 'ehr': tensor([[ 0.1214,  0.0020, -0.0869,  ..., -0.0972, -0.0283,  0.0600],\n",
       "         [ 0.0198, -0.0503,  0.0010,  ...,  0.0978, -0.0425,  0.0070],\n",
       "         [-0.1725, -0.0335,  0.0275,  ...,  0.1081,  0.0453,  0.0440],\n",
       "         [-0.0454,  0.0133, -0.0996,  ..., -0.0259,  0.1215, -0.0659],\n",
       "         [-0.1060,  0.0310,  0.0621,  ..., -0.1430, -0.1087, -0.0959]],\n",
       "        grad_fn=<AddmmBackward>)}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4dce8ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self,\n",
    "                temperature: float =0.07):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.temperature = nn.Parameter(torch.tensor(temperature))\n",
    "\n",
    "\n",
    "    def forward(self, cxr_feats, ehr_feats):\n",
    "\n",
    "        cos_sim = F.cosine_similarity(cxr_feats[:,None,:], ehr_feats[None,:,:], dim=-1)\n",
    "\n",
    "        cos_sim = cos_sim / self.temperature\n",
    "        self_mask = torch.eye(cos_sim.shape[0], dtype=torch.bool,  device=cos_sim.device)\n",
    "\n",
    "        cos_sim_negative = torch.clone(cos_sim)\n",
    "        cos_sim_negative.masked_fill_(self_mask, -9e15)\n",
    "        \n",
    "        # Compute based on img->ehr\n",
    "        nll_1 = cos_sim[self_mask] - torch.logsumexp(cos_sim_negative, dim=1)\n",
    "        \n",
    "        # Compute based on ehr->img\n",
    "        nll_2 = cos_sim[self_mask] - torch.logsumexp(cos_sim_negative, dim=0) \n",
    "\n",
    "        # Total loss \n",
    "        loss = -(nll_1 + nll_2).mean()\n",
    "                     \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a09abe1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.9495, grad_fn=<NegBackward>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ContrastiveLoss()(embeds['ehr'],embeds['cxr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20390860",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ALIGNTrainer(pl.LightningModule):\n",
    "    def __init__(self,\n",
    "\n",
    "                 hidden_dim: int =256, \n",
    "                 input_dim: int =76, \n",
    "                 batch_first: bool = True, \n",
    "                 dropout: float = 0.0, \n",
    "                 layers: int = 1,\n",
    "                 backbone: str = 'resnet34',\n",
    "                 projection_dim: int = 512,\n",
    "                 temperature: float = 0.07,\n",
    "                 lr: float = 0.0001,\n",
    "                 wd=0.001,\n",
    "                 max_epochs: int = 100):\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "        self.model = ALIGN(hidden_dim=hidden_dim,\n",
    "                          input_dim=input_dim,\n",
    "                          batch_first=batch_first,\n",
    "                          dropout=dropout,\n",
    "                          layers=layers,\n",
    "                          backbone=backbone,\n",
    "                          projection_dim=projection_dim)\n",
    "        \n",
    "\n",
    "\n",
    "        self.criterion = ContrastiveLoss(temperature=temperature)        \n",
    "        \n",
    "        \n",
    "        self.lr = lr\n",
    "        self.wd = wd\n",
    "        self.max_epochs = max_epochs\n",
    "        \n",
    "    \n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \n",
    "        ehr, cxr,_ , _, seq_lengths, _ = batch\n",
    "        ehr,seq_lengths = self._swap(ehr,seq_lengths)\n",
    "        \n",
    "        embeddings = self.model(cxr.cuda(),ehr.cuda(),seq_lengths.to('cpu'))\n",
    "        \n",
    "        loss = self.criterion(embeddings['cxr'], embeddings['ehr']) \n",
    "        self.log(\"train_loss\", loss, on_epoch= True,on_step=True , logger=True, prog_bar=True)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), \n",
    "                                      lr=self.lr,\n",
    "                                      weight_decay=self.wd)\n",
    "        \n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer=optimizer,\n",
    "                                                               eta_min=0.0,\n",
    "                                                               T_max=self.max_epochs)\n",
    "        return {'optimizer': optimizer,\n",
    "               'lr_scheduler': scheduler\n",
    "               }\n",
    "    \n",
    "    def _swap(self,ehr,seqs):\n",
    "    \n",
    "        ehr = torch.tensor(ehr,dtype= torch.float32)\n",
    "        seqs = torch.tensor(seqs, dtype= torch.float32)\n",
    "        b_size = ehr.shape[0]\n",
    "    \n",
    "        # number of samples to sap\n",
    "        count = random.randint(int(0.16*b_size),int(0.2*b_size))\n",
    "    \n",
    "        # first slice limits retrieval \n",
    "        group1_start = random.randint(0,int(0.4*b_size))\n",
    "        group1_end = group1_start + count\n",
    "        ehr1 = torch.clone(ehr[group1_start:group1_end])\n",
    "        seqs1 = torch.clone(seqs[group1_start:group1_end])\n",
    "    \n",
    "        # second slice limits retrieval\n",
    "        group2_start = random.randint(int(0.6*b_size),int(0.8*b_size))\n",
    "        group2_end = group2_start + count\n",
    "        ehr2 = torch.clone(ehr[group2_start:group2_end])\n",
    "        seqs2 = torch.clone(seqs[group2_start:group2_end])\n",
    "    \n",
    "        # perform swapping\n",
    "        ehr[group1_start:group1_end] = ehr2\n",
    "        seqs[group1_start:group1_end] = seqs2\n",
    "    \n",
    "        ehr[group2_start:group2_end] = ehr1\n",
    "        seqs[group2_start:group2_end] = seqs1\n",
    "    \n",
    "        return ehr, seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e7db7c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALIGNTrainer().training_step(z,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "411f3d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "def initiate_parsing():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    \n",
    "    # Task setup\n",
    "    parser.add_argument('--device', type=str, help='cuda device', default='0')\n",
    "    parser.add_argument('--num_gpu', type=int, help='number of gpus for training', default=1)\n",
    "    parser.add_argument('--epochs', type=int, default=300, help='number of epochs to train for')\n",
    "    parser.add_argument('--lr', type=float, default=0.0001, help='base learning rate simclr pretraining')\n",
    "    parser.add_argument('--save_dir', type=str, help='Directory where all output files are stored', default='.')\n",
    "    parser.add_argument('--labels_set', type=str, default='pheno', help='pheno, radiology')\n",
    "    parser.add_argument('--task', type=str, default='phenotyping', help='train or eval for in-hospital-mortality or phenotyping')\n",
    "    parser.add_argument('--retrive_cxr',type=str,default='recent', choices=['recent','all'], help='either to retrieve all cxr or only the most recent')\n",
    "    parser.add_argument('--data_pairs', type=str, default='paired', help='paired, ehr_only, radiology, joint_ehr')\n",
    "    parser.add_argument('--mode', type=str, default=\"train\", help='mode: train or eval')  \n",
    "    parser.add_argument('--tag', type=str, default=\"simclr train\", help='simclr_train_phenotyping')      \n",
    "    parser.add_argument('--pretrain_type', type=str, default=\"simclr\", help='type of pretraining')    \n",
    "    parser.add_argument('--file_name', type=str, default=None, help='prefix of model file name')      \n",
    "    parser.add_argument('--load_state', type=str, default=None, help='state dir path for simclr model')\n",
    "    parser.add_argument('--eval_set', type=str, default='val', help='evaluation set: val or test')\n",
    "    parser.add_argument('--job_number', type=str, default='0', help='slurm job number for jubail')\n",
    "    parser.add_argument('--eval_epoch', type=int, help='epoch to evaluate for model selection', default=0)\n",
    "\n",
    "    \n",
    "    # EHR setup\n",
    "    parser.add_argument('--load_state_ehr', type=str, default=None, help='state dir path for uni ehr model')\n",
    "    parser.add_argument('--num_classes', type=int, default=25, help='number of classes for ehr related tasks')\n",
    "    parser.add_argument('--rec_dropout', type=float, default=0.0, help=\"dropout rate for recurrent connections\")\n",
    "    parser.add_argument('--timestep', type=float, default=1.0, help=\"fixed timestep used in the dataset\")\n",
    "    parser.add_argument('--imputation', type=str, default='previous')\n",
    "    parser.add_argument('--ehr_data_root', type=str, help='Path to the ehr data', default='/scratch/fs999/shamoutlab/data/mimic-iv-extracted')\n",
    "    parser.add_argument('--layers', default=1, type=int, help='number of lstm stacked layers')\n",
    "    parser.add_argument('--dim', type=int, default=256,\n",
    "                        help='number of hidden units for uni ehr lstm model')\n",
    "\n",
    "    # CXR setup\n",
    "    parser.add_argument('--load_state_cxr', type=str, default=None, help='state dir path for uni cxr model')\n",
    "    parser.add_argument('--cxr_data_root', type=str, help='Path to the cxr data', default='/scratch/fs999/shamoutlab/data/physionet.org/files/mimic-cxr-jpg/2.0.0')\n",
    "    parser.add_argument('--vision-backbone', default='resnet34', type=str, help='[densenet121, densenet169, densenet201, resnet34]')\n",
    "    parser.add_argument('--pretrained', dest='pretrained', action='store_true',  help='load imagenet pretrained model for cxr')\n",
    "    parser.add_argument('--layer_after', default=4, type=int, help='apply mmtm/daft module after fourth layer[-1, 0,1,2,3,4] -1 indicates mmtm after every layer')\n",
    "    parser.add_argument('--vision_num_classes', default=14, type=int, help='number of cxr classes')\n",
    "    parser.add_argument('--resize', default=256, type=int, help='cxr transform resize')\n",
    "    parser.add_argument('--crop', default=224, type=int, help='cxr transform crop size')\n",
    "    parser.add_argument('--dropout', type=float, default=0.0)# TODO: double check\n",
    "    parser.add_argument('--hidden_dim', type=int, default=128)# TODO: double check\n",
    "\n",
    "\n",
    "\n",
    "    # SimCLR setup\n",
    "    parser.add_argument('--load_state_simclr', type=str, default=None, help='state dir path for simclr model')\n",
    "    parser.add_argument('--batch_size', type=int, default=256)\n",
    "    parser.add_argument('--transforms_cxr', type=str, default='simclrv2', help='set image transforms of simclrv2')\n",
    "    parser.add_argument('--temperature', type=float, default=0.01, help='simclr temperature')\n",
    "    parser.add_argument('--weight_decay', type=float, default=1e-4, help='weight decay')\n",
    "    parser.add_argument('--finetune', action='store_true',  help='finetune simclr')  \n",
    "    parser.add_argument('--dataset', type=str, default='evaluation_task', help='type of dataset to work with (all being unrestricted pairs)')\n",
    "    parser.add_argument('--width', type=int, default=1, help='width of projection module')\n",
    "    parser.add_argument('--save_features', action='store_true', help='save features after each epoch')\n",
    "    parser.add_argument('--beta_infonce', action='store_true', help='include time difference in loss computation')\n",
    "\n",
    "\n",
    "    \n",
    "    parser.add_argument('--linearclassify', action='store_true',  help='perform linear classification after simclr')  \n",
    "    parser.add_argument('--load_state_lc', type=str, default=None, help='state dir path for linear class model')\n",
    "    parser.add_argument('--lr_linearclassify', type=float, default=0.0001, help='learning rate for linear classification')\n",
    "    parser.add_argument('--epochs_linearclassify', type=int, default=100, help='number of epochs to train for for linear class')\n",
    "    parser.add_argument('--overwrite_classifier', action='store_true',  help='retrain the logistic regression model and overwrite')    \n",
    "\n",
    "    \n",
    "    # Fusion setup\n",
    "    parser.add_argument('--fusion_type', type=str, default='None', help='train or eval for fusion types [joint, early, uni_cxr, uni_ehr, lstm]')\n",
    "    parser.add_argument('--data_ratio', type=float, default=1.0, help='percentage of uppaired data samples')\n",
    "    parser.add_argument('--mmtm_ratio', type=float, default=4, help='mmtm ratio hyperparameter')\n",
    "    parser.add_argument('--fusion_layer', type=int, default=0, help='fusion layer')\n",
    "\n",
    "\n",
    "\n",
    "    # Unknown classify later \n",
    "    parser.add_argument('--beta_1', type=float, default=0.9,\n",
    "                        help='beta_1 param for Adam optimizer')\n",
    "    parser.add_argument('--normalizer_state', type=str, default=None,\n",
    "                        help='Path to a state file of a normalizer. Leave none if you want to use one of the provided ones.')\n",
    "    \n",
    "    \n",
    "    # Vicreg\n",
    "    parser.add_argument('--sim_coeff', type=float, default=25, help='vicreg sim coeff')\n",
    "    parser.add_argument('--std_coeff', type=float, default=25, help='vicreg std coeff')\n",
    "    parser.add_argument('--cov_coeff', type=float, default=1, help='vicreg cov coeff')\n",
    "    parser.add_argument('--vicreg', action='store_true', help='vicreg loss computation')\n",
    "    \n",
    "    parser.add_argument('-f')\n",
    "    return parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "id": "dc75ce2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = initiate_parsing()\n",
    "args = parser.parse_args()\n",
    "args.beta_infonce =True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "id": "ed104a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device = 0\n",
      "num_gpu = 1\n",
      "epochs = 300\n",
      "lr = 0.0001\n",
      "save_dir = .\n",
      "labels_set = pheno\n",
      "task = phenotyping\n",
      "retrive_cxr = recent\n",
      "data_pairs = paired\n",
      "mode = train\n",
      "tag = simclr train\n",
      "pretrain_type = simclr\n",
      "file_name = None\n",
      "load_state = None\n",
      "eval_set = val\n",
      "job_number = 0\n",
      "eval_epoch = 0\n",
      "load_state_ehr = None\n",
      "num_classes = 25\n",
      "rec_dropout = 0.0\n",
      "timestep = 1.0\n",
      "imputation = previous\n",
      "ehr_data_root = /scratch/fs999/shamoutlab/data/mimic-iv-extracted\n",
      "layers = 1\n",
      "dim = 256\n",
      "load_state_cxr = None\n",
      "cxr_data_root = /scratch/fs999/shamoutlab/data/physionet.org/files/mimic-cxr-jpg/2.0.0\n",
      "vision_backbone = resnet34\n",
      "pretrained = False\n",
      "layer_after = 4\n",
      "vision_num_classes = 14\n",
      "resize = 256\n",
      "crop = 224\n",
      "dropout = 0.0\n",
      "hidden_dim = 128\n",
      "load_state_simclr = None\n",
      "batch_size = 256\n",
      "transforms_cxr = simclrv2\n",
      "temperature = 0.01\n",
      "weight_decay = 0.0001\n",
      "finetune = False\n",
      "dataset = evaluation_task\n",
      "width = 1\n",
      "save_features = False\n",
      "beta_infonce = True\n",
      "linearclassify = False\n",
      "load_state_lc = None\n",
      "lr_linearclassify = 0.0001\n",
      "epochs_linearclassify = 100\n",
      "overwrite_classifier = False\n",
      "fusion_type = None\n",
      "data_ratio = 1.0\n",
      "mmtm_ratio = 4\n",
      "fusion_layer = 0\n",
      "beta_1 = 0.9\n",
      "normalizer_state = None\n",
      "sim_coeff = 25\n",
      "std_coeff = 25\n",
      "cov_coeff = 1\n",
      "vicreg = False\n",
      "f = /home/sas10092/.local/share/jupyter/runtime/kernel-c9361d4b-9c90-472f-be3c-9de6ab838955.json\n"
     ]
    }
   ],
   "source": [
    "for arg in vars(args):\n",
    "    print (arg,'=' ,getattr(args, arg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "id": "f4769e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pandas as pd \n",
    "import json\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import glob\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "id": "9cd9760a",
   "metadata": {},
   "outputs": [],
   "source": [
    "R_CLASSES  = ['Atelectasis', 'Cardiomegaly', 'Consolidation', 'Edema',\n",
    "       'Enlarged Cardiomediastinum', 'Fracture', 'Lung Lesion',\n",
    "       'Lung Opacity', 'No Finding', 'Pleural Effusion', 'Pleural Other',\n",
    "       'Pneumonia', 'Pneumothorax', 'Support Devices']\n",
    "\n",
    "CLASSES = [\n",
    "       'Acute and unspecified renal failure', 'Acute cerebrovascular disease',\n",
    "       'Acute myocardial infarction', 'Cardiac dysrhythmias',\n",
    "       'Chronic kidney disease',\n",
    "       'Chronic obstructive pulmonary disease and bronchiectasis',\n",
    "       'Complications of surgical procedures or medical care',\n",
    "       'Conduction disorders', 'Congestive heart failure; nonhypertensive',\n",
    "       'Coronary atherosclerosis and other heart disease',\n",
    "       'Diabetes mellitus with complications',\n",
    "       'Diabetes mellitus without complication',\n",
    "       'Disorders of lipid metabolism', 'Essential hypertension',\n",
    "       'Fluid and electrolyte disorders', 'Gastrointestinal hemorrhage',\n",
    "       'Hypertension with complications and secondary hypertension',\n",
    "       'Other liver diseases', 'Other lower respiratory disease',\n",
    "       'Other upper respiratory disease',\n",
    "       'Pleurisy; pneumothorax; pulmonary collapse',\n",
    "       'Pneumonia (except that caused by tuberculosis or sexually transmitted disease)',\n",
    "       'Respiratory failure; insufficiency; arrest (adult)',\n",
    "       'Septicemia (except in labor)', 'Shock'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266edb12",
   "metadata": {},
   "source": [
    "# CXR Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "id": "8365f1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transforms(args):\n",
    "    normalize = transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    \n",
    "    train_transforms = []\n",
    "    train_transforms.append(transforms.Resize(args.resize))\n",
    "    train_transforms.append(transforms.RandomHorizontalFlip())\n",
    "    train_transforms.append(transforms.RandomAffine(degrees=45, scale=(.85, 1.15), shear=0, translate=(0.15, 0.15)))\n",
    "    train_transforms.append(transforms.CenterCrop(224))#args.crop))\n",
    "    train_transforms.append(transforms.ToTensor())\n",
    "    #train_transforms.append(normalize)      \n",
    "\n",
    "\n",
    "    test_transforms = []\n",
    "    test_transforms.append(transforms.Resize(args.resize))\n",
    "    test_transforms.append(transforms.CenterCrop(224))#args.crop))\n",
    "    test_transforms.append(transforms.ToTensor())\n",
    "    #test_transforms.append(normalize)\n",
    "\n",
    "    return train_transforms, test_transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "id": "5f8b7588",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Clip(object):\n",
    "    \"\"\"Transformation to clip image values between 0 and 1.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        return torch.clip(sample, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "id": "f9a4a2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomCrop(object):\n",
    "    \"Randomly crop an image\"\n",
    "    \n",
    "    def __call__(self, sample):\n",
    "        resize = 256\n",
    "        #print(np.random.uniform(0.4*resize,resize,1))\n",
    "        random_crop_size = int(np.random.uniform(0.6*resize,resize,1))\n",
    "        sample=transforms.RandomCrop(random_crop_size)(sample)\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "id": "90e71349",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomColorDistortion(object):\n",
    "    \"Apply random color distortions to the image\"\n",
    "    \n",
    "    def __call__(self, sample):\n",
    "        resize=256\n",
    "\n",
    "        # Random color distortion\n",
    "        strength = 1.0 # 1.0 imagenet setting and CIFAR uses 0.5\n",
    "        brightness = 0.8 * strength \n",
    "        contrast = 0.8 * strength\n",
    "        saturation = 0.8 * strength\n",
    "        hue = 0.2 * strength\n",
    "        prob = np.random.uniform(0,1,1) \n",
    "        if prob < 0.8:\n",
    "            sample=transforms.ColorJitter(brightness, contrast, saturation, hue)(sample)\n",
    "\n",
    "        # Random Grayscale\n",
    "        sample=transforms.RandomGrayscale(p=0.2)(sample)\n",
    "\n",
    "        # Gaussian blur also based on imagenet but not used for CIFAR\n",
    "        #prob = np.random.uniform(0,1,1)\n",
    "        #if prob < 0.3:\n",
    "        #    sample=transforms.GaussianBlur(kernel_size=resize//10)(sample)\n",
    "        #    sample=transforms.Pad(0)(sample)\n",
    "        return sample \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "id": "cdd5a0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transforms_simclr(args):\n",
    "    normalize = transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    \n",
    "    train_transforms = []\n",
    "    # Resize all images to same size, then randomly crop and resize again\n",
    "    train_transforms.append(transforms.Resize([args.resize, args.resize]))\n",
    "    # Random affine\n",
    "    train_transforms.append(transforms.RandomAffine(degrees=(-45, 45), translate=(0.1,0.1), scale=(0.7, 1.5), shear=(-25, 25)))\n",
    "    # Random crop\n",
    "    train_transforms.append(RandomCrop())\n",
    "    # Resize again\n",
    "    # train_transforms.append(transforms.Resize([args.resize, args.resize], interpolation=3))\n",
    "    train_transforms.append(transforms.Resize([224, 224], interpolation=3))\n",
    "    # Random horizontal flip \n",
    "    train_transforms.append(transforms.RandomHorizontalFlip())\n",
    "    # Random color distortions\n",
    "    train_transforms.append(RandomColorDistortion())\n",
    "    # Convert to tensor\n",
    "    train_transforms.append(transforms.ToTensor())\n",
    "    # Clip values between 0 and 1 and normalize\n",
    "    #train_transforms.append(Clip())\n",
    "    #train_transforms.append(normalize)      \n",
    "\n",
    "    test_transforms = []\n",
    "    # Resize all images to same size, then center crop and resize again\n",
    "    test_transforms.append(transforms.Resize([args.resize, args.resize]))\n",
    "    crop_proportion=0.875\n",
    "    test_transforms.append(transforms.CenterCrop([int(0.875*args.resize), int(0.875*args.resize)]))\n",
    "    # test_transforms.append(transforms.Resize([args.resize, args.resize], interpolation=3))\n",
    "    test_transforms.append(transforms.Resize([224, 224], interpolation=3))\n",
    "    #Convert to tensor\n",
    "    test_transforms.append(transforms.ToTensor())\n",
    "    # Clip values between 0 and 1 and normalize\n",
    "    #test_transforms.append(Clip())\n",
    "    #test_transforms.append(normalize)\n",
    "\n",
    "    return train_transforms, test_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "id": "6db0534e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note this function needs to be editted to mimic function above \n",
    "def visualize_transforms_simclr(args, orig_img, split='train'):\n",
    "    # Create array of images \n",
    "    print(orig_img)\n",
    "    new_images = [orig_img]\n",
    "    tt = ['Original image']\n",
    "    #normalize = transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    if split == 'train':\n",
    "        # Resize all images to same size\n",
    "        new_images = new_images + [transforms.Resize([args.resize, args.resize])(orig_img)]\n",
    "        tt = tt + ['Resize original image']\n",
    "        # Random affine\n",
    "        new_images = new_images + [transforms.RandomAffine(degrees=(-45, 45), translate=(0.1,0.1), scale=(0.7, 1.5), shear=(-25, 25))(new_images[-1])]\n",
    "        tt = tt + ['Random affine']\n",
    "        # Random crop\n",
    "        new_images = new_images + [RandomCrop()(new_images[-1])]\n",
    "        tt = tt + ['Random Crop']\n",
    "        # Resize to 256 x 256\n",
    "        new_images = new_images + [transforms.Resize([args.resize, args.resize], interpolation=3)(new_images[-1])]\n",
    "        tt = tt + ['Resize patch']\n",
    "        # Random horizontal flip \n",
    "        new_images = new_images + [transforms.RandomHorizontalFlip()(new_images[-1])]\n",
    "        tt = tt + ['Random horizontal flip']\n",
    "        # Random color distortions\n",
    "        new_images = new_images + [RandomColorDistortion()(new_images[-1])]\n",
    "        tt = tt + ['Random color distortion']\n",
    "        \n",
    "        # Convert all to tensors\n",
    "        for i in range(0, len(new_images)):\n",
    "            new_images[i]=transforms.ToTensor()(new_images[i])\n",
    "#         # Clip values between 0 and 1 and normalize\n",
    "#         new_images = new_images + [Clip()(new_images[-1])]\n",
    "#         tt = tt + ['Clip values (0,1)']\n",
    "#         # Normalize values\n",
    "#         new_images = new_images + [normalize(new_images[-1])]\n",
    "#         tt = tt + ['Normalize values']\n",
    "    return new_images, tt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "id": "cfa9b697",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MIMICCXR(Dataset):\n",
    "    def __init__(self, paths, args, transform=None, split='train'):\n",
    "        self.data_dir = args.cxr_data_root\n",
    "        self.args = args\n",
    "        self.CLASSES  = ['Atelectasis', 'Cardiomegaly', 'Consolidation', 'Edema',\n",
    "       'Enlarged Cardiomediastinum', 'Fracture', 'Lung Lesion',\n",
    "       'Lung Opacity', 'No Finding', 'Pleural Effusion', 'Pleural Other',\n",
    "       'Pneumonia', 'Pneumothorax', 'Support Devices']\n",
    "        self.filenames_to_path = {path.split('/')[-1].split('.')[0]: path for path in paths}\n",
    "\n",
    "        metadata = pd.read_csv(f'{self.data_dir}/mimic-cxr-2.0.0-metadata.csv')\n",
    "        labels = pd.read_csv(f'{self.data_dir}/mimic-cxr-2.0.0-chexpert.csv')\n",
    "        labels[self.CLASSES] = labels[self.CLASSES].fillna(0)\n",
    "        labels = labels.replace(-1.0, 0.0)\n",
    "        \n",
    "        splits = pd.read_csv(f'{self.data_dir}/mimic-cxr-ehr-split.csv')\n",
    "\n",
    "\n",
    "        metadata_with_labels = metadata.merge(labels[self.CLASSES+['study_id'] ], how='inner', on='study_id')\n",
    "\n",
    "\n",
    "        self.filesnames_to_labels = dict(zip(metadata_with_labels['dicom_id'].values, metadata_with_labels[self.CLASSES].values))\n",
    "        self.filenames_loaded = splits.loc[splits.split==split]['dicom_id'].values\n",
    "        self.transform = transform\n",
    "        self.filenames_loaded = [filename  for filename in self.filenames_loaded if filename in self.filesnames_to_labels]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if isinstance(index, str):\n",
    "            img = Image.open(self.filenames_to_path[index]).convert('RGB')\n",
    "            #print(self.filenames_to_path[index])\n",
    "            labels = torch.tensor(self.filesnames_to_labels[index]).float()\n",
    "            if self.transform is not None:\n",
    "                img = self.transform(img)\n",
    "            return img, labels\n",
    "          \n",
    "        \n",
    "        filename = self.filenames_loaded[index]\n",
    "        img = Image.open(self.filenames_to_path[filename]).convert('RGB')\n",
    "        labels = torch.tensor(self.filesnames_to_labels[filename]).float()\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        return img, labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.filenames_loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "id": "3f6a9852",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cxr_datasets(args):\n",
    "    if args.transforms_cxr=='simclrv2':\n",
    "        print(\"Appling SimCLR image transforms...\")\n",
    "        train_transforms, test_transforms = get_transforms_simclr(args)\n",
    "    else:\n",
    "        print(\"Applying linear evaluation transforms...\")\n",
    "        train_transforms, test_transforms = get_transforms(args)\n",
    "\n",
    "    data_dir = args.cxr_data_root\n",
    "    filepath = f'{args.cxr_data_root}/new_paths.npy'\n",
    "    if os.path.exists(filepath):\n",
    "        paths = np.load(filepath)\n",
    "    else:\n",
    "        paths = glob.glob(f'{data_dir}/resized/**/*.jpg', recursive = True)\n",
    "        np.save(filepath, paths)\n",
    "    \n",
    "    dataset_train = MIMICCXR(paths, args, split='train', transform=transforms.Compose(train_transforms))\n",
    "    dataset_validate = MIMICCXR(paths, args, split='validate', transform=transforms.Compose(test_transforms),)\n",
    "    dataset_test = MIMICCXR(paths, args, split='test', transform=transforms.Compose(test_transforms),)\n",
    "\n",
    "    return dataset_train, dataset_validate, dataset_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee8188e",
   "metadata": {},
   "source": [
    "# EHR utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "id": "44ab8a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import platform\n",
    "import pickle\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "id": "b99809e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discretizer:\n",
    "    def __init__(self, timestep=0.8, store_masks=True, impute_strategy='zero', start_time='zero',\n",
    "                 config_path= '../ehr_utils/resources/discretizer_config.json'):\n",
    "\n",
    "        with open(config_path) as f:\n",
    "            config = json.load(f)\n",
    "            self._id_to_channel = config['id_to_channel']\n",
    "            self._channel_to_id = dict(zip(self._id_to_channel, range(len(self._id_to_channel))))\n",
    "            self._is_categorical_channel = config['is_categorical_channel']\n",
    "            self._possible_values = config['possible_values']\n",
    "            self._normal_values = config['normal_values']\n",
    "\n",
    "        self._header = [\"Hours\"] + self._id_to_channel\n",
    "        self._timestep = timestep\n",
    "        self._store_masks = store_masks\n",
    "        self._start_time = start_time\n",
    "        self._impute_strategy = impute_strategy\n",
    "\n",
    "        # for statistics\n",
    "        self._done_count = 0\n",
    "        self._empty_bins_sum = 0\n",
    "        self._unused_data_sum = 0\n",
    "\n",
    "    def transform(self, X, header=None, end=None):\n",
    "        if header is None:\n",
    "            header = self._header\n",
    "        assert header[0] == \"Hours\"\n",
    "        eps = 1e-6\n",
    "\n",
    "        N_channels = len(self._id_to_channel)\n",
    "        ts = [float(row[0]) for row in X]\n",
    "        for i in range(len(ts) - 1):\n",
    "            assert ts[i] < ts[i+1] + eps\n",
    "\n",
    "        if self._start_time == 'relative':\n",
    "            first_time = ts[0]\n",
    "        elif self._start_time == 'zero':\n",
    "            first_time = 0\n",
    "        else:\n",
    "            raise ValueError(\"start_time is invalid\")\n",
    "\n",
    "        if end is None:\n",
    "            max_hours = max(ts) - first_time\n",
    "        else:\n",
    "            max_hours = end - first_time\n",
    "\n",
    "        N_bins = int(max_hours / self._timestep + 1.0 - eps)\n",
    "\n",
    "        cur_len = 0\n",
    "        begin_pos = [0 for i in range(N_channels)]\n",
    "        end_pos = [0 for i in range(N_channels)]\n",
    "        for i in range(N_channels):\n",
    "            channel = self._id_to_channel[i]\n",
    "            begin_pos[i] = cur_len\n",
    "            if self._is_categorical_channel[channel]:\n",
    "                end_pos[i] = begin_pos[i] + len(self._possible_values[channel])\n",
    "            else:\n",
    "                end_pos[i] = begin_pos[i] + 1\n",
    "            cur_len = end_pos[i]\n",
    "\n",
    "        data = np.zeros(shape=(N_bins, cur_len), dtype=float)\n",
    "        mask = np.zeros(shape=(N_bins, N_channels), dtype=int)\n",
    "        original_value = [[\"\" for j in range(N_channels)] for i in range(N_bins)]\n",
    "        total_data = 0\n",
    "        unused_data = 0\n",
    "\n",
    "        def write(data, bin_id, channel, value, begin_pos):\n",
    "            channel_id = self._channel_to_id[channel]\n",
    "            if self._is_categorical_channel[channel]:\n",
    "                # print(\"list: \", self._possible_values[channel], \"val: \", value, \"channel:\", channel)\n",
    "                category_id = self._possible_values[channel].index(value)\n",
    "                N_values = len(self._possible_values[channel])\n",
    "                one_hot = np.zeros((N_values,))\n",
    "                one_hot[category_id] = 1\n",
    "                for pos in range(N_values):\n",
    "                    data[bin_id, begin_pos[channel_id] + pos] = one_hot[pos]\n",
    "            else:\n",
    "                data[bin_id, begin_pos[channel_id]] = float(value)\n",
    "\n",
    "        for row in X:\n",
    "            t = float(row[0]) - first_time\n",
    "            if t > max_hours + eps:\n",
    "                continue\n",
    "            bin_id = int(t / self._timestep - eps)\n",
    "            assert 0 <= bin_id < N_bins\n",
    "\n",
    "            for j in range(1, len(row)):\n",
    "                if row[j] == \"\":\n",
    "                    continue\n",
    "                channel = header[j]\n",
    "                channel_id = self._channel_to_id[channel]\n",
    "\n",
    "                total_data += 1\n",
    "                if mask[bin_id][channel_id] == 1:\n",
    "                    unused_data += 1\n",
    "                mask[bin_id][channel_id] = 1\n",
    "\n",
    "                write(data, bin_id, channel, row[j], begin_pos)\n",
    "                original_value[bin_id][channel_id] = row[j]\n",
    "\n",
    "        # impute missing values\n",
    "\n",
    "        if self._impute_strategy not in ['zero', 'normal_value', 'previous', 'next']:\n",
    "            raise ValueError(\"impute strategy is invalid\")\n",
    "\n",
    "        if self._impute_strategy in ['normal_value', 'previous']:\n",
    "            prev_values = [[] for i in range(len(self._id_to_channel))]\n",
    "            for bin_id in range(N_bins):\n",
    "                for channel in self._id_to_channel:\n",
    "                    channel_id = self._channel_to_id[channel]\n",
    "                    if mask[bin_id][channel_id] == 1:\n",
    "                        prev_values[channel_id].append(original_value[bin_id][channel_id])\n",
    "                        continue\n",
    "                    if self._impute_strategy == 'normal_value':\n",
    "                        imputed_value = self._normal_values[channel]\n",
    "                    if self._impute_strategy == 'previous':\n",
    "                        if len(prev_values[channel_id]) == 0:\n",
    "                            imputed_value = self._normal_values[channel]\n",
    "                        else:\n",
    "                            imputed_value = prev_values[channel_id][-1]\n",
    "                    write(data, bin_id, channel, imputed_value, begin_pos)\n",
    "\n",
    "        if self._impute_strategy == 'next':\n",
    "            prev_values = [[] for i in range(len(self._id_to_channel))]\n",
    "            for bin_id in range(N_bins-1, -1, -1):\n",
    "                for channel in self._id_to_channel:\n",
    "                    channel_id = self._channel_to_id[channel]\n",
    "                    if mask[bin_id][channel_id] == 1:\n",
    "                        prev_values[channel_id].append(original_value[bin_id][channel_id])\n",
    "                        continue\n",
    "                    if len(prev_values[channel_id]) == 0:\n",
    "                        imputed_value = self._normal_values[channel]\n",
    "                    else:\n",
    "                        imputed_value = prev_values[channel_id][-1]\n",
    "                    write(data, bin_id, channel, imputed_value, begin_pos)\n",
    "\n",
    "        empty_bins = np.sum([1 - min(1, np.sum(mask[i, :])) for i in range(N_bins)])\n",
    "        self._done_count += 1\n",
    "        self._empty_bins_sum += empty_bins / (N_bins + eps)\n",
    "        self._unused_data_sum += unused_data / (total_data + eps)\n",
    "\n",
    "        if self._store_masks:\n",
    "            data = np.hstack([data, mask.astype(np.float32)])\n",
    "\n",
    "        # create new header\n",
    "        new_header = []\n",
    "        for channel in self._id_to_channel:\n",
    "            if self._is_categorical_channel[channel]:\n",
    "                values = self._possible_values[channel]\n",
    "                for value in values:\n",
    "                    new_header.append(channel + \"->\" + value)\n",
    "            else:\n",
    "                new_header.append(channel)\n",
    "\n",
    "        if self._store_masks:\n",
    "            for i in range(len(self._id_to_channel)):\n",
    "                channel = self._id_to_channel[i]\n",
    "                new_header.append(\"mask->\" + channel)\n",
    "\n",
    "        new_header = \",\".join(new_header)\n",
    "\n",
    "        return (data, new_header)\n",
    "\n",
    "    def print_statistics(self):\n",
    "        print(\"statistics of discretizer:\")\n",
    "        print(\"\\tconverted {} examples\".format(self._done_count))\n",
    "        print(\"\\taverage unused data = {:.2f} percent\".format(100.0 * self._unused_data_sum / self._done_count))\n",
    "        print(\"\\taverage empty  bins = {:.2f} percent\".format(100.0 * self._empty_bins_sum / self._done_count))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "id": "2d7e04ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalizer:\n",
    "    def __init__(self, fields=None):\n",
    "        self._means = None\n",
    "        self._stds = None\n",
    "        self._fields = None\n",
    "        if fields is not None:\n",
    "            self._fields = [col for col in fields]\n",
    "\n",
    "        self._sum_x = None\n",
    "        self._sum_sq_x = None\n",
    "        self._count = 0\n",
    "\n",
    "    def _feed_data(self, x):\n",
    "        x = np.array(x)\n",
    "        self._count += x.shape[0]\n",
    "        if self._sum_x is None:\n",
    "            self._sum_x = np.sum(x, axis=0)\n",
    "            self._sum_sq_x = np.sum(x**2, axis=0)\n",
    "        else:\n",
    "            self._sum_x += np.sum(x, axis=0)\n",
    "            self._sum_sq_x += np.sum(x**2, axis=0)\n",
    "\n",
    "    def _save_params(self, save_file_path):\n",
    "        eps = 1e-7\n",
    "        with open(save_file_path, \"wb\") as save_file:\n",
    "            N = self._count\n",
    "            self._means = 1.0 / N * self._sum_x\n",
    "            self._stds = np.sqrt(1.0/(N - 1) * (self._sum_sq_x - 2.0 * self._sum_x * self._means + N * self._means**2))\n",
    "            self._stds[self._stds < eps] = eps\n",
    "            pickle.dump(obj={'means': self._means,\n",
    "                             'stds': self._stds},\n",
    "                        file=save_file,\n",
    "                        protocol=2)\n",
    "\n",
    "    def load_params(self, load_file_path):\n",
    "        with open(load_file_path, \"rb\") as load_file:\n",
    "            if platform.python_version()[0] == '2':\n",
    "                dct = pickle.load(load_file)\n",
    "            else:\n",
    "                dct = pickle.load(load_file, encoding='latin1')\n",
    "            self._means = dct['means']\n",
    "            self._stds = dct['stds']\n",
    "\n",
    "    def transform(self, X):\n",
    "        if self._fields is None:\n",
    "            fields = range(X.shape[1])\n",
    "        else:\n",
    "            fields = self._fields\n",
    "        ret = 1.0 * X\n",
    "        for col in fields:\n",
    "            ret[:, col] = (X[:, col] - self._means[col]) / self._stds[col]\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f57a3cf",
   "metadata": {},
   "source": [
    "# EHR Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "id": "44103d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "discretizer = Discretizer()\n",
    "normalizer = Normalizer()\n",
    "normalizer.load_params('../ph_ts0.8.input_str:previous.start_time:zero.normalizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "id": "e439e102",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTransform(object):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        views,\n",
    "        normal_values,\n",
    "        _is_categorical_channel,\n",
    "        augmentation,\n",
    "        begin_pos\n",
    "    ):\n",
    "        self.views = views\n",
    "        self.normal_values = normal_values\n",
    "        self.rows = np.array([value for value in self.normal_values.values()])\n",
    "        self.augmentation = augmentation\n",
    "        self.continuous_variable = [0 if _is_categorical_channel[key] == True else 1 for key in _is_categorical_channel]\n",
    "        self.begin_pos = begin_pos\n",
    "        \n",
    "    def vertical_mask(self, data, max_percent=0.4):\n",
    "        # mask over each timestep (t, features)\n",
    "        length = data.shape[0]\n",
    "        if length < 4:\n",
    "            return data\n",
    "        size = int(np.random.randint(low=0, high=max(int(max_percent*length),1), size=1))\n",
    "        a = np.zeros(length , dtype=int)\n",
    "        a[:size] = 1\n",
    "        np.random.shuffle(a)\n",
    "        a = a.astype(bool)\n",
    "        data[a,1:] = self.rows\n",
    "        return data\n",
    "\n",
    "    def horizontal_mask(self, data, max_percent=0.4):\n",
    "        # mask over each feature (t, features)\n",
    "        length = data.shape[1] - 1\n",
    "        size = int(np.random.randint(low=0, high=max(int(max_percent*length),1), size=1))\n",
    "        features = np.unique(np.random.randint(low=1, high=length, size=size))\n",
    "        for i in features:\n",
    "            data[:,i+1] = self.normal_values[i]\n",
    "        return data\n",
    "    \n",
    "    def drop_start(self, data, max_percent=0.4):\n",
    "        length = data.shape[0]\n",
    "        start = int(np.random.randint(low=0, high=max(int(max_percent*length),1), size=1))\n",
    "        return data[start:,:]\n",
    "\n",
    "    def gaussian_blur(self, data):\n",
    "        mean, std = 1,0 \n",
    "        data[:, self.begin_pos] = data[:, self.begin_pos]  + np.random.normal(mean, std, (data.shape[0], len(self.begin_pos)))\n",
    "        return data\n",
    "\n",
    "    def rotation(self, data):\n",
    "        if choice([0,1]):\n",
    "            return np.flip(data, axis=0)\n",
    "        return data\n",
    "\n",
    "    def downsample(self, data):\n",
    "        if data.shape[0] < 20:\n",
    "            return data\n",
    "        step = choice([1, 2, 3])\n",
    "        return data[::step]\n",
    "\n",
    "    def __call__(self, data):\n",
    "        data_views = []                    \n",
    "        data_views.append(self.vertical_mask(data))\n",
    "        data_views.append(self.horizontal_mask(data))\n",
    "        data_views.append(self.horizontal_mask(self.vertical_mask(data)))\n",
    "        data_views.append((self.drop_start(data)))\n",
    "        data_views.append(data)\n",
    "\n",
    "        return data_views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "id": "022b2df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transforms(args):\n",
    "    normalize = transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    \n",
    "    train_transforms = []\n",
    "    train_transforms.append(transforms.Resize(args.resize))\n",
    "    train_transforms.append(transforms.RandomHorizontalFlip())\n",
    "    train_transforms.append(transforms.RandomAffine(degrees=45, scale=(.85, 1.15), shear=0, translate=(0.15, 0.15)))\n",
    "    train_transforms.append(transforms.CenterCrop(args.crop))\n",
    "    train_transforms.append(transforms.ToTensor())\n",
    "    #train_transforms.append(normalize)      \n",
    "\n",
    "\n",
    "    test_transforms = []\n",
    "    test_transforms.append(transforms.Resize(args.resize))\n",
    "    test_transforms.append(transforms.CenterCrop(args.crop))\n",
    "    test_transforms.append(transforms.ToTensor())\n",
    "    #test_transforms.append(normalize)\n",
    "\n",
    "    return train_transforms, test_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "id": "d2d46202",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EHRdataset(Dataset):\n",
    "    def __init__(self, discretizer, normalizer, listfile, dataset_dir, return_names=True, period_length=48.0, transforms=None):\n",
    "        self.return_names = return_names\n",
    "        self.discretizer = discretizer\n",
    "        self.normalizer = normalizer\n",
    "        self._period_length = period_length\n",
    "\n",
    "        self._dataset_dir = dataset_dir\n",
    "        listfile_path = listfile\n",
    "        with open(listfile_path, \"r\") as lfile:\n",
    "            self._data = lfile.readlines()\n",
    "        self._listfile_header = self._data[0]\n",
    "        self.CLASSES = self._listfile_header.strip().split(',')[3:]\n",
    "        self._data = self._data[1:]\n",
    "        self.transforms = transforms\n",
    "\n",
    "\n",
    "        self._data = [line.split(',') for line in self._data]\n",
    "        self.data_map = {\n",
    "            mas[0]: {\n",
    "                'labels': list(map(int, mas[3:])),\n",
    "                'stay_id': float(mas[2]),\n",
    "                'time': float(mas[1]),\n",
    "                }\n",
    "                for mas in self._data\n",
    "        }\n",
    "\n",
    "        self.names = list(self.data_map.keys())\n",
    "    \n",
    "#     def _read_timeseries(self, ts_filename):\n",
    "        \n",
    "#         ret = []\n",
    "#         with open(os.path.join(self._dataset_dir, ts_filename), \"r\") as tsfile:\n",
    "#             header = tsfile.readline().strip().split(',')\n",
    "#             assert header[0] == \"Hours\"\n",
    "#             for line in tsfile:\n",
    "#                 mas = line.strip().split(',')\n",
    "#                 ret.append(np.array(mas))\n",
    "#         return (np.stack(ret), header)\n",
    "\n",
    "    def read_timeseries(self,ts_filename, lower_bound=0,upper_bound=12):\n",
    "        \n",
    "        ret = []\n",
    "        with open(os.path.join(self._dataset_dir, ts_filename), \"r\") as tsfile:\n",
    "            header = tsfile.readline().strip().split(',')\n",
    "            assert header[0] == \"Hours\"\n",
    "            for line in tsfile:\n",
    "                mas = line.strip().split(',')\n",
    "            \n",
    "                t = float(mas[0])\n",
    "                if t < lower_bound:\n",
    "                    continue\n",
    "                elif (t> lower_bound) & (t <upper_bound) :\n",
    "                    ret.append(np.array(mas))\n",
    "                elif t > upper_bound:\n",
    "                    break\n",
    "            \n",
    "    #             if time_bound is not None:\n",
    "    #                 t = float(mas[0])\n",
    "    #                 if t > time_bound + 1e-6:\n",
    "    #                     break\n",
    "    #             ret.append(np.array(mas))\n",
    "        try: \n",
    "            return (np.stack(ret), header)\n",
    "        except ValueError:\n",
    "            \n",
    "            ret = ([['0.11666666666666667', '', '', '', '', '', '', '', '', '109', '',\n",
    "                     '', '', '30', '', '', '', ''],\n",
    "                    ['0.16666666666666666', '', '61.0', '', '', '', '', '', '', '109',\n",
    "                    '', '64', '97.0', '29', '74.0', '', '', '']])\n",
    "            return (np.stack(ret), header)\n",
    "            \n",
    "         \n",
    "    \n",
    "#     def read_by_file_name(self, index):\n",
    "#         t = self.data_map[index]['time']\n",
    "#         y = self.data_map[index]['labels']\n",
    "#         stay_id = self.data_map[index]['stay_id']\n",
    "#         (X, header) = self._read_timeseries(index)\n",
    "#         print(index)\n",
    "#         return {\"X\": X,\n",
    "#                 \"t\": t,\n",
    "#                 \"y\": y,\n",
    "#                 'stay_id': stay_id,\n",
    "#                 \"header\": header,\n",
    "#                 \"name\": index}\n",
    "\n",
    "    def read_by_file_name(self,index, lower_bound=0,upper_bound=12):\n",
    "        t = self.data_map[index]['time'] #if upper_bound is None else upper_bound\n",
    "        y = self.data_map[index]['labels']\n",
    "        stay_id = self.data_map[index]['stay_id']\n",
    "        (X, header) = self.read_timeseries(index, lower_bound=lower_bound,upper_bound=upper_bound)\n",
    "        \n",
    "        return {\"X\": X,\n",
    "                \"t\": t,\n",
    "                \"y\": y,\n",
    "                'stay_id': stay_id,\n",
    "                \"header\": header,\n",
    "                \"name\": index}\n",
    "\n",
    "    def __getitem__(self, index,lower,upper):\n",
    "        if isinstance(index, int):\n",
    "            index = self.names[index]\n",
    "        ret = self.read_by_file_name(index,lower,upper)\n",
    "        data = ret[\"X\"]\n",
    "#         print(index)\n",
    "        ts = data.shape[0]#ret[\"t\"] if ret['t'] > 0.0 else self._period_length\n",
    "        \n",
    "        \n",
    "        ## Added block\n",
    "        if self.transforms is not None:\n",
    "            data = self.transforms(data)\n",
    "            \n",
    "            for i in range(len(data)):\n",
    "                data[i] = self.discretizer.transform(data[i], end=ts)[0]\n",
    "                print(data[i]).shape\n",
    "                if 'gaussian' in self.transforms.augmentation and i != 0:\n",
    "                    data[i] = self.transforms.gaussian_blur(data[i])\n",
    "                if 'sampling' in self.transforms.augmentation and i != 0: #carry last value forward \n",
    "                    data[i] = self.transforms.downsample(data[i])\n",
    "                if (self.normalizer is not None):\n",
    "                    data[i] = self.normalizer.transform(data[i])\n",
    "        else:\n",
    "            data = self.discretizer.transform(data, end=ts)[0]\n",
    "            if (self.normalizer is not None):\n",
    "                data = self.normalizer.transform(data)\n",
    "        #########  \n",
    "        \n",
    "        # data = self.discretizer.transform(data, end=ts)[0] \n",
    "        # if (self.normalizer is not None):\n",
    "        #     data = self.normalizer.transform(data)\n",
    "        # print(data.shape)\n",
    "\n",
    "        ys = ret[\"y\"]\n",
    "        names = ret[\"name\"]\n",
    "        ys = np.array(ys, dtype=np.int32) if len(ys) > 1 else np.array(ys, dtype=np.int32)[0]\n",
    "        stay_ids = ret['stay_id']\n",
    "        return data, ys\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "id": "a3071179",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datasets(discretizer, normalizer, args):\n",
    "    # if context == True:\n",
    "    #     transform = MultiTransform(views=11, normal_values=discretizer._id_normal_values, _is_categorical_channel=discretizer._is_categorical_channel, augmentation=augmentation, begin_pos=begin_pos)\n",
    "    # else:\n",
    "    #     transform = None\n",
    "    # changed definition of normal_values\n",
    "    # augmentation = 'gaussian'\n",
    "    # transform = MultiTransform(views=11, normal_values=discretizer._normal_values, _is_categorical_channel=discretizer._is_categorical_channel, augmentation=augmentation, begin_pos=discretizer._start_time)\n",
    "    transform = None\n",
    "    train_ds = EHRdataset(discretizer, normalizer, f'{args.ehr_data_root}/{args.task}/train_listfile.csv', os.path.join(args.ehr_data_root, f'{args.task}/train'), transforms=transform)\n",
    "    val_ds = EHRdataset(discretizer, normalizer, f'{args.ehr_data_root}/{args.task}/val_listfile.csv', os.path.join(args.ehr_data_root, f'{args.task}/train'), transforms = transform)\n",
    "    test_ds = EHRdataset(discretizer, normalizer, f'{args.ehr_data_root}/{args.task}/test_listfile.csv', os.path.join(args.ehr_data_root, f'{args.task}/test'), transforms = transform)\n",
    "    return train_ds, val_ds, test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "id": "fe6c9ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_collate(batch):\n",
    "    x = [item[0] for item in batch]\n",
    "    x, seq_length = pad_zeros(x)\n",
    "    targets = np.array([item[1] for item in batch])\n",
    "    return [x, targets, seq_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "id": "d4325676",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_zeros(arr, min_length=None):\n",
    "\n",
    "    dtype = arr[0].dtype\n",
    "    seq_length = [x.shape[0] for x in arr]\n",
    "    max_len = max(seq_length)\n",
    "    ret = [np.concatenate([x, np.zeros((max_len - x.shape[0],) + x.shape[1:], dtype=dtype)], axis=0)\n",
    "           for x in arr]\n",
    "    if (min_length is not None) and ret[0].shape[0] < min_length:\n",
    "        ret = [np.concatenate([x, np.zeros((min_length - x.shape[0],) + x.shape[1:], dtype=dtype)], axis=0)\n",
    "               for x in ret]\n",
    "    return np.array(ret), seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "id": "dc357009",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_loader(discretizer, normalizer, dataset_dir, batch_size):\n",
    "    train_ds, val_ds, test_ds = get_datasets(discretizer, normalizer, args)\n",
    "    train_dl = DataLoader(train_ds, batch_size, shuffle=True, collate_fn=my_collate, pin_memory=True, num_workers=16)\n",
    "    val_dl = DataLoader(val_ds, batch_size, shuffle=False, collate_fn=my_collate, pin_memory=True, num_workers=16)\n",
    "\n",
    "    return train_dl, val_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "id": "fd41e015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appling SimCLR image transforms...\n"
     ]
    }
   ],
   "source": [
    "ehrtr, ehrva, ehrts = get_datasets(discretizer,normalizer,args)\n",
    "cxrtr, cxrva, cxrts = get_cxr_datasets(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "id": "a1642294",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-1.66062755e+01, -6.02181801e-02,  9.75482416e-03, ...,\n",
       "         -5.58505849e-01,  6.88005369e+00, -2.91264127e-01],\n",
       "        [-1.66062755e+01, -6.02181801e-02,  4.87291875e-02, ...,\n",
       "         -5.58505849e-01, -1.45347657e-01, -2.91264127e-01],\n",
       "        [-1.66062755e+01, -6.02181801e-02,  2.27462786e-02, ...,\n",
       "         -5.58505849e-01, -1.45347657e-01, -2.91264127e-01],\n",
       "        ...,\n",
       "        [-1.66062755e+01, -6.02181801e-02, -2.67396204e-01, ...,\n",
       "         -5.58505849e-01, -1.45347657e-01, -2.91264127e-01],\n",
       "        [-1.66062755e+01, -6.02181801e-02, -2.67396204e-01, ...,\n",
       "         -5.58505849e-01, -1.45347657e-01, -2.91264127e-01],\n",
       "        [-1.66062755e+01, -6.02181801e-02, -2.67396204e-01, ...,\n",
       "         -5.58505849e-01, -1.45347657e-01, -2.91264127e-01]]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0], dtype=int32),\n",
       " 31205490.0)"
      ]
     },
     "execution_count": 566,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ehrtr.__getitem__(4,0,12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e33c1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1a06d01e",
   "metadata": {},
   "source": [
    "# Fusion Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "id": "ddf847b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MIMIC_CXR_EHR(Dataset):\n",
    "    def __init__(self, args, metadata_with_labels, ehr_ds, cxr_ds, split='train'):\n",
    "        \n",
    "        # select classes\n",
    "        self.CLASSES = CLASSES\n",
    "        if 'radiology' in args.labels_set:\n",
    "            self.CLASSES = R_CLASSES\n",
    "        \n",
    "        self.metadata_with_labels = metadata_with_labels\n",
    "        \n",
    "        self.cxr_files_paired = self.metadata_with_labels.dicom_id.values\n",
    "        self.ehr_files_paired = (self.metadata_with_labels['stay'].values)\n",
    "        self.time_diff = self.metadata_with_labels.time_diff\n",
    "        self.lower = self.metadata_with_labels.lower\n",
    "        self.upper = self.metadata_with_labels.upper\n",
    "        \n",
    "        self.cxr_files_all = cxr_ds.filenames_loaded\n",
    "        self.ehr_files_all = ehr_ds.names\n",
    "        \n",
    "        self.ehr_files_unpaired = list(set(self.ehr_files_all) - set(self.ehr_files_paired))\n",
    "        \n",
    "        self.ehr_ds = ehr_ds\n",
    "        self.cxr_ds = cxr_ds\n",
    "        \n",
    "        self.args = args\n",
    "        self.split = split\n",
    "        self.data_ratio = self.args.data_ratio if split=='train' else 1.0\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.args.data_pairs == 'paired':\n",
    "            cxr_data, labels_cxr = self.cxr_ds[self.cxr_files_paired[index]]\n",
    "            \n",
    "            lower = self.metadata_with_labels.iloc[index].lower\n",
    "            upper = self.metadata_with_labels.iloc[index].upper\n",
    "            \n",
    "            ehr_data, labels_ehr = self.ehr_ds.__getitem__(self.ehr_files_paired[index],lower,upper)\n",
    "            time_diff = self.metadata_with_labels.iloc[index].time_diff\n",
    "            \n",
    "            #dicom_id =  self.cxr_files_paired[index]\n",
    "            #stay_id = self.ehr_files_paired[index]\n",
    "            #time_diff = \n",
    "                        \n",
    "            if self.args.beta_infonce:\n",
    "                return ehr_data, cxr_data, labels_ehr, labels_cxr, time_diff\n",
    "            else:\n",
    "                return ehr_data, cxr_data, labels_ehr, labels_cxr\n",
    "        \n",
    "        elif self.args.data_pairs == 'radiology':\n",
    "            ehr_data, labels_ehr = np.zeros((1, 10)), np.zeros(self.args.num_classes)\n",
    "            cxr_data, labels_cxr = self.cxr_ds[self.cxr_files_all[index]]\n",
    "            return ehr_data, cxr_data, labels_ehr, labels_cxr\n",
    "        \n",
    "        elif self.args.data_pairs == 'ehr_only':\n",
    "            ehr_data, labels_ehr = self.ehr_ds[self.ehr_files_all[index]]\n",
    "            cxr_data, labels_cxr = None, None\n",
    "            return ehr_data, cxr_data, labels_ehr, labels_cxr\n",
    "        \n",
    "        elif self.args.data_pairs == 'joint_ehr':\n",
    "            if index < len(self.ehr_files_paired):\n",
    "                ehr_data, labels_ehr = self.ehr_ds[self.ehr_files_paired[index]]\n",
    "                cxr_data, labels_cxr = self.cxr_ds[self.cxr_files_paired[index]]\n",
    "            else:\n",
    "                index = random.randint(0, len(self.ehr_files_unpaired)-1) \n",
    "                ehr_data, labels_ehr = self.ehr_ds[self.ehr_files_unpaired[index]]\n",
    "                cxr_data, labels_cxr = None, None\n",
    "            return ehr_data, cxr_data, labels_ehr, labels_cxr\n",
    "\n",
    "       \n",
    "    \n",
    "    def __len__(self):\n",
    "        if self.args.data_pairs == 'paired':\n",
    "            return len(self.ehr_files_paired)\n",
    "        elif self.args.data_pairs == 'ehr_only':\n",
    "            return len(self.ehr_files_all)\n",
    "        elif self.args.data_pairs == 'radiology':\n",
    "            return len(self.cxr_files_all)\n",
    "        elif self.args.data_pairs == 'joint_ehr':\n",
    "            return len(self.ehr_files_paired) + int(self.data_ratio * len(self.ehr_files_unpaired)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "id": "51e73245",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43, 76)"
      ]
     },
     "execution_count": 497,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MIMIC_CXR_EHR(args,train_meta_with_labels,ehrtr,cxrtr).__getitem__(3)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "id": "569b348c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadmetadata(args):\n",
    "\n",
    "    def time_offsets(table):\n",
    "        ids = table.stay_id.unique()\n",
    "        data = []\n",
    "        for id in ids:\n",
    "            temp = table[table.stay_id == id]\n",
    "            offsets = list(range(0,int(temp.LOS.max())+12,12))\n",
    "            times = []\n",
    "            for i,time in enumerate(temp.intime):\n",
    "                times.append(time+ timedelta(hours=offsets[i])) \n",
    "            temp.intime = times\n",
    "            data.append(temp)\n",
    "        data = pd.concat(data,ignore_index=True)\n",
    "        data.time_diff = (data.StudyDateTime - data.intime).apply(lambda x: np.round(x.total_seconds()/60/60,3))\n",
    "        data['lower'] = data.LOS + (data.intime - data.outtime).apply(lambda x: np.round(x.total_seconds()/60/60,3))\n",
    "        data['upper'] = data.apply(lambda x: x.lower + 12 if (x.lower + 12) < x.LOS else (x.LOS+1),axis=1)\n",
    "        return data\n",
    "    \n",
    "    cxr_metadata = pd.read_csv(f'{args.cxr_data_root}/mimic-cxr-2.0.0-metadata.csv')\n",
    "    print('Number of CXR images=', len(cxr_metadata))\n",
    "    icu_stay_metadata = pd.read_csv(f'{args.ehr_data_root}/root/all_stays.csv')\n",
    "    print('Number of ICU stays=', len(icu_stay_metadata))\n",
    "    columns = ['subject_id', 'stay_id', 'intime', 'outtime']\n",
    "    \n",
    "    # only common subjects with both icu stay and an xray\n",
    "    # Note that inner merge includes rows if a chest X-ray is associated with multiple stays\n",
    "    cxr_merged_icustays = cxr_metadata.merge(icu_stay_metadata[columns], how='inner', on='subject_id')\n",
    "    print('Number of CXR associated with ICU stay based on subject ID=', len(cxr_merged_icustays))\n",
    "    print('Number of unique CXR dicoms=', len(cxr_merged_icustays.dicom_id.unique()))\n",
    "    print('Number of unique CXR study id=', len(cxr_merged_icustays.study_id.unique()))\n",
    "        \n",
    "    # combine study date time\n",
    "    cxr_merged_icustays['StudyTime'] = cxr_merged_icustays['StudyTime'].apply(lambda x: f'{int(float(x)):06}' )\n",
    "    cxr_merged_icustays['StudyDateTime'] = pd.to_datetime(cxr_merged_icustays['StudyDate'].astype(str) + ' ' + cxr_merged_icustays['StudyTime'].astype(str) ,format=\"%Y%m%d %H%M%S\")\n",
    "\n",
    "    cxr_merged_icustays.intime=pd.to_datetime(cxr_merged_icustays.intime)\n",
    "    cxr_merged_icustays.outtime=pd.to_datetime(cxr_merged_icustays.outtime)\n",
    "    \n",
    "    cxr_merged_icustays['time_diff'] = cxr_merged_icustays.StudyDateTime-cxr_merged_icustays.intime\n",
    "    cxr_merged_icustays['time_diff'] = cxr_merged_icustays['time_diff'].apply(lambda x: np.round(x.total_seconds()/60/60,3))\n",
    "    \n",
    "    cxr_merged_icustays['LOS'] = cxr_merged_icustays.outtime-cxr_merged_icustays.intime\n",
    "    cxr_merged_icustays['LOS'] = cxr_merged_icustays['LOS'].apply(lambda x: np.round(x.total_seconds()/60/60,3))\n",
    "    \n",
    "    \n",
    "    # For LE/ FT  (evaluation datasets)\n",
    "    if (args.dataset !='all'):\n",
    "\n",
    "        if args.task == 'decompensation' or args.task == 'length-of-stay':\n",
    "            train_listfile = pd.read_csv(f'/scratch/se1525/mml-ssl/{args.task}/train_listfile.csv')\n",
    "            train_listfile.columns = ['stay' , 'period_length' , 'stay_id' ,'y_true', 'intime' , 'endtime']\n",
    "            test_listfile = pd.read_csv(f'/scratch/se1525/mml-ssl/{args.task}/test_listfile.csv')\n",
    "            test_listfile.columns = ['stay' , 'period_length' , 'stay_id' ,'y_true', 'intime' , 'endtime']\n",
    "            listfile = train_listfile.append(test_listfile)\n",
    "            listfile['subject_id'] = listfile['stay'].apply(lambda x: x.split(\"_\")[0])\n",
    "            print(listfile.head)\n",
    "\n",
    "            columns2 = ['subject_id', 'endtime']\n",
    "            listfile['subject_id'] = listfile['subject_id'].astype('int64')\n",
    "            cxr_merged_icustays = cxr_merged_icustays.merge(listfile[columns2], how='inner', on='subject_id')\n",
    "            cxr_merged_icustays.endtime=pd.to_datetime(cxr_merged_icustays.endtime)\n",
    "            cxr_merged_icustays_during = cxr_merged_icustays.loc[((cxr_merged_icustays.StudyDateTime>=cxr_merged_icustays.intime)&(cxr_merged_icustays.StudyDateTime<=cxr_merged_icustays.endtime))]\n",
    "\n",
    "        if args.task == 'in-hospital-mortality':\n",
    "            end_time = cxr_merged_icustays.intime + pd.DateOffset(hours=48)\n",
    "            cxr_merged_icustays_during = cxr_merged_icustays.loc[(cxr_merged_icustays.StudyDateTime>=cxr_merged_icustays.intime)&((cxr_merged_icustays.StudyDateTime<=end_time))]\n",
    "\n",
    "        if args.task == 'phenotyping':\n",
    "            end_time = cxr_merged_icustays.outtime\n",
    "            cxr_merged_icustays_during = cxr_merged_icustays.loc[(cxr_merged_icustays.StudyDateTime>=cxr_merged_icustays.intime)&\n",
    "                                                                 ((cxr_merged_icustays.StudyDateTime<=end_time))]\n",
    "        \n",
    "        # select cxrs with the ViewPosition == 'AP\n",
    "        cxr_merged_icustays_AP = cxr_merged_icustays_during[cxr_merged_icustays_during['ViewPosition'] == 'AP']\n",
    "        \n",
    "        if args.retrive_cxr == 'recent':\n",
    "            groups = cxr_merged_icustays_AP.groupby('stay_id')\n",
    "            groups_selected = []\n",
    "            for group in groups:\n",
    "                # select the latest cxr for the icu stay\n",
    "                selected = group[1].sort_values('StudyDateTime').tail(1).reset_index()\n",
    "                groups_selected.append(selected)\n",
    "            groups = pd.concat(groups_selected, ignore_index=True)\n",
    "            groups = groups.groupby('study_id').first()\n",
    "            groups = groups.reset_index()\n",
    "            groups = groups.groupby('study_id').first().sort_values(by=['stay_id','StudyDateTime'])\n",
    "            groups = groups.reset_index()\n",
    "            #groups['num_cxr_windows'] = groups.groupby(['stay_id'])['stay_id'].transform('count')\n",
    "            #groups['cxr_window_length'] = groups['LOS']/groups['num_cxr_windows']\n",
    "            groups['num_ehr_windows'] = np.ceil(groups['LOS']/12).astype(int)\n",
    "            groups = groups.loc[groups.index.repeat(groups.num_ehr_windows)].reset_index(drop=True)\n",
    "            groups = time_offsets(groups)\n",
    "        else: \n",
    "            groups = cxr_merged_icustays_AP.groupby('study_id').first()\n",
    "            groups = groups.reset_index()\n",
    "            groups = groups.groupby('study_id').first().sort_values(by=['stay_id','StudyDateTime'])\n",
    "            groups = groups.reset_index()\n",
    "            #groups['num_cxr_windows'] = groups.groupby(['stay_id'])['stay_id'].transform('count')\n",
    "            #groups['cxr_window_length'] = groups['LOS']/groups['num_cxr_windows']\n",
    "            groups['num_ehr_windows'] = np.ceil(groups['LOS']/12).astype(int)\n",
    "            \n",
    "            \n",
    "    # For SIMCLR pretraining (large dataset)\n",
    "#     else:\n",
    "#         # print(cxr_merged_icustays.ViewPosition.unique())\n",
    "#         cxr_merged_icustays_AP = cxr_merged_icustays[cxr_merged_icustays['ViewPosition'] == 'AP']\n",
    "#         print(\"Number of CXR associated with ICU stay and in AP view=\", len(cxr_merged_icustays_AP))\n",
    "#         groups = cxr_merged_icustays_AP\n",
    "        \n",
    "    print(\"Mean time cxr - intime= \", groups.time_diff.mean())\n",
    "    print(\"Minimum time =\", groups.time_diff.min())\n",
    "    print(\"Maximum time =\", groups.time_diff.max())\n",
    "\n",
    "#     plt.hist(groups.time_diff.apply(lambda x: x.days).astype(\"float64\"))\n",
    "#     plt.xlabel('Time difference in days')\n",
    "#     plt.show()\n",
    "\n",
    "    #print(groups.iloc[0])\n",
    "    return groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "id": "b111998b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CXR images= 377110\n",
      "Number of ICU stays= 59372\n",
      "Number of CXR associated with ICU stay based on subject ID= 368350\n",
      "Number of unique CXR dicoms= 181195\n",
      "Number of unique CXR study id= 122087\n",
      "Mean time cxr - intime=  74.23168108953072\n",
      "Minimum time = -601.239\n",
      "Maximum time = 2368.942\n"
     ]
    }
   ],
   "source": [
    "meta = loadmetadata(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "id": "3a55a2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cxr_ehr(args, ehr_train_ds, ehr_val_ds, ehr_test_ds, cxr_train_ds, cxr_val_ds, cxr_test_ds):\n",
    "    \n",
    "    # Load cxr and ehr groups\n",
    "    cxr_merged_icustays = loadmetadata(args) \n",
    "    \n",
    "    # Add the labels \n",
    "    splits_labels_train = pd.read_csv(f'{args.ehr_data_root}/{args.task}/train_listfile.csv')\n",
    "    splits_labels_val = pd.read_csv(f'{args.ehr_data_root}/{args.task}/val_listfile.csv')\n",
    "    splits_labels_test = pd.read_csv(f'{args.ehr_data_root}/{args.task}/test_listfile.csv')\n",
    "    \n",
    "    #TODO: investigate why total size of cxr_merged_icustays drops after the three steps below\n",
    "    train_meta_with_labels = cxr_merged_icustays.merge(splits_labels_train, how='inner', on='stay_id')#change dataset size here\n",
    "    val_meta_with_labels = cxr_merged_icustays.merge(splits_labels_val, how='inner', on='stay_id')\n",
    "    test_meta_with_labels = cxr_merged_icustays.merge(splits_labels_test, how='inner', on='stay_id')\n",
    "    \n",
    "    # Get rid of chest X-rays that don't have radiology reports\n",
    "    metadata = pd.read_csv(f'{args.cxr_data_root}/mimic-cxr-2.0.0-metadata.csv')\n",
    "    labels = pd.read_csv(f'{args.cxr_data_root}/mimic-cxr-2.0.0-chexpert.csv')\n",
    "    metadata_with_labels = metadata.merge(labels[['study_id']], how='inner', on='study_id').drop_duplicates(subset=['dicom_id'])\n",
    "    train_meta_with_labels = train_meta_with_labels.merge(metadata_with_labels[['dicom_id']], how='inner', on='dicom_id')\n",
    "    val_meta_with_labels = val_meta_with_labels.merge(metadata_with_labels[['dicom_id']], how='inner', on='dicom_id')\n",
    "    test_meta_with_labels = test_meta_with_labels.merge(metadata_with_labels[['dicom_id']], how='inner', on='dicom_id')\n",
    "    \n",
    "    print(\"Excluding CXR with missing radiology reports = \",len(train_meta_with_labels))\n",
    "\n",
    "    # Multimodal class\n",
    "    train_ds = MIMIC_CXR_EHR(args, train_meta_with_labels, ehr_train_ds, cxr_train_ds)\n",
    "    print(len(train_ds))\n",
    "    val_ds = MIMIC_CXR_EHR(args, val_meta_with_labels, ehr_val_ds, cxr_val_ds, split='val')\n",
    "    print(len(val_ds))\n",
    "    test_ds = MIMIC_CXR_EHR(args, test_meta_with_labels, ehr_test_ds, cxr_test_ds, split='test')\n",
    "    print(len(test_ds))\n",
    "    \n",
    "    if args.beta_infonce:\n",
    "        collate = my_collate_beta\n",
    "    else:\n",
    "        collate = my_collate\n",
    "    \n",
    "    # Multimodal data loader \n",
    "    train_dl = DataLoader(train_ds, args.batch_size, shuffle=True, collate_fn=collate, drop_last=True)#, pin_memory=True, num_workers=24)\n",
    "    val_dl = DataLoader(val_ds, args.batch_size, shuffle=False, collate_fn=collate, drop_last=False) #pin_memory=True, num_workers=16,\n",
    "    test_dl = DataLoader(test_ds, args.batch_size, shuffle=False, collate_fn=collate, drop_last=False) # pin_memory=True,num_workers=16,\n",
    "\n",
    "    return train_dl, val_dl, test_dl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "id": "be15603b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_collate(batch):\n",
    "    x = [item[0] for item in batch]\n",
    "    pairs = [False if item[1] is None else True for item in batch]\n",
    "    img = torch.stack([torch.zeros(3, 224, 224) if item[1] is None else item[1] for item in batch])\n",
    "    x, seq_length = pad_zeros(x)\n",
    "    targets_ehr = np.array([item[2] for item in batch])\n",
    "    targets_cxr = torch.stack([torch.zeros(14) if item[3] is None else item[3] for item in batch])\n",
    "    return [x, img, targets_ehr, targets_cxr, seq_length, pairs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "id": "e857140c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_collate_beta(batch, beta_infonce=False):\n",
    "    x = [item[0] for item in batch]\n",
    "    pairs = [False if item[1] is None else True for item in batch]\n",
    "    img = torch.stack([torch.zeros(3, 224, 224) if item[1] is None else item[1] for item in batch])\n",
    "    x, seq_length = pad_zeros(x)\n",
    "    targets_ehr = np.array([item[2] for item in batch])\n",
    "    targets_cxr = torch.stack([torch.zeros(14) if item[3] is None else item[3] for item in batch])\n",
    "    time_diff = [item[4] for item in batch]\n",
    "    \n",
    "    return [x, img, targets_ehr, targets_cxr, seq_length, pairs, time_diff]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "id": "033d74c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_zeros(arr, min_length=None):\n",
    "    dtype = arr[0].dtype\n",
    "    seq_length = [x.shape[0] for x in arr]\n",
    "    max_len = max(seq_length)\n",
    "    ret = [np.concatenate([x, np.zeros((max_len - x.shape[0],) + x.shape[1:], dtype=dtype)], axis=0)\n",
    "           for x in arr]\n",
    "    if (min_length is not None) and ret[0].shape[0] < min_length:\n",
    "        ret = [np.concatenate([x, np.zeros((min_length - x.shape[0],) + x.shape[1:], dtype=dtype)], axis=0)\n",
    "               for x in ret]\n",
    "    return np.array(ret), seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "id": "e8007e63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stay</th>\n",
       "      <th>period_length</th>\n",
       "      <th>stay_id</th>\n",
       "      <th>Acute and unspecified renal failure</th>\n",
       "      <th>Acute cerebrovascular disease</th>\n",
       "      <th>Acute myocardial infarction</th>\n",
       "      <th>Cardiac dysrhythmias</th>\n",
       "      <th>Chronic kidney disease</th>\n",
       "      <th>Chronic obstructive pulmonary disease and bronchiectasis</th>\n",
       "      <th>Complications of surgical procedures or medical care</th>\n",
       "      <th>Conduction disorders</th>\n",
       "      <th>Congestive heart failure; nonhypertensive</th>\n",
       "      <th>Coronary atherosclerosis and other heart disease</th>\n",
       "      <th>Diabetes mellitus with complications</th>\n",
       "      <th>Diabetes mellitus without complication</th>\n",
       "      <th>Disorders of lipid metabolism</th>\n",
       "      <th>Essential hypertension</th>\n",
       "      <th>Fluid and electrolyte disorders</th>\n",
       "      <th>Gastrointestinal hemorrhage</th>\n",
       "      <th>Hypertension with complications and secondary hypertension</th>\n",
       "      <th>Other liver diseases</th>\n",
       "      <th>Other lower respiratory disease</th>\n",
       "      <th>Other upper respiratory disease</th>\n",
       "      <th>Pleurisy; pneumothorax; pulmonary collapse</th>\n",
       "      <th>Pneumonia (except that caused by tuberculosis or sexually transmitted disease)</th>\n",
       "      <th>Respiratory failure; insufficiency; arrest (adult)</th>\n",
       "      <th>Septicemia (except in labor)</th>\n",
       "      <th>Shock</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000032_episode1_timeseries.csv</td>\n",
       "      <td>9.846389</td>\n",
       "      <td>39553978</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               stay  period_length   stay_id  \\\n",
       "0  10000032_episode1_timeseries.csv       9.846389  39553978   \n",
       "\n",
       "   Acute and unspecified renal failure  Acute cerebrovascular disease  \\\n",
       "0                                    0                              0   \n",
       "\n",
       "   Acute myocardial infarction  Cardiac dysrhythmias  Chronic kidney disease  \\\n",
       "0                            0                     0                       0   \n",
       "\n",
       "   Chronic obstructive pulmonary disease and bronchiectasis  \\\n",
       "0                                                  1          \n",
       "\n",
       "   Complications of surgical procedures or medical care  Conduction disorders  \\\n",
       "0                                                  1                        0   \n",
       "\n",
       "   Congestive heart failure; nonhypertensive  \\\n",
       "0                                          0   \n",
       "\n",
       "   Coronary atherosclerosis and other heart disease  \\\n",
       "0                                                 0   \n",
       "\n",
       "   Diabetes mellitus with complications  \\\n",
       "0                                     0   \n",
       "\n",
       "   Diabetes mellitus without complication  Disorders of lipid metabolism  \\\n",
       "0                                       0                              0   \n",
       "\n",
       "   Essential hypertension  Fluid and electrolyte disorders  \\\n",
       "0                       0                                1   \n",
       "\n",
       "   Gastrointestinal hemorrhage  \\\n",
       "0                            0   \n",
       "\n",
       "   Hypertension with complications and secondary hypertension  \\\n",
       "0                                                  0            \n",
       "\n",
       "   Other liver diseases  Other lower respiratory disease  \\\n",
       "0                     1                                0   \n",
       "\n",
       "   Other upper respiratory disease  \\\n",
       "0                                0   \n",
       "\n",
       "   Pleurisy; pneumothorax; pulmonary collapse  \\\n",
       "0                                           0   \n",
       "\n",
       "   Pneumonia (except that caused by tuberculosis or sexually transmitted disease)  \\\n",
       "0                                                  0                                \n",
       "\n",
       "   Respiratory failure; insufficiency; arrest (adult)  \\\n",
       "0                                                  1    \n",
       "\n",
       "   Septicemia (except in labor)  Shock  \n",
       "0                             0      0  "
      ]
     },
     "execution_count": 504,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits_labels_train = pd.read_csv(f'{args.ehr_data_root}/{args.task}/train_listfile.csv')\n",
    "splits_labels_train.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "id": "b8123d4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>study_id</th>\n",
       "      <th>index</th>\n",
       "      <th>dicom_id</th>\n",
       "      <th>subject_id</th>\n",
       "      <th>PerformedProcedureStepDescription</th>\n",
       "      <th>ViewPosition</th>\n",
       "      <th>Rows</th>\n",
       "      <th>Columns</th>\n",
       "      <th>StudyDate</th>\n",
       "      <th>StudyTime</th>\n",
       "      <th>ProcedureCodeSequence_CodeMeaning</th>\n",
       "      <th>ViewCodeSequence_CodeMeaning</th>\n",
       "      <th>PatientOrientationCodeSequence_CodeMeaning</th>\n",
       "      <th>stay_id</th>\n",
       "      <th>intime</th>\n",
       "      <th>outtime</th>\n",
       "      <th>StudyDateTime</th>\n",
       "      <th>time_diff</th>\n",
       "      <th>LOS</th>\n",
       "      <th>num_ehr_windows</th>\n",
       "      <th>lower</th>\n",
       "      <th>upper</th>\n",
       "      <th>stay</th>\n",
       "      <th>period_length</th>\n",
       "      <th>Acute and unspecified renal failure</th>\n",
       "      <th>Acute cerebrovascular disease</th>\n",
       "      <th>Acute myocardial infarction</th>\n",
       "      <th>Cardiac dysrhythmias</th>\n",
       "      <th>Chronic kidney disease</th>\n",
       "      <th>Chronic obstructive pulmonary disease and bronchiectasis</th>\n",
       "      <th>Complications of surgical procedures or medical care</th>\n",
       "      <th>Conduction disorders</th>\n",
       "      <th>Congestive heart failure; nonhypertensive</th>\n",
       "      <th>Coronary atherosclerosis and other heart disease</th>\n",
       "      <th>Diabetes mellitus with complications</th>\n",
       "      <th>Diabetes mellitus without complication</th>\n",
       "      <th>Disorders of lipid metabolism</th>\n",
       "      <th>Essential hypertension</th>\n",
       "      <th>Fluid and electrolyte disorders</th>\n",
       "      <th>Gastrointestinal hemorrhage</th>\n",
       "      <th>Hypertension with complications and secondary hypertension</th>\n",
       "      <th>Other liver diseases</th>\n",
       "      <th>Other lower respiratory disease</th>\n",
       "      <th>Other upper respiratory disease</th>\n",
       "      <th>Pleurisy; pneumothorax; pulmonary collapse</th>\n",
       "      <th>Pneumonia (except that caused by tuberculosis or sexually transmitted disease)</th>\n",
       "      <th>Respiratory failure; insufficiency; arrest (adult)</th>\n",
       "      <th>Septicemia (except in labor)</th>\n",
       "      <th>Shock</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>59469162</td>\n",
       "      <td>291809</td>\n",
       "      <td>5dfd960b-2e6378a2-1de9c84f-24ec4b38-f9d1a19a</td>\n",
       "      <td>17938576</td>\n",
       "      <td>CHEST (PORTABLE AP)</td>\n",
       "      <td>AP</td>\n",
       "      <td>2539</td>\n",
       "      <td>3050</td>\n",
       "      <td>21580123</td>\n",
       "      <td>213357</td>\n",
       "      <td>CHEST (PORTABLE AP)</td>\n",
       "      <td>antero-posterior</td>\n",
       "      <td>Erect</td>\n",
       "      <td>30002498</td>\n",
       "      <td>2158-01-23 16:00:00</td>\n",
       "      <td>2158-01-24 17:36:04</td>\n",
       "      <td>2158-01-23 21:33:57</td>\n",
       "      <td>5.566</td>\n",
       "      <td>25.601</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>17938576_episode1_timeseries.csv</td>\n",
       "      <td>25.601111</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   study_id   index                                      dicom_id  subject_id  \\\n",
       "0  59469162  291809  5dfd960b-2e6378a2-1de9c84f-24ec4b38-f9d1a19a    17938576   \n",
       "\n",
       "  PerformedProcedureStepDescription ViewPosition  Rows  Columns  StudyDate  \\\n",
       "0               CHEST (PORTABLE AP)           AP  2539     3050   21580123   \n",
       "\n",
       "  StudyTime ProcedureCodeSequence_CodeMeaning ViewCodeSequence_CodeMeaning  \\\n",
       "0    213357               CHEST (PORTABLE AP)             antero-posterior   \n",
       "\n",
       "  PatientOrientationCodeSequence_CodeMeaning   stay_id              intime  \\\n",
       "0                                      Erect  30002498 2158-01-23 16:00:00   \n",
       "\n",
       "              outtime       StudyDateTime  time_diff     LOS  num_ehr_windows  \\\n",
       "0 2158-01-24 17:36:04 2158-01-23 21:33:57      5.566  25.601                3   \n",
       "\n",
       "   lower  upper                              stay  period_length  \\\n",
       "0    0.0   12.0  17938576_episode1_timeseries.csv      25.601111   \n",
       "\n",
       "   Acute and unspecified renal failure  Acute cerebrovascular disease  \\\n",
       "0                                    0                              1   \n",
       "\n",
       "   Acute myocardial infarction  Cardiac dysrhythmias  Chronic kidney disease  \\\n",
       "0                            0                     0                       0   \n",
       "\n",
       "   Chronic obstructive pulmonary disease and bronchiectasis  \\\n",
       "0                                                  1          \n",
       "\n",
       "   Complications of surgical procedures or medical care  Conduction disorders  \\\n",
       "0                                                  0                        0   \n",
       "\n",
       "   Congestive heart failure; nonhypertensive  \\\n",
       "0                                          0   \n",
       "\n",
       "   Coronary atherosclerosis and other heart disease  \\\n",
       "0                                                 1   \n",
       "\n",
       "   Diabetes mellitus with complications  \\\n",
       "0                                     0   \n",
       "\n",
       "   Diabetes mellitus without complication  Disorders of lipid metabolism  \\\n",
       "0                                       0                              0   \n",
       "\n",
       "   Essential hypertension  Fluid and electrolyte disorders  \\\n",
       "0                       0                                0   \n",
       "\n",
       "   Gastrointestinal hemorrhage  \\\n",
       "0                            0   \n",
       "\n",
       "   Hypertension with complications and secondary hypertension  \\\n",
       "0                                                  0            \n",
       "\n",
       "   Other liver diseases  Other lower respiratory disease  \\\n",
       "0                     0                                0   \n",
       "\n",
       "   Other upper respiratory disease  \\\n",
       "0                                0   \n",
       "\n",
       "   Pleurisy; pneumothorax; pulmonary collapse  \\\n",
       "0                                           0   \n",
       "\n",
       "   Pneumonia (except that caused by tuberculosis or sexually transmitted disease)  \\\n",
       "0                                                  1                                \n",
       "\n",
       "   Respiratory failure; insufficiency; arrest (adult)  \\\n",
       "0                                                  0    \n",
       "\n",
       "   Septicemia (except in labor)  Shock  \n",
       "0                             1      1  "
      ]
     },
     "execution_count": 505,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_meta_with_labels = meta.merge(splits_labels_train, how='inner', on='stay_id')\n",
    "train_meta_with_labels.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "id": "30e4c95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.read_csv(f'{args.cxr_data_root}/mimic-cxr-2.0.0-metadata.csv')\n",
    "labels = pd.read_csv(f'{args.cxr_data_root}/mimic-cxr-2.0.0-chexpert.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "id": "a172a624",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dicom_id</th>\n",
       "      <th>subject_id</th>\n",
       "      <th>study_id</th>\n",
       "      <th>PerformedProcedureStepDescription</th>\n",
       "      <th>ViewPosition</th>\n",
       "      <th>Rows</th>\n",
       "      <th>Columns</th>\n",
       "      <th>StudyDate</th>\n",
       "      <th>StudyTime</th>\n",
       "      <th>ProcedureCodeSequence_CodeMeaning</th>\n",
       "      <th>ViewCodeSequence_CodeMeaning</th>\n",
       "      <th>PatientOrientationCodeSequence_CodeMeaning</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>02aa804e-bde0afdd-112c0b34-7bc16630-4e384014</td>\n",
       "      <td>10000032</td>\n",
       "      <td>50414267</td>\n",
       "      <td>CHEST (PA AND LAT)</td>\n",
       "      <td>PA</td>\n",
       "      <td>3056</td>\n",
       "      <td>2544</td>\n",
       "      <td>21800506</td>\n",
       "      <td>213014.531</td>\n",
       "      <td>CHEST (PA AND LAT)</td>\n",
       "      <td>postero-anterior</td>\n",
       "      <td>Erect</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       dicom_id  subject_id  study_id  \\\n",
       "0  02aa804e-bde0afdd-112c0b34-7bc16630-4e384014    10000032  50414267   \n",
       "\n",
       "  PerformedProcedureStepDescription ViewPosition  Rows  Columns  StudyDate  \\\n",
       "0                CHEST (PA AND LAT)           PA  3056     2544   21800506   \n",
       "\n",
       "    StudyTime ProcedureCodeSequence_CodeMeaning ViewCodeSequence_CodeMeaning  \\\n",
       "0  213014.531                CHEST (PA AND LAT)             postero-anterior   \n",
       "\n",
       "  PatientOrientationCodeSequence_CodeMeaning  \n",
       "0                                      Erect  "
      ]
     },
     "execution_count": 507,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "id": "9edd43d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>study_id</th>\n",
       "      <th>Atelectasis</th>\n",
       "      <th>Cardiomegaly</th>\n",
       "      <th>Consolidation</th>\n",
       "      <th>Edema</th>\n",
       "      <th>Enlarged Cardiomediastinum</th>\n",
       "      <th>Fracture</th>\n",
       "      <th>Lung Lesion</th>\n",
       "      <th>Lung Opacity</th>\n",
       "      <th>No Finding</th>\n",
       "      <th>Pleural Effusion</th>\n",
       "      <th>Pleural Other</th>\n",
       "      <th>Pneumonia</th>\n",
       "      <th>Pneumothorax</th>\n",
       "      <th>Support Devices</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000032</td>\n",
       "      <td>50414267</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subject_id  study_id  Atelectasis  Cardiomegaly  Consolidation  Edema  \\\n",
       "0    10000032  50414267          NaN           NaN            NaN    NaN   \n",
       "\n",
       "   Enlarged Cardiomediastinum  Fracture  Lung Lesion  Lung Opacity  \\\n",
       "0                         NaN       NaN          NaN           NaN   \n",
       "\n",
       "   No Finding  Pleural Effusion  Pleural Other  Pneumonia  Pneumothorax  \\\n",
       "0         1.0               NaN            NaN        NaN           NaN   \n",
       "\n",
       "   Support Devices  \n",
       "0              NaN  "
      ]
     },
     "execution_count": 508,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "id": "3505dadf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dicom_id</th>\n",
       "      <th>subject_id</th>\n",
       "      <th>study_id</th>\n",
       "      <th>PerformedProcedureStepDescription</th>\n",
       "      <th>ViewPosition</th>\n",
       "      <th>Rows</th>\n",
       "      <th>Columns</th>\n",
       "      <th>StudyDate</th>\n",
       "      <th>StudyTime</th>\n",
       "      <th>ProcedureCodeSequence_CodeMeaning</th>\n",
       "      <th>ViewCodeSequence_CodeMeaning</th>\n",
       "      <th>PatientOrientationCodeSequence_CodeMeaning</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>02aa804e-bde0afdd-112c0b34-7bc16630-4e384014</td>\n",
       "      <td>10000032</td>\n",
       "      <td>50414267</td>\n",
       "      <td>CHEST (PA AND LAT)</td>\n",
       "      <td>PA</td>\n",
       "      <td>3056</td>\n",
       "      <td>2544</td>\n",
       "      <td>21800506</td>\n",
       "      <td>213014.531</td>\n",
       "      <td>CHEST (PA AND LAT)</td>\n",
       "      <td>postero-anterior</td>\n",
       "      <td>Erect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>174413ec-4ec4c1f7-34ea26b7-c5f994f8-79ef1962</td>\n",
       "      <td>10000032</td>\n",
       "      <td>50414267</td>\n",
       "      <td>CHEST (PA AND LAT)</td>\n",
       "      <td>LATERAL</td>\n",
       "      <td>3056</td>\n",
       "      <td>2544</td>\n",
       "      <td>21800506</td>\n",
       "      <td>213014.531</td>\n",
       "      <td>CHEST (PA AND LAT)</td>\n",
       "      <td>lateral</td>\n",
       "      <td>Erect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2a2277a9-b0ded155-c0de8eb9-c124d10e-82c5caab</td>\n",
       "      <td>10000032</td>\n",
       "      <td>53189527</td>\n",
       "      <td>CHEST (PA AND LAT)</td>\n",
       "      <td>PA</td>\n",
       "      <td>3056</td>\n",
       "      <td>2544</td>\n",
       "      <td>21800626</td>\n",
       "      <td>165500.312</td>\n",
       "      <td>CHEST (PA AND LAT)</td>\n",
       "      <td>postero-anterior</td>\n",
       "      <td>Erect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>e084de3b-be89b11e-20fe3f9f-9c8d8dfe-4cfd202c</td>\n",
       "      <td>10000032</td>\n",
       "      <td>53189527</td>\n",
       "      <td>CHEST (PA AND LAT)</td>\n",
       "      <td>LATERAL</td>\n",
       "      <td>3056</td>\n",
       "      <td>2544</td>\n",
       "      <td>21800626</td>\n",
       "      <td>165500.312</td>\n",
       "      <td>CHEST (PA AND LAT)</td>\n",
       "      <td>lateral</td>\n",
       "      <td>Erect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>68b5c4b1-227d0485-9cc38c3f-7b84ab51-4b472714</td>\n",
       "      <td>10000032</td>\n",
       "      <td>53911762</td>\n",
       "      <td>CHEST (PORTABLE AP)</td>\n",
       "      <td>AP</td>\n",
       "      <td>2705</td>\n",
       "      <td>2539</td>\n",
       "      <td>21800723</td>\n",
       "      <td>80556.875</td>\n",
       "      <td>CHEST (PORTABLE AP)</td>\n",
       "      <td>antero-posterior</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377090</th>\n",
       "      <td>428e2c18-5721d8f3-35a05001-36f3d080-9053b83c</td>\n",
       "      <td>19999733</td>\n",
       "      <td>57132437</td>\n",
       "      <td>CHEST (PA AND LAT)</td>\n",
       "      <td>PA</td>\n",
       "      <td>3056</td>\n",
       "      <td>2544</td>\n",
       "      <td>21520708</td>\n",
       "      <td>224550.171</td>\n",
       "      <td>CHEST (PA AND LAT)</td>\n",
       "      <td>postero-anterior</td>\n",
       "      <td>Erect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377091</th>\n",
       "      <td>58c403aa-35ff8bd9-73e39f54-8dc9cc5d-e0ec3fa9</td>\n",
       "      <td>19999733</td>\n",
       "      <td>57132437</td>\n",
       "      <td>CHEST (PA AND LAT)</td>\n",
       "      <td>LATERAL</td>\n",
       "      <td>3056</td>\n",
       "      <td>2544</td>\n",
       "      <td>21520708</td>\n",
       "      <td>224550.171</td>\n",
       "      <td>CHEST (PA AND LAT)</td>\n",
       "      <td>lateral</td>\n",
       "      <td>Erect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377092</th>\n",
       "      <td>58766883-376a15ce-3b323a28-6af950a0-16b793bd</td>\n",
       "      <td>19999987</td>\n",
       "      <td>55368167</td>\n",
       "      <td>CHEST (PORTABLE AP)</td>\n",
       "      <td>AP</td>\n",
       "      <td>2544</td>\n",
       "      <td>3056</td>\n",
       "      <td>21451104</td>\n",
       "      <td>51448.218</td>\n",
       "      <td>CHEST (PORTABLE AP)</td>\n",
       "      <td>antero-posterior</td>\n",
       "      <td>Erect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377093</th>\n",
       "      <td>7ba273af-3d290f8d-e28d0ab4-484b7a86-7fc12b08</td>\n",
       "      <td>19999987</td>\n",
       "      <td>58621812</td>\n",
       "      <td>CHEST (PORTABLE AP)</td>\n",
       "      <td>AP</td>\n",
       "      <td>3056</td>\n",
       "      <td>2544</td>\n",
       "      <td>21451102</td>\n",
       "      <td>202809.234</td>\n",
       "      <td>CHEST (PORTABLE AP)</td>\n",
       "      <td>antero-posterior</td>\n",
       "      <td>Erect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377094</th>\n",
       "      <td>1a1fe7e3-cbac5d93-b339aeda-86bb86b5-4f31e82e</td>\n",
       "      <td>19999987</td>\n",
       "      <td>58971208</td>\n",
       "      <td>CHEST (PORTABLE AP)</td>\n",
       "      <td>AP</td>\n",
       "      <td>3056</td>\n",
       "      <td>2544</td>\n",
       "      <td>21451103</td>\n",
       "      <td>50507.625</td>\n",
       "      <td>CHEST (PORTABLE AP)</td>\n",
       "      <td>antero-posterior</td>\n",
       "      <td>Recumbent</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>377095 rows  12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            dicom_id  subject_id  study_id  \\\n",
       "0       02aa804e-bde0afdd-112c0b34-7bc16630-4e384014    10000032  50414267   \n",
       "1       174413ec-4ec4c1f7-34ea26b7-c5f994f8-79ef1962    10000032  50414267   \n",
       "2       2a2277a9-b0ded155-c0de8eb9-c124d10e-82c5caab    10000032  53189527   \n",
       "3       e084de3b-be89b11e-20fe3f9f-9c8d8dfe-4cfd202c    10000032  53189527   \n",
       "4       68b5c4b1-227d0485-9cc38c3f-7b84ab51-4b472714    10000032  53911762   \n",
       "...                                              ...         ...       ...   \n",
       "377090  428e2c18-5721d8f3-35a05001-36f3d080-9053b83c    19999733  57132437   \n",
       "377091  58c403aa-35ff8bd9-73e39f54-8dc9cc5d-e0ec3fa9    19999733  57132437   \n",
       "377092  58766883-376a15ce-3b323a28-6af950a0-16b793bd    19999987  55368167   \n",
       "377093  7ba273af-3d290f8d-e28d0ab4-484b7a86-7fc12b08    19999987  58621812   \n",
       "377094  1a1fe7e3-cbac5d93-b339aeda-86bb86b5-4f31e82e    19999987  58971208   \n",
       "\n",
       "       PerformedProcedureStepDescription ViewPosition  Rows  Columns  \\\n",
       "0                     CHEST (PA AND LAT)           PA  3056     2544   \n",
       "1                     CHEST (PA AND LAT)      LATERAL  3056     2544   \n",
       "2                     CHEST (PA AND LAT)           PA  3056     2544   \n",
       "3                     CHEST (PA AND LAT)      LATERAL  3056     2544   \n",
       "4                    CHEST (PORTABLE AP)           AP  2705     2539   \n",
       "...                                  ...          ...   ...      ...   \n",
       "377090                CHEST (PA AND LAT)           PA  3056     2544   \n",
       "377091                CHEST (PA AND LAT)      LATERAL  3056     2544   \n",
       "377092               CHEST (PORTABLE AP)           AP  2544     3056   \n",
       "377093               CHEST (PORTABLE AP)           AP  3056     2544   \n",
       "377094               CHEST (PORTABLE AP)           AP  3056     2544   \n",
       "\n",
       "        StudyDate   StudyTime ProcedureCodeSequence_CodeMeaning  \\\n",
       "0        21800506  213014.531                CHEST (PA AND LAT)   \n",
       "1        21800506  213014.531                CHEST (PA AND LAT)   \n",
       "2        21800626  165500.312                CHEST (PA AND LAT)   \n",
       "3        21800626  165500.312                CHEST (PA AND LAT)   \n",
       "4        21800723   80556.875               CHEST (PORTABLE AP)   \n",
       "...           ...         ...                               ...   \n",
       "377090   21520708  224550.171                CHEST (PA AND LAT)   \n",
       "377091   21520708  224550.171                CHEST (PA AND LAT)   \n",
       "377092   21451104   51448.218               CHEST (PORTABLE AP)   \n",
       "377093   21451102  202809.234               CHEST (PORTABLE AP)   \n",
       "377094   21451103   50507.625               CHEST (PORTABLE AP)   \n",
       "\n",
       "       ViewCodeSequence_CodeMeaning PatientOrientationCodeSequence_CodeMeaning  \n",
       "0                  postero-anterior                                      Erect  \n",
       "1                           lateral                                      Erect  \n",
       "2                  postero-anterior                                      Erect  \n",
       "3                           lateral                                      Erect  \n",
       "4                  antero-posterior                                        NaN  \n",
       "...                             ...                                        ...  \n",
       "377090             postero-anterior                                      Erect  \n",
       "377091                      lateral                                      Erect  \n",
       "377092             antero-posterior                                      Erect  \n",
       "377093             antero-posterior                                      Erect  \n",
       "377094             antero-posterior                                  Recumbent  \n",
       "\n",
       "[377095 rows x 12 columns]"
      ]
     },
     "execution_count": 509,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata_with_labels = metadata.merge(labels[['study_id']], how='inner', on='study_id').drop_duplicates(subset=['dicom_id'])\n",
    "metadata_with_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "id": "8c604450",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>study_id</th>\n",
       "      <th>index</th>\n",
       "      <th>dicom_id</th>\n",
       "      <th>subject_id</th>\n",
       "      <th>PerformedProcedureStepDescription</th>\n",
       "      <th>ViewPosition</th>\n",
       "      <th>Rows</th>\n",
       "      <th>Columns</th>\n",
       "      <th>StudyDate</th>\n",
       "      <th>StudyTime</th>\n",
       "      <th>ProcedureCodeSequence_CodeMeaning</th>\n",
       "      <th>ViewCodeSequence_CodeMeaning</th>\n",
       "      <th>PatientOrientationCodeSequence_CodeMeaning</th>\n",
       "      <th>stay_id</th>\n",
       "      <th>intime</th>\n",
       "      <th>outtime</th>\n",
       "      <th>StudyDateTime</th>\n",
       "      <th>time_diff</th>\n",
       "      <th>LOS</th>\n",
       "      <th>num_ehr_windows</th>\n",
       "      <th>lower</th>\n",
       "      <th>upper</th>\n",
       "      <th>stay</th>\n",
       "      <th>period_length</th>\n",
       "      <th>Acute and unspecified renal failure</th>\n",
       "      <th>Acute cerebrovascular disease</th>\n",
       "      <th>Acute myocardial infarction</th>\n",
       "      <th>Cardiac dysrhythmias</th>\n",
       "      <th>Chronic kidney disease</th>\n",
       "      <th>Chronic obstructive pulmonary disease and bronchiectasis</th>\n",
       "      <th>Complications of surgical procedures or medical care</th>\n",
       "      <th>Conduction disorders</th>\n",
       "      <th>Congestive heart failure; nonhypertensive</th>\n",
       "      <th>Coronary atherosclerosis and other heart disease</th>\n",
       "      <th>Diabetes mellitus with complications</th>\n",
       "      <th>Diabetes mellitus without complication</th>\n",
       "      <th>Disorders of lipid metabolism</th>\n",
       "      <th>Essential hypertension</th>\n",
       "      <th>Fluid and electrolyte disorders</th>\n",
       "      <th>Gastrointestinal hemorrhage</th>\n",
       "      <th>Hypertension with complications and secondary hypertension</th>\n",
       "      <th>Other liver diseases</th>\n",
       "      <th>Other lower respiratory disease</th>\n",
       "      <th>Other upper respiratory disease</th>\n",
       "      <th>Pleurisy; pneumothorax; pulmonary collapse</th>\n",
       "      <th>Pneumonia (except that caused by tuberculosis or sexually transmitted disease)</th>\n",
       "      <th>Respiratory failure; insufficiency; arrest (adult)</th>\n",
       "      <th>Septicemia (except in labor)</th>\n",
       "      <th>Shock</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>59469162</td>\n",
       "      <td>291809</td>\n",
       "      <td>5dfd960b-2e6378a2-1de9c84f-24ec4b38-f9d1a19a</td>\n",
       "      <td>17938576</td>\n",
       "      <td>CHEST (PORTABLE AP)</td>\n",
       "      <td>AP</td>\n",
       "      <td>2539</td>\n",
       "      <td>3050</td>\n",
       "      <td>21580123</td>\n",
       "      <td>213357</td>\n",
       "      <td>CHEST (PORTABLE AP)</td>\n",
       "      <td>antero-posterior</td>\n",
       "      <td>Erect</td>\n",
       "      <td>30002498</td>\n",
       "      <td>2158-01-23 16:00:00</td>\n",
       "      <td>2158-01-24 17:36:04</td>\n",
       "      <td>2158-01-23 21:33:57</td>\n",
       "      <td>5.566</td>\n",
       "      <td>25.601</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.000</td>\n",
       "      <td>17938576_episode1_timeseries.csv</td>\n",
       "      <td>25.601111</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>59469162</td>\n",
       "      <td>291809</td>\n",
       "      <td>5dfd960b-2e6378a2-1de9c84f-24ec4b38-f9d1a19a</td>\n",
       "      <td>17938576</td>\n",
       "      <td>CHEST (PORTABLE AP)</td>\n",
       "      <td>AP</td>\n",
       "      <td>2539</td>\n",
       "      <td>3050</td>\n",
       "      <td>21580123</td>\n",
       "      <td>213357</td>\n",
       "      <td>CHEST (PORTABLE AP)</td>\n",
       "      <td>antero-posterior</td>\n",
       "      <td>Erect</td>\n",
       "      <td>30002498</td>\n",
       "      <td>2158-01-24 04:00:00</td>\n",
       "      <td>2158-01-24 17:36:04</td>\n",
       "      <td>2158-01-23 21:33:57</td>\n",
       "      <td>-6.434</td>\n",
       "      <td>25.601</td>\n",
       "      <td>3</td>\n",
       "      <td>12.0</td>\n",
       "      <td>24.000</td>\n",
       "      <td>17938576_episode1_timeseries.csv</td>\n",
       "      <td>25.601111</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>59469162</td>\n",
       "      <td>291809</td>\n",
       "      <td>5dfd960b-2e6378a2-1de9c84f-24ec4b38-f9d1a19a</td>\n",
       "      <td>17938576</td>\n",
       "      <td>CHEST (PORTABLE AP)</td>\n",
       "      <td>AP</td>\n",
       "      <td>2539</td>\n",
       "      <td>3050</td>\n",
       "      <td>21580123</td>\n",
       "      <td>213357</td>\n",
       "      <td>CHEST (PORTABLE AP)</td>\n",
       "      <td>antero-posterior</td>\n",
       "      <td>Erect</td>\n",
       "      <td>30002498</td>\n",
       "      <td>2158-01-24 16:00:00</td>\n",
       "      <td>2158-01-24 17:36:04</td>\n",
       "      <td>2158-01-23 21:33:57</td>\n",
       "      <td>-18.434</td>\n",
       "      <td>25.601</td>\n",
       "      <td>3</td>\n",
       "      <td>24.0</td>\n",
       "      <td>26.601</td>\n",
       "      <td>17938576_episode1_timeseries.csv</td>\n",
       "      <td>25.601111</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>51967233</td>\n",
       "      <td>323833</td>\n",
       "      <td>d2bae3a3-3917d71b-f44edcd7-81c7017a-f15288e7</td>\n",
       "      <td>18730522</td>\n",
       "      <td>CHEST (PORTABLE AP)</td>\n",
       "      <td>AP</td>\n",
       "      <td>2539</td>\n",
       "      <td>3050</td>\n",
       "      <td>21530912</td>\n",
       "      <td>123408</td>\n",
       "      <td>DX CHEST PORTABLE PICC LINE PLACEMENT</td>\n",
       "      <td>antero-posterior</td>\n",
       "      <td>Erect</td>\n",
       "      <td>30004391</td>\n",
       "      <td>2153-09-05 13:12:00</td>\n",
       "      <td>2153-09-13 18:21:18</td>\n",
       "      <td>2153-09-12 12:34:08</td>\n",
       "      <td>167.369</td>\n",
       "      <td>197.155</td>\n",
       "      <td>17</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.000</td>\n",
       "      <td>18730522_episode2_timeseries.csv</td>\n",
       "      <td>197.155000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>51967233</td>\n",
       "      <td>323833</td>\n",
       "      <td>d2bae3a3-3917d71b-f44edcd7-81c7017a-f15288e7</td>\n",
       "      <td>18730522</td>\n",
       "      <td>CHEST (PORTABLE AP)</td>\n",
       "      <td>AP</td>\n",
       "      <td>2539</td>\n",
       "      <td>3050</td>\n",
       "      <td>21530912</td>\n",
       "      <td>123408</td>\n",
       "      <td>DX CHEST PORTABLE PICC LINE PLACEMENT</td>\n",
       "      <td>antero-posterior</td>\n",
       "      <td>Erect</td>\n",
       "      <td>30004391</td>\n",
       "      <td>2153-09-06 01:12:00</td>\n",
       "      <td>2153-09-13 18:21:18</td>\n",
       "      <td>2153-09-12 12:34:08</td>\n",
       "      <td>155.369</td>\n",
       "      <td>197.155</td>\n",
       "      <td>17</td>\n",
       "      <td>12.0</td>\n",
       "      <td>24.000</td>\n",
       "      <td>18730522_episode2_timeseries.csv</td>\n",
       "      <td>197.155000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   study_id   index                                      dicom_id  subject_id  \\\n",
       "0  59469162  291809  5dfd960b-2e6378a2-1de9c84f-24ec4b38-f9d1a19a    17938576   \n",
       "1  59469162  291809  5dfd960b-2e6378a2-1de9c84f-24ec4b38-f9d1a19a    17938576   \n",
       "2  59469162  291809  5dfd960b-2e6378a2-1de9c84f-24ec4b38-f9d1a19a    17938576   \n",
       "3  51967233  323833  d2bae3a3-3917d71b-f44edcd7-81c7017a-f15288e7    18730522   \n",
       "4  51967233  323833  d2bae3a3-3917d71b-f44edcd7-81c7017a-f15288e7    18730522   \n",
       "\n",
       "  PerformedProcedureStepDescription ViewPosition  Rows  Columns  StudyDate  \\\n",
       "0               CHEST (PORTABLE AP)           AP  2539     3050   21580123   \n",
       "1               CHEST (PORTABLE AP)           AP  2539     3050   21580123   \n",
       "2               CHEST (PORTABLE AP)           AP  2539     3050   21580123   \n",
       "3               CHEST (PORTABLE AP)           AP  2539     3050   21530912   \n",
       "4               CHEST (PORTABLE AP)           AP  2539     3050   21530912   \n",
       "\n",
       "  StudyTime      ProcedureCodeSequence_CodeMeaning  \\\n",
       "0    213357                    CHEST (PORTABLE AP)   \n",
       "1    213357                    CHEST (PORTABLE AP)   \n",
       "2    213357                    CHEST (PORTABLE AP)   \n",
       "3    123408  DX CHEST PORTABLE PICC LINE PLACEMENT   \n",
       "4    123408  DX CHEST PORTABLE PICC LINE PLACEMENT   \n",
       "\n",
       "  ViewCodeSequence_CodeMeaning PatientOrientationCodeSequence_CodeMeaning  \\\n",
       "0             antero-posterior                                      Erect   \n",
       "1             antero-posterior                                      Erect   \n",
       "2             antero-posterior                                      Erect   \n",
       "3             antero-posterior                                      Erect   \n",
       "4             antero-posterior                                      Erect   \n",
       "\n",
       "    stay_id              intime             outtime       StudyDateTime  \\\n",
       "0  30002498 2158-01-23 16:00:00 2158-01-24 17:36:04 2158-01-23 21:33:57   \n",
       "1  30002498 2158-01-24 04:00:00 2158-01-24 17:36:04 2158-01-23 21:33:57   \n",
       "2  30002498 2158-01-24 16:00:00 2158-01-24 17:36:04 2158-01-23 21:33:57   \n",
       "3  30004391 2153-09-05 13:12:00 2153-09-13 18:21:18 2153-09-12 12:34:08   \n",
       "4  30004391 2153-09-06 01:12:00 2153-09-13 18:21:18 2153-09-12 12:34:08   \n",
       "\n",
       "   time_diff      LOS  num_ehr_windows  lower   upper  \\\n",
       "0      5.566   25.601                3    0.0  12.000   \n",
       "1     -6.434   25.601                3   12.0  24.000   \n",
       "2    -18.434   25.601                3   24.0  26.601   \n",
       "3    167.369  197.155               17    0.0  12.000   \n",
       "4    155.369  197.155               17   12.0  24.000   \n",
       "\n",
       "                               stay  period_length  \\\n",
       "0  17938576_episode1_timeseries.csv      25.601111   \n",
       "1  17938576_episode1_timeseries.csv      25.601111   \n",
       "2  17938576_episode1_timeseries.csv      25.601111   \n",
       "3  18730522_episode2_timeseries.csv     197.155000   \n",
       "4  18730522_episode2_timeseries.csv     197.155000   \n",
       "\n",
       "   Acute and unspecified renal failure  Acute cerebrovascular disease  \\\n",
       "0                                    0                              1   \n",
       "1                                    0                              1   \n",
       "2                                    0                              1   \n",
       "3                                    1                              0   \n",
       "4                                    1                              0   \n",
       "\n",
       "   Acute myocardial infarction  Cardiac dysrhythmias  Chronic kidney disease  \\\n",
       "0                            0                     0                       0   \n",
       "1                            0                     0                       0   \n",
       "2                            0                     0                       0   \n",
       "3                            0                     1                       0   \n",
       "4                            0                     1                       0   \n",
       "\n",
       "   Chronic obstructive pulmonary disease and bronchiectasis  \\\n",
       "0                                                  1          \n",
       "1                                                  1          \n",
       "2                                                  1          \n",
       "3                                                  0          \n",
       "4                                                  0          \n",
       "\n",
       "   Complications of surgical procedures or medical care  Conduction disorders  \\\n",
       "0                                                  0                        0   \n",
       "1                                                  0                        0   \n",
       "2                                                  0                        0   \n",
       "3                                                  0                        0   \n",
       "4                                                  0                        0   \n",
       "\n",
       "   Congestive heart failure; nonhypertensive  \\\n",
       "0                                          0   \n",
       "1                                          0   \n",
       "2                                          0   \n",
       "3                                          1   \n",
       "4                                          1   \n",
       "\n",
       "   Coronary atherosclerosis and other heart disease  \\\n",
       "0                                                 1   \n",
       "1                                                 1   \n",
       "2                                                 1   \n",
       "3                                                 0   \n",
       "4                                                 0   \n",
       "\n",
       "   Diabetes mellitus with complications  \\\n",
       "0                                     0   \n",
       "1                                     0   \n",
       "2                                     0   \n",
       "3                                     0   \n",
       "4                                     0   \n",
       "\n",
       "   Diabetes mellitus without complication  Disorders of lipid metabolism  \\\n",
       "0                                       0                              0   \n",
       "1                                       0                              0   \n",
       "2                                       0                              0   \n",
       "3                                       0                              0   \n",
       "4                                       0                              0   \n",
       "\n",
       "   Essential hypertension  Fluid and electrolyte disorders  \\\n",
       "0                       0                                0   \n",
       "1                       0                                0   \n",
       "2                       0                                0   \n",
       "3                       0                                1   \n",
       "4                       0                                1   \n",
       "\n",
       "   Gastrointestinal hemorrhage  \\\n",
       "0                            0   \n",
       "1                            0   \n",
       "2                            0   \n",
       "3                            0   \n",
       "4                            0   \n",
       "\n",
       "   Hypertension with complications and secondary hypertension  \\\n",
       "0                                                  0            \n",
       "1                                                  0            \n",
       "2                                                  0            \n",
       "3                                                  0            \n",
       "4                                                  0            \n",
       "\n",
       "   Other liver diseases  Other lower respiratory disease  \\\n",
       "0                     0                                0   \n",
       "1                     0                                0   \n",
       "2                     0                                0   \n",
       "3                     0                                0   \n",
       "4                     0                                0   \n",
       "\n",
       "   Other upper respiratory disease  \\\n",
       "0                                0   \n",
       "1                                0   \n",
       "2                                0   \n",
       "3                                0   \n",
       "4                                0   \n",
       "\n",
       "   Pleurisy; pneumothorax; pulmonary collapse  \\\n",
       "0                                           0   \n",
       "1                                           0   \n",
       "2                                           0   \n",
       "3                                           0   \n",
       "4                                           0   \n",
       "\n",
       "   Pneumonia (except that caused by tuberculosis or sexually transmitted disease)  \\\n",
       "0                                                  1                                \n",
       "1                                                  1                                \n",
       "2                                                  1                                \n",
       "3                                                  1                                \n",
       "4                                                  1                                \n",
       "\n",
       "   Respiratory failure; insufficiency; arrest (adult)  \\\n",
       "0                                                  0    \n",
       "1                                                  0    \n",
       "2                                                  0    \n",
       "3                                                  1    \n",
       "4                                                  1    \n",
       "\n",
       "   Septicemia (except in labor)  Shock  \n",
       "0                             1      1  \n",
       "1                             1      1  \n",
       "2                             1      1  \n",
       "3                             1      1  \n",
       "4                             1      1  "
      ]
     },
     "execution_count": 510,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_meta_with_labels = train_meta_with_labels.merge(metadata_with_labels[['dicom_id']], how='inner', on='dicom_id')\n",
    "train_meta_with_labels.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "id": "74370bf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['study_id', 'index', 'dicom_id', 'subject_id',\n",
       "       'PerformedProcedureStepDescription', 'ViewPosition', 'Rows', 'Columns',\n",
       "       'StudyDate', 'StudyTime', 'ProcedureCodeSequence_CodeMeaning',\n",
       "       'ViewCodeSequence_CodeMeaning',\n",
       "       'PatientOrientationCodeSequence_CodeMeaning', 'stay_id', 'intime',\n",
       "       'outtime', 'StudyDateTime', 'time_diff', 'LOS', 'num_ehr_windows',\n",
       "       'lower', 'upper', 'stay', 'period_length',\n",
       "       'Acute and unspecified renal failure', 'Acute cerebrovascular disease',\n",
       "       'Acute myocardial infarction', 'Cardiac dysrhythmias',\n",
       "       'Chronic kidney disease',\n",
       "       'Chronic obstructive pulmonary disease and bronchiectasis',\n",
       "       'Complications of surgical procedures or medical care',\n",
       "       'Conduction disorders', 'Congestive heart failure; nonhypertensive',\n",
       "       'Coronary atherosclerosis and other heart disease',\n",
       "       'Diabetes mellitus with complications',\n",
       "       'Diabetes mellitus without complication',\n",
       "       'Disorders of lipid metabolism', 'Essential hypertension',\n",
       "       'Fluid and electrolyte disorders', 'Gastrointestinal hemorrhage',\n",
       "       'Hypertension with complications and secondary hypertension',\n",
       "       'Other liver diseases', 'Other lower respiratory disease',\n",
       "       'Other upper respiratory disease',\n",
       "       'Pleurisy; pneumothorax; pulmonary collapse',\n",
       "       'Pneumonia (except that caused by tuberculosis or sexually transmitted disease)',\n",
       "       'Respiratory failure; insufficiency; arrest (adult)',\n",
       "       'Septicemia (except in labor)', 'Shock'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 567,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_meta_with_labels.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "id": "2e262802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CXR images= 377110\n",
      "Number of ICU stays= 59372\n",
      "Number of CXR associated with ICU stay based on subject ID= 368350\n",
      "Number of unique CXR dicoms= 181195\n",
      "Number of unique CXR study id= 122087\n",
      "Mean time cxr - intime=  74.23168108953072\n",
      "Minimum time = -601.239\n",
      "Maximum time = 2368.942\n",
      "Excluding CXR with missing radiology reports =  69657\n",
      "69657\n",
      "7696\n",
      "20442\n"
     ]
    }
   ],
   "source": [
    "tr, va, ts = load_cxr_ehr(args,\n",
    "             ehr_train_ds=ehrtr,\n",
    "             ehr_val_ds=ehrva,\n",
    "             ehr_test_ds=ehrts,\n",
    "             cxr_train_ds=cxrtr,\n",
    "             cxr_val_ds=cxrva,\n",
    "             cxr_test_ds=cxrts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "id": "683e8e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = (next(iter(tr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "id": "22c9fa45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[[-16.60627547,  -0.06021818,  -0.2673962 , ...,  -0.55850585,\n",
       "           -0.14534766,  -0.29126413],\n",
       "         [-16.60627547,  -0.06021818,  -0.2673962 , ...,  -0.55850585,\n",
       "           -0.14534766,  -0.29126413],\n",
       "         [-16.60627547,  -0.06021818,  -0.2673962 , ...,  -0.55850585,\n",
       "           -0.14534766,  -0.29126413],\n",
       "         ...,\n",
       "         [  0.        ,   0.        ,   0.        , ...,   0.        ,\n",
       "            0.        ,   0.        ],\n",
       "         [  0.        ,   0.        ,   0.        , ...,   0.        ,\n",
       "            0.        ,   0.        ],\n",
       "         [  0.        ,   0.        ,   0.        , ...,   0.        ,\n",
       "            0.        ,   0.        ]],\n",
       " \n",
       "        [[-16.60627547,  -0.06021818,  -0.2673962 , ...,  -0.55850585,\n",
       "           -0.14534766,  -0.29126413],\n",
       "         [-16.60627547,  -0.06021818,  -0.2673962 , ...,  -0.55850585,\n",
       "           -0.14534766,  -0.29126413],\n",
       "         [-16.60627547,  -0.06021818,  -0.2673962 , ...,  -0.55850585,\n",
       "           -0.14534766,  -0.29126413],\n",
       "         ...,\n",
       "         [  0.        ,   0.        ,   0.        , ...,   0.        ,\n",
       "            0.        ,   0.        ],\n",
       "         [  0.        ,   0.        ,   0.        , ...,   0.        ,\n",
       "            0.        ,   0.        ],\n",
       "         [  0.        ,   0.        ,   0.        , ...,   0.        ,\n",
       "            0.        ,   0.        ]],\n",
       " \n",
       "        [[-16.60627547,  -0.06021818,  -0.2673962 , ...,  -0.55850585,\n",
       "           -0.14534766,  -0.29126413],\n",
       "         [-16.60627547,  -0.06021818,  -0.2673962 , ...,  -0.55850585,\n",
       "           -0.14534766,  -0.29126413],\n",
       "         [-16.60627547,  -0.06021818,  -0.2673962 , ...,  -0.55850585,\n",
       "           -0.14534766,  -0.29126413],\n",
       "         ...,\n",
       "         [  0.        ,   0.        ,   0.        , ...,   0.        ,\n",
       "            0.        ,   0.        ],\n",
       "         [  0.        ,   0.        ,   0.        , ...,   0.        ,\n",
       "            0.        ,   0.        ],\n",
       "         [  0.        ,   0.        ,   0.        , ...,   0.        ,\n",
       "            0.        ,   0.        ]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[-16.60627547,  -0.06021818,  -0.2673962 , ...,  -0.55850585,\n",
       "           -0.14534766,  -0.29126413],\n",
       "         [-16.60627547,  -0.06021818,  -0.2673962 , ...,  -0.55850585,\n",
       "           -0.14534766,  -0.29126413],\n",
       "         [-16.60627547,  -0.06021818,  -0.2673962 , ...,  -0.55850585,\n",
       "           -0.14534766,  -0.29126413],\n",
       "         ...,\n",
       "         [  0.        ,   0.        ,   0.        , ...,   0.        ,\n",
       "            0.        ,   0.        ],\n",
       "         [  0.        ,   0.        ,   0.        , ...,   0.        ,\n",
       "            0.        ,   0.        ],\n",
       "         [  0.        ,   0.        ,   0.        , ...,   0.        ,\n",
       "            0.        ,   0.        ]],\n",
       " \n",
       "        [[-16.60627547,  -0.06021818,  -0.2673962 , ...,  -0.55850585,\n",
       "           -0.14534766,  -0.29126413],\n",
       "         [-16.60627547,  -0.06021818,  -0.2673962 , ...,  -0.55850585,\n",
       "           -0.14534766,  -0.29126413],\n",
       "         [-16.60627547,  -0.06021818,  -0.2673962 , ...,  -0.55850585,\n",
       "           -0.14534766,  -0.29126413],\n",
       "         ...,\n",
       "         [  0.        ,   0.        ,   0.        , ...,   0.        ,\n",
       "            0.        ,   0.        ],\n",
       "         [  0.        ,   0.        ,   0.        , ...,   0.        ,\n",
       "            0.        ,   0.        ],\n",
       "         [  0.        ,   0.        ,   0.        , ...,   0.        ,\n",
       "            0.        ,   0.        ]],\n",
       " \n",
       "        [[-16.60627547,  -0.06021818,  -0.2673962 , ...,  -0.55850585,\n",
       "           -0.14534766,  -0.29126413],\n",
       "         [-16.60627547,  -0.06021818,  -0.2673962 , ...,  -0.55850585,\n",
       "           -0.14534766,  -0.29126413],\n",
       "         [-16.60627547,  -0.06021818,  -0.2673962 , ...,  -0.55850585,\n",
       "           -0.14534766,  -0.29126413],\n",
       "         ...,\n",
       "         [  0.        ,   0.        ,   0.        , ...,   0.        ,\n",
       "            0.        ,   0.        ],\n",
       "         [  0.        ,   0.        ,   0.        , ...,   0.        ,\n",
       "            0.        ,   0.        ],\n",
       "         [  0.        ,   0.        ,   0.        , ...,   0.        ,\n",
       "            0.        ,   0.        ]]]),\n",
       " tensor([[[[0.0157, 0.0157, 0.0157,  ..., 0.0549, 0.0588, 0.0627],\n",
       "           [0.0157, 0.0157, 0.0157,  ..., 0.0549, 0.0549, 0.0588],\n",
       "           [0.0157, 0.0157, 0.0157,  ..., 0.0510, 0.0510, 0.0549],\n",
       "           ...,\n",
       "           [0.0157, 0.0157, 0.0157,  ..., 0.0157, 0.0157, 0.0157],\n",
       "           [0.0157, 0.0157, 0.0157,  ..., 0.0157, 0.0157, 0.0157],\n",
       "           [0.0157, 0.0157, 0.0157,  ..., 0.0157, 0.0157, 0.0157]],\n",
       " \n",
       "          [[0.0157, 0.0157, 0.0157,  ..., 0.0549, 0.0588, 0.0627],\n",
       "           [0.0157, 0.0157, 0.0157,  ..., 0.0549, 0.0549, 0.0588],\n",
       "           [0.0157, 0.0157, 0.0157,  ..., 0.0510, 0.0510, 0.0549],\n",
       "           ...,\n",
       "           [0.0157, 0.0157, 0.0157,  ..., 0.0157, 0.0157, 0.0157],\n",
       "           [0.0157, 0.0157, 0.0157,  ..., 0.0157, 0.0157, 0.0157],\n",
       "           [0.0157, 0.0157, 0.0157,  ..., 0.0157, 0.0157, 0.0157]],\n",
       " \n",
       "          [[0.0157, 0.0157, 0.0157,  ..., 0.0549, 0.0588, 0.0627],\n",
       "           [0.0157, 0.0157, 0.0157,  ..., 0.0549, 0.0549, 0.0588],\n",
       "           [0.0157, 0.0157, 0.0157,  ..., 0.0510, 0.0510, 0.0549],\n",
       "           ...,\n",
       "           [0.0157, 0.0157, 0.0157,  ..., 0.0157, 0.0157, 0.0157],\n",
       "           [0.0157, 0.0157, 0.0157,  ..., 0.0157, 0.0157, 0.0157],\n",
       "           [0.0157, 0.0157, 0.0157,  ..., 0.0157, 0.0157, 0.0157]]],\n",
       " \n",
       " \n",
       "         [[[0.0078, 0.0039, 0.0000,  ..., 0.0392, 0.0392, 0.0353],\n",
       "           [0.0078, 0.0039, 0.0039,  ..., 0.0431, 0.0431, 0.0392],\n",
       "           [0.0078, 0.0078, 0.0118,  ..., 0.0392, 0.0431, 0.0431],\n",
       "           ...,\n",
       "           [0.6314, 0.6314, 0.6392,  ..., 0.3608, 0.3686, 0.3765],\n",
       "           [0.6431, 0.6392, 0.6431,  ..., 0.3569, 0.3725, 0.3922],\n",
       "           [0.6431, 0.6431, 0.6588,  ..., 0.3569, 0.3725, 0.3882]],\n",
       " \n",
       "          [[0.0078, 0.0039, 0.0000,  ..., 0.0392, 0.0392, 0.0353],\n",
       "           [0.0078, 0.0039, 0.0039,  ..., 0.0431, 0.0431, 0.0392],\n",
       "           [0.0078, 0.0078, 0.0118,  ..., 0.0392, 0.0431, 0.0431],\n",
       "           ...,\n",
       "           [0.6314, 0.6314, 0.6392,  ..., 0.3608, 0.3686, 0.3765],\n",
       "           [0.6431, 0.6392, 0.6431,  ..., 0.3569, 0.3725, 0.3922],\n",
       "           [0.6431, 0.6431, 0.6588,  ..., 0.3569, 0.3725, 0.3882]],\n",
       " \n",
       "          [[0.0078, 0.0039, 0.0000,  ..., 0.0392, 0.0392, 0.0353],\n",
       "           [0.0078, 0.0039, 0.0039,  ..., 0.0431, 0.0431, 0.0392],\n",
       "           [0.0078, 0.0078, 0.0118,  ..., 0.0392, 0.0431, 0.0431],\n",
       "           ...,\n",
       "           [0.6314, 0.6314, 0.6392,  ..., 0.3608, 0.3686, 0.3765],\n",
       "           [0.6431, 0.6392, 0.6431,  ..., 0.3569, 0.3725, 0.3922],\n",
       "           [0.6431, 0.6431, 0.6588,  ..., 0.3569, 0.3725, 0.3882]]],\n",
       " \n",
       " \n",
       "         [[[0.2039, 0.2039, 0.2039,  ..., 0.1843, 0.1843, 0.1843],\n",
       "           [0.2039, 0.2039, 0.2039,  ..., 0.1804, 0.1804, 0.1804],\n",
       "           [0.2039, 0.2039, 0.2039,  ..., 0.1804, 0.1804, 0.1804],\n",
       "           ...,\n",
       "           [0.2471, 0.2471, 0.2471,  ..., 0.2588, 0.2588, 0.2588],\n",
       "           [0.2471, 0.2471, 0.2471,  ..., 0.2588, 0.2588, 0.2588],\n",
       "           [0.2471, 0.2471, 0.2510,  ..., 0.2588, 0.2588, 0.2588]],\n",
       " \n",
       "          [[0.2039, 0.2039, 0.2039,  ..., 0.1843, 0.1843, 0.1843],\n",
       "           [0.2039, 0.2039, 0.2039,  ..., 0.1804, 0.1804, 0.1804],\n",
       "           [0.2039, 0.2039, 0.2039,  ..., 0.1804, 0.1804, 0.1804],\n",
       "           ...,\n",
       "           [0.2471, 0.2471, 0.2471,  ..., 0.2588, 0.2588, 0.2588],\n",
       "           [0.2471, 0.2471, 0.2471,  ..., 0.2588, 0.2588, 0.2588],\n",
       "           [0.2471, 0.2471, 0.2510,  ..., 0.2588, 0.2588, 0.2588]],\n",
       " \n",
       "          [[0.2039, 0.2039, 0.2039,  ..., 0.1843, 0.1843, 0.1843],\n",
       "           [0.2039, 0.2039, 0.2039,  ..., 0.1804, 0.1804, 0.1804],\n",
       "           [0.2039, 0.2039, 0.2039,  ..., 0.1804, 0.1804, 0.1804],\n",
       "           ...,\n",
       "           [0.2471, 0.2471, 0.2471,  ..., 0.2588, 0.2588, 0.2588],\n",
       "           [0.2471, 0.2471, 0.2471,  ..., 0.2588, 0.2588, 0.2588],\n",
       "           [0.2471, 0.2471, 0.2510,  ..., 0.2588, 0.2588, 0.2588]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[0.0078, 0.0078, 0.0078,  ..., 0.0078, 0.0078, 0.0078],\n",
       "           [0.0078, 0.0078, 0.0078,  ..., 0.0078, 0.0078, 0.0078],\n",
       "           [0.0078, 0.0078, 0.0078,  ..., 0.0078, 0.0078, 0.0078],\n",
       "           ...,\n",
       "           [0.0078, 0.0078, 0.0078,  ..., 0.0078, 0.0078, 0.0078],\n",
       "           [0.0078, 0.0078, 0.0078,  ..., 0.0078, 0.0078, 0.0078],\n",
       "           [0.0078, 0.0078, 0.0078,  ..., 0.0078, 0.0078, 0.0078]],\n",
       " \n",
       "          [[0.0078, 0.0078, 0.0078,  ..., 0.0078, 0.0078, 0.0078],\n",
       "           [0.0078, 0.0078, 0.0078,  ..., 0.0078, 0.0078, 0.0078],\n",
       "           [0.0078, 0.0078, 0.0078,  ..., 0.0078, 0.0078, 0.0078],\n",
       "           ...,\n",
       "           [0.0078, 0.0078, 0.0078,  ..., 0.0078, 0.0078, 0.0078],\n",
       "           [0.0078, 0.0078, 0.0078,  ..., 0.0078, 0.0078, 0.0078],\n",
       "           [0.0078, 0.0078, 0.0078,  ..., 0.0078, 0.0078, 0.0078]],\n",
       " \n",
       "          [[0.0078, 0.0078, 0.0078,  ..., 0.0078, 0.0078, 0.0078],\n",
       "           [0.0078, 0.0078, 0.0078,  ..., 0.0078, 0.0078, 0.0078],\n",
       "           [0.0078, 0.0078, 0.0078,  ..., 0.0078, 0.0078, 0.0078],\n",
       "           ...,\n",
       "           [0.0078, 0.0078, 0.0078,  ..., 0.0078, 0.0078, 0.0078],\n",
       "           [0.0078, 0.0078, 0.0078,  ..., 0.0078, 0.0078, 0.0078],\n",
       "           [0.0078, 0.0078, 0.0078,  ..., 0.0078, 0.0078, 0.0078]]],\n",
       " \n",
       " \n",
       "         [[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           ...,\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       " \n",
       "          [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           ...,\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       " \n",
       "          [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           ...,\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],\n",
       " \n",
       " \n",
       "         [[[0.0000, 0.0000, 0.0000,  ..., 0.0118, 0.0275, 0.0275],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0353, 0.0275, 0.0275],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0471, 0.0392, 0.0392],\n",
       "           ...,\n",
       "           [0.5137, 0.5176, 0.5255,  ..., 0.8863, 0.8863, 0.8863],\n",
       "           [0.4941, 0.5176, 0.5255,  ..., 0.8863, 0.8863, 0.8863],\n",
       "           [0.4863, 0.5137, 0.5255,  ..., 0.8863, 0.8863, 0.8863]],\n",
       " \n",
       "          [[0.0000, 0.0000, 0.0000,  ..., 0.0118, 0.0275, 0.0275],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0353, 0.0275, 0.0275],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0471, 0.0392, 0.0392],\n",
       "           ...,\n",
       "           [0.5137, 0.5176, 0.5255,  ..., 0.8863, 0.8863, 0.8863],\n",
       "           [0.4941, 0.5176, 0.5255,  ..., 0.8863, 0.8863, 0.8863],\n",
       "           [0.4863, 0.5137, 0.5255,  ..., 0.8863, 0.8863, 0.8863]],\n",
       " \n",
       "          [[0.0000, 0.0000, 0.0000,  ..., 0.0118, 0.0275, 0.0275],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0353, 0.0275, 0.0275],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0471, 0.0392, 0.0392],\n",
       "           ...,\n",
       "           [0.5137, 0.5176, 0.5255,  ..., 0.8863, 0.8863, 0.8863],\n",
       "           [0.4941, 0.5176, 0.5255,  ..., 0.8863, 0.8863, 0.8863],\n",
       "           [0.4863, 0.5137, 0.5255,  ..., 0.8863, 0.8863, 0.8863]]]]),\n",
       " array([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [1, 0, 1, ..., 0, 0, 1],\n",
       "        [0, 0, 0, ..., 1, 0, 0],\n",
       "        ...,\n",
       "        [1, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 1, 1, 1],\n",
       "        [1, 0, 1, ..., 1, 0, 1]], dtype=int32),\n",
       " tensor([[0., 0., 0.,  ..., 0., 0., 1.],\n",
       "         [0., 0., 0.,  ..., 1., 0., 1.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 1.],\n",
       "         ...,\n",
       "         [1., 0., 0.,  ..., 0., 0., 1.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 1.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 1.]]),\n",
       " [19,\n",
       "  18,\n",
       "  25,\n",
       "  34,\n",
       "  22,\n",
       "  30,\n",
       "  52,\n",
       "  18,\n",
       "  20,\n",
       "  30,\n",
       "  17,\n",
       "  24,\n",
       "  37,\n",
       "  3,\n",
       "  27,\n",
       "  10,\n",
       "  30,\n",
       "  17,\n",
       "  23,\n",
       "  19,\n",
       "  22,\n",
       "  7,\n",
       "  17,\n",
       "  19,\n",
       "  22,\n",
       "  33,\n",
       "  27,\n",
       "  20,\n",
       "  23,\n",
       "  20,\n",
       "  17,\n",
       "  33,\n",
       "  34,\n",
       "  32,\n",
       "  20,\n",
       "  19,\n",
       "  18,\n",
       "  19,\n",
       "  28,\n",
       "  18,\n",
       "  29,\n",
       "  25,\n",
       "  25,\n",
       "  20,\n",
       "  20,\n",
       "  22,\n",
       "  25,\n",
       "  22,\n",
       "  15,\n",
       "  22,\n",
       "  35,\n",
       "  10,\n",
       "  18,\n",
       "  4,\n",
       "  32,\n",
       "  49,\n",
       "  27,\n",
       "  44,\n",
       "  23,\n",
       "  24,\n",
       "  4,\n",
       "  5,\n",
       "  34,\n",
       "  30,\n",
       "  19,\n",
       "  32,\n",
       "  32,\n",
       "  32,\n",
       "  24,\n",
       "  23,\n",
       "  19,\n",
       "  35,\n",
       "  35,\n",
       "  19,\n",
       "  20,\n",
       "  19,\n",
       "  18,\n",
       "  39,\n",
       "  20,\n",
       "  19,\n",
       "  17,\n",
       "  34,\n",
       "  23,\n",
       "  24,\n",
       "  15,\n",
       "  9,\n",
       "  33,\n",
       "  78,\n",
       "  19,\n",
       "  33,\n",
       "  27,\n",
       "  19,\n",
       "  20,\n",
       "  19,\n",
       "  39,\n",
       "  20,\n",
       "  17,\n",
       "  13,\n",
       "  3,\n",
       "  22,\n",
       "  18,\n",
       "  18,\n",
       "  17,\n",
       "  34,\n",
       "  18,\n",
       "  23,\n",
       "  9,\n",
       "  28,\n",
       "  34,\n",
       "  30,\n",
       "  7,\n",
       "  18,\n",
       "  20,\n",
       "  8,\n",
       "  20,\n",
       "  32,\n",
       "  20,\n",
       "  14,\n",
       "  18,\n",
       "  5,\n",
       "  20,\n",
       "  42,\n",
       "  27,\n",
       "  14,\n",
       "  28,\n",
       "  24,\n",
       "  18,\n",
       "  25,\n",
       "  20,\n",
       "  18,\n",
       "  18,\n",
       "  45,\n",
       "  44,\n",
       "  22,\n",
       "  45,\n",
       "  17,\n",
       "  33,\n",
       "  19,\n",
       "  22,\n",
       "  19,\n",
       "  33,\n",
       "  17,\n",
       "  18,\n",
       "  24,\n",
       "  24,\n",
       "  18,\n",
       "  34,\n",
       "  24,\n",
       "  22,\n",
       "  49,\n",
       "  9,\n",
       "  24,\n",
       "  22,\n",
       "  22,\n",
       "  30,\n",
       "  23,\n",
       "  19,\n",
       "  4,\n",
       "  39,\n",
       "  34,\n",
       "  34,\n",
       "  18,\n",
       "  18,\n",
       "  32,\n",
       "  23,\n",
       "  20,\n",
       "  43,\n",
       "  18,\n",
       "  32,\n",
       "  27,\n",
       "  3,\n",
       "  28,\n",
       "  18,\n",
       "  19,\n",
       "  15,\n",
       "  37,\n",
       "  25,\n",
       "  28,\n",
       "  28,\n",
       "  20,\n",
       "  32,\n",
       "  3,\n",
       "  3,\n",
       "  34,\n",
       "  24,\n",
       "  33,\n",
       "  37,\n",
       "  15,\n",
       "  3,\n",
       "  19,\n",
       "  67,\n",
       "  30,\n",
       "  45,\n",
       "  48,\n",
       "  18,\n",
       "  2,\n",
       "  19,\n",
       "  18,\n",
       "  20,\n",
       "  19,\n",
       "  2,\n",
       "  35,\n",
       "  34,\n",
       "  30,\n",
       "  9,\n",
       "  18,\n",
       "  25,\n",
       "  22,\n",
       "  35,\n",
       "  14,\n",
       "  44,\n",
       "  18,\n",
       "  3,\n",
       "  33,\n",
       "  23,\n",
       "  64,\n",
       "  19,\n",
       "  23,\n",
       "  20,\n",
       "  32,\n",
       "  3,\n",
       "  42,\n",
       "  24,\n",
       "  24,\n",
       "  3,\n",
       "  23,\n",
       "  52,\n",
       "  22,\n",
       "  18,\n",
       "  20,\n",
       "  25,\n",
       "  5,\n",
       "  24,\n",
       "  22,\n",
       "  28,\n",
       "  24,\n",
       "  3,\n",
       "  38,\n",
       "  20,\n",
       "  17,\n",
       "  19,\n",
       "  19,\n",
       "  22,\n",
       "  20,\n",
       "  35,\n",
       "  17,\n",
       "  17,\n",
       "  17,\n",
       "  24,\n",
       "  25,\n",
       "  27,\n",
       "  25,\n",
       "  24,\n",
       "  35,\n",
       "  22,\n",
       "  25],\n",
       " [True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True],\n",
       " [-19.641,\n",
       "  7.325,\n",
       "  -14.768,\n",
       "  -3.845,\n",
       "  97.336,\n",
       "  -23.493,\n",
       "  19.271,\n",
       "  -26.325,\n",
       "  44.963,\n",
       "  275.681,\n",
       "  -41.597,\n",
       "  60.485,\n",
       "  41.897,\n",
       "  -23.849,\n",
       "  86.826,\n",
       "  -21.708,\n",
       "  721.994,\n",
       "  148.142,\n",
       "  170.86,\n",
       "  11.172,\n",
       "  119.91,\n",
       "  -3.894,\n",
       "  -3.711,\n",
       "  273.48,\n",
       "  1.769,\n",
       "  -39.087,\n",
       "  34.478,\n",
       "  207.005,\n",
       "  58.221,\n",
       "  109.192,\n",
       "  1.924,\n",
       "  4.21,\n",
       "  -28.661,\n",
       "  35.482,\n",
       "  351.652,\n",
       "  -17.279,\n",
       "  -1.387,\n",
       "  35.373,\n",
       "  940.758,\n",
       "  194.359,\n",
       "  -8.472,\n",
       "  9.632,\n",
       "  94.571,\n",
       "  302.826,\n",
       "  -43.715,\n",
       "  364.631,\n",
       "  86.266,\n",
       "  207.443,\n",
       "  -145.629,\n",
       "  34.789,\n",
       "  432.295,\n",
       "  -1.103,\n",
       "  309.587,\n",
       "  -30.421,\n",
       "  -13.469,\n",
       "  31.001,\n",
       "  -5.103,\n",
       "  23.336,\n",
       "  11.124,\n",
       "  80.926,\n",
       "  -10.559,\n",
       "  -22.431,\n",
       "  -32.043,\n",
       "  -29.053,\n",
       "  108.123,\n",
       "  28.591,\n",
       "  93.424,\n",
       "  25.902,\n",
       "  99.519,\n",
       "  52.976,\n",
       "  23.857,\n",
       "  4.935,\n",
       "  -16.455,\n",
       "  -4.37,\n",
       "  11.944,\n",
       "  275.721,\n",
       "  15.171,\n",
       "  166.044,\n",
       "  -77.768,\n",
       "  10.505,\n",
       "  -0.581,\n",
       "  53.611,\n",
       "  182.048,\n",
       "  55.66,\n",
       "  10.873,\n",
       "  -32.612,\n",
       "  166.368,\n",
       "  236.707,\n",
       "  150.748,\n",
       "  -23.856,\n",
       "  672.295,\n",
       "  57.144,\n",
       "  -52.971,\n",
       "  -10.336,\n",
       "  59.514,\n",
       "  -3.944,\n",
       "  252.009,\n",
       "  -33.496,\n",
       "  -29.594,\n",
       "  17.443,\n",
       "  81.684,\n",
       "  235.111,\n",
       "  -63.611,\n",
       "  46.08,\n",
       "  -7.339,\n",
       "  1147.133,\n",
       "  -12.589,\n",
       "  46.694,\n",
       "  -2.278,\n",
       "  170.427,\n",
       "  -14.052,\n",
       "  84.586,\n",
       "  71.101,\n",
       "  -22.936,\n",
       "  168.752,\n",
       "  -33.572,\n",
       "  327.203,\n",
       "  -45.159,\n",
       "  -19.941,\n",
       "  -25.921,\n",
       "  80.99,\n",
       "  321.994,\n",
       "  101.561,\n",
       "  -4.206,\n",
       "  -2.664,\n",
       "  -13.461,\n",
       "  -3.344,\n",
       "  57.075,\n",
       "  114.964,\n",
       "  596.336,\n",
       "  4.996,\n",
       "  45.874,\n",
       "  20.162,\n",
       "  -78.55,\n",
       "  -18.013,\n",
       "  -17.661,\n",
       "  211.986,\n",
       "  21.346,\n",
       "  -50.582,\n",
       "  -10.299,\n",
       "  9.734,\n",
       "  -23.911,\n",
       "  107.711,\n",
       "  62.418,\n",
       "  -1.946,\n",
       "  47.747,\n",
       "  28.936,\n",
       "  -26.473,\n",
       "  -11.351,\n",
       "  212.403,\n",
       "  -18.386,\n",
       "  13.409,\n",
       "  -8.868,\n",
       "  52.499,\n",
       "  1.086,\n",
       "  282.355,\n",
       "  -2.371,\n",
       "  -9.401,\n",
       "  223.929,\n",
       "  -14.69,\n",
       "  -15.041,\n",
       "  -9.921,\n",
       "  -68.644,\n",
       "  16.884,\n",
       "  -8.24,\n",
       "  21.623,\n",
       "  61.769,\n",
       "  5.729,\n",
       "  24.589,\n",
       "  5.467,\n",
       "  -9.934,\n",
       "  61.335,\n",
       "  59.013,\n",
       "  153.994,\n",
       "  -37.342,\n",
       "  12.694,\n",
       "  13.355,\n",
       "  2.203,\n",
       "  714.91,\n",
       "  632.472,\n",
       "  459.652,\n",
       "  -31.061,\n",
       "  -67.626,\n",
       "  55.37,\n",
       "  43.703,\n",
       "  -42.338,\n",
       "  414.867,\n",
       "  -23.362,\n",
       "  -27.897,\n",
       "  -0.241,\n",
       "  67.361,\n",
       "  31.777,\n",
       "  25.133,\n",
       "  26.582,\n",
       "  -14.394,\n",
       "  -12.809,\n",
       "  175.212,\n",
       "  1158.553,\n",
       "  263.143,\n",
       "  126.468,\n",
       "  -6.296,\n",
       "  134.475,\n",
       "  28.23,\n",
       "  138.468,\n",
       "  -9.929,\n",
       "  27.958,\n",
       "  33.765,\n",
       "  16.701,\n",
       "  34.311,\n",
       "  1.731,\n",
       "  30.015,\n",
       "  53.894,\n",
       "  -37.266,\n",
       "  48.346,\n",
       "  369.237,\n",
       "  78.564,\n",
       "  10.087,\n",
       "  182.204,\n",
       "  0.325,\n",
       "  -19.655,\n",
       "  -59.801,\n",
       "  132.944,\n",
       "  13.054,\n",
       "  52.13,\n",
       "  -31.655,\n",
       "  12.748,\n",
       "  27.867,\n",
       "  483.944,\n",
       "  78.174,\n",
       "  91.316,\n",
       "  -42.854,\n",
       "  -1.642,\n",
       "  -17.105,\n",
       "  -16.841,\n",
       "  14.772,\n",
       "  110.712,\n",
       "  -8.182,\n",
       "  47.652,\n",
       "  -5.446,\n",
       "  126.191,\n",
       "  -25.615,\n",
       "  92.615,\n",
       "  37.984,\n",
       "  -11.187,\n",
       "  -36.263,\n",
       "  0.956,\n",
       "  -10.074,\n",
       "  -33.23,\n",
       "  93.434,\n",
       "  22.204,\n",
       "  357.561,\n",
       "  88.609,\n",
       "  138.64,\n",
       "  123.298,\n",
       "  179.929,\n",
       "  -6.561]]"
      ]
     },
     "execution_count": 513,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "35a7001c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 512])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "ehr = torch.randn(5,512)\n",
    "ehr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "a8200857",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 512])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cxr = torch.randn(5,512)\n",
    "cxr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "12b8037a",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_diff1 =  [0.1,0.2,0.1,0.1,0.1]\n",
    "time_diff2 =  [10.,50.,30.,70.,90.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "8b7f8337",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modified_info_nce_loss(feats_ehr, feats_img, time_diff, mode='train'):\n",
    "        # Calculate cosine similarity matrix\n",
    "        cos_sim = F.cosine_similarity(feats_img[:,None,:], feats_ehr[None,:,:], dim=-1)\n",
    "#         print(cos_sim)\n",
    "        cos_sim = cos_sim /  0.07\n",
    "        self_mask = torch.eye(cos_sim.shape[0], dtype=torch.bool,  device=cos_sim.device)\n",
    "        cos_sim_negative = torch.clone(cos_sim)\n",
    "        cos_sim_negative.masked_fill_(self_mask, -9e15)\n",
    "        \n",
    "        # Compute the values of beta\n",
    "        k = 1\n",
    "        time_diff = torch.FloatTensor(time_diff)\n",
    "#         print(time_diff.mean())\n",
    "#         time_diff = time_diff/ time_diff.mean()\n",
    "#         print(time_diff)\n",
    "        beta = torch.exp(-k*time_diff)#.to(self.device)\n",
    "        print(beta)\n",
    "        # Compute based on img->ehr\n",
    "        nll_1 = cos_sim[self_mask] - torch.logsumexp(cos_sim_negative, dim=1)\n",
    "#         print(nll_1)\n",
    "        nll_1 = (k-beta)*nll_1\n",
    "        \n",
    "        # Compute based on ehr->img\n",
    "        nll_2 = cos_sim[self_mask] - torch.logsumexp(cos_sim_negative, dim=0)\n",
    "        nll_2 = (k-beta)*nll_2\n",
    "        \n",
    "        # Total loss \n",
    "        loss = -(nll_1 + nll_2).mean()\n",
    "       \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "1ee15dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9048, 0.8187, 0.9048, 0.9048, 0.9048])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.3130)"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modified_info_nce_loss(ehr,cxr,time_diff1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "3717c862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4.5400e-05, 1.9287e-22, 9.3576e-14, 3.9754e-31, 8.1940e-40])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(2.4332)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modified_info_nce_loss(ehr,cxr,time_diff2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd4ef13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e2b415",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MML SSL",
   "language": "python",
   "name": "mml-ssl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
