{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7708be8-afb1-4de5-8eac-869c066085c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Pytorch \n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_dim=128, input_dim=76, num_classes=1, batch_first=True, dropout=0.0, layers=1):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layers = layers\n",
    "        for layer in range(layers):\n",
    "            setattr(self, f'layer{layer}', nn.LSTM(\n",
    "                input_dim, hidden_dim,\n",
    "                batch_first=batch_first,\n",
    "                dropout = dropout)\n",
    "            )\n",
    "            input_dim = hidden_dim\n",
    "\n",
    "        self.do = nn.Dropout(dropout)\n",
    "        self.feats_dim = hidden_dim\n",
    "        self.dense_layer = nn.Identity() #nn.Linear(hidden_dim, num_classes)\n",
    "        self.initialize_weights()\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        for model in self.modules():\n",
    "\n",
    "            if type(model) in [nn.Linear]:\n",
    "                nn.init.xavier_uniform_(model.weight)\n",
    "                nn.init.zeros_(model.bias)\n",
    "            elif type(model) in [nn.LSTM, nn.RNN, nn.GRU]:\n",
    "                nn.init.orthogonal_(model.weight_hh_l0)\n",
    "                nn.init.xavier_uniform_(model.weight_ih_l0)\n",
    "                nn.init.zeros_(model.bias_hh_l0)\n",
    "                nn.init.zeros_(model.bias_ih_l0)\n",
    "\n",
    "    def forward(self, x, seq_lengths):\n",
    "        x = torch.nn.utils.rnn.pack_padded_sequence(x, seq_lengths, batch_first=True, enforce_sorted=False)\n",
    "        for layer in range(self.layers):\n",
    "             x, (ht, _) = getattr(self, f'layer{layer}')(x)\n",
    "        feats = ht.squeeze()\n",
    "        out = self.do(feats)\n",
    "        out = self.dense_layer(out)\n",
    "#         scores = torch.sigmoid(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class CXRModels(nn.Module):\n",
    "\n",
    "    def __init__(self, args, hidden_dim, device='cpu'):\n",
    "        super(CXRModels, self).__init__()\n",
    "        self.args = args\n",
    "        self.device = device\n",
    "        self.vision_backbone = getattr(torchvision.models, self.args.vision_backbone)(pretrained=self.args.pretrained) #,\n",
    "                                                                                     #num_classes=4*hidden_dim)\n",
    "        #d_visual = self.vision_backbone.fc.in_features\n",
    "        # try changing to adaptive averge pooling \n",
    "        classifiers = [ 'classifier', 'fc']\n",
    "        for classifier in classifiers:\n",
    "            cls_layer = getattr(self.vision_backbone, classifier, None)\n",
    "            # print(cls_layer)\n",
    "            if cls_layer is None:\n",
    "                continue\n",
    "            d_visual = cls_layer.in_features\n",
    "            setattr(self.vision_backbone, classifier, nn.Identity(d_visual))\n",
    "            break\n",
    "            \n",
    "        self.bce_loss = torch.nn.BCELoss(size_average=True)\n",
    "        #self.classifier = nn.Sequential(nn.Linear(d_visual, self.args.vision_num_classes), nn.Sigmoid())\n",
    "        self.feats_dim = d_visual\n",
    "        # self.feats_dim = 2048\n",
    "        self.feats_dim = 512\n",
    "        \n",
    "\n",
    "    def forward(self, x, labels=None, n_crops=0, bs=16):\n",
    "        lossvalue_bce = torch.zeros(1).to(self.device)\n",
    "\n",
    "        visual_feats = self.vision_backbone(x)\n",
    "#         preds = self.vision_backbone.fc(visual_feats)\n",
    "#         if n_crops > 0:\n",
    "#             preds = preds.view(bs, n_crops, -1).mean(1)\n",
    "#         if labels is not None:\n",
    "#             lossvalue_bce = self.bce_loss(preds, labels)\n",
    "\n",
    "        return  visual_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7760d03b-e1f0-451a-be09-76c716205699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2022 Farah E. Shamout\n",
    "#\n",
    "# TODO: licsense\n",
    "# ==============================================================================\n",
    "\"\"\"This script defines the different fusion functions that can be used with SimCLR and baseline models.\"\"\"\n",
    "\n",
    "#TODO: move to models folder later\n",
    "\n",
    "import os\n",
    "\n",
    "# Import Pytorch \n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "\n",
    "# Pytorch flask to get LARS\n",
    "import flash\n",
    "from flash.core.optimizers import LARS\n",
    "\n",
    "## Performance metrics\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "import numpy as np\n",
    "\n",
    "# Other\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Custom\n",
    "import load_tasks as tasks\n",
    "\n",
    "\n",
    "class Fusion(nn.Module): \n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "        hidden_dim=args.hidden_dim\n",
    "        fusion_dim = {0:{},\n",
    "                      3:{}}\n",
    "\n",
    "        if 'ehr' not in args.fusion_type:\n",
    "            # Base model chest X-ray modality f(.)\n",
    "            self.cxr_model = CXRModels(args, args.hidden_dim)\n",
    "            # MLP for chest X-ray modality g(.)\n",
    "            w=args.width\n",
    "            print(self.cxr_model.vision_backbone.fc)\n",
    "            self.cxr_model_g = nn.Sequential(\n",
    "                self.cxr_model.vision_backbone.fc,  # Linear(ResNet output, 4*hidden_dim)\n",
    "                # shaza: changed linear layer input from 512 to 2048\n",
    "                nn.Linear(512, w*hidden_dim, bias=True),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Linear(w*hidden_dim, w*hidden_dim, bias=True),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Linear(w*hidden_dim, w*hidden_dim, bias=False)\n",
    "            )\n",
    "\n",
    "            fusion_dim[0]['cxr']= self.cxr_model.feats_dim\n",
    "            fusion_dim[3]['cxr']=w*hidden_dim\n",
    "\n",
    "        if 'cxr' not in args.fusion_type:\n",
    "            # Base model EHR f(.)\n",
    "            w=args.width\n",
    "            self.ehr_model = LSTM(hidden_dim=args.hidden_dim, input_dim=76, num_classes=w*args.hidden_dim, dropout=args.dropout, layers=args.layers)\n",
    "\n",
    "            # MLP for EHR modality g(.) \n",
    "            self.ehr_model_g = nn.Sequential(\n",
    "                self.ehr_model.dense_layer,  # this is identify in encoders.py\n",
    "                nn.Linear(128, w*hidden_dim, bias=True),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Linear(w*hidden_dim, w*hidden_dim, bias=True),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Linear(w*hidden_dim, w*hidden_dim, bias=False)\n",
    "            )\n",
    "\n",
    "            fusion_dim[0]['ehr']=self.ehr_model.feats_dim\n",
    "            fusion_dim[3]['ehr']=w*hidden_dim\n",
    "        \n",
    "        \n",
    "        # Single layer for linear evaluation of representations\n",
    "        if self.args.fusion_type == 'lineareval_ehr':\n",
    "            feats_dim = fusion_dim[args.fusion_layer]['ehr']\n",
    "            #feats_dim = self.ehr_model.feats_dim\n",
    "        \n",
    "        elif self.args.fusion_type == 'lineareval_cxr':\n",
    "            feats_dim = fusion_dim[args.fusion_layer]['cxr']\n",
    "            #feats_dim = self.cxr_model.feats_dim\n",
    "        \n",
    "        else:\n",
    "            feats_dim = fusion_dim[args.fusion_layer]['ehr'] + fusion_dim[args.fusion_layer]['cxr']\n",
    "            #feats_dim = self.ehr_model.feats_dim + self.cxr_model.feats_dim\n",
    "        \n",
    "        # print(self.args.fusion_type)\n",
    "        if self.args.fusion_type != 'None':\n",
    "            self.fused_cls = nn.Sequential(\n",
    "                nn.Linear(feats_dim, self.args.num_classes),\n",
    "                nn.Sigmoid()\n",
    "            )\n",
    "        \n",
    "    def forward(self, x=None, seq_lengths=None, img=None, pairs=None):\n",
    "        # New for SimCLR\n",
    "        if self.args.fusion_type == 'lineareval_ehr':\n",
    "            return self.forward_uni_eval_ehr(x, seq_lengths=seq_lengths)\n",
    "        elif self.args.fusion_type == 'lineareval_cxr':\n",
    "            return self.forward_uni_eval_cxr(img=img)\n",
    "        elif self.args.fusion_type in ['joint',  'early', 'late_avg', 'unified', 'lineareval']:\n",
    "            return self.forward_fused(x, seq_lengths=seq_lengths, img=img, pairs=pairs)\n",
    "        else:\n",
    "            return self.forward_simclr(x, seq_lengths=seq_lengths, img=img, pairs=pairs)\n",
    "        \n",
    "    def forward_simclr(self, x, seq_lengths, img, pairs=None):\n",
    "        if self.args.mode == 'eval':\n",
    "            feats_img_0 = self.cxr_model(img)\n",
    "            feats_img_3 = self.cxr_model_g(feats_img_0)\n",
    "            feats_ehr_0 = self.ehr_model(x, seq_lengths)\n",
    "            feats_ehr_3 = self.ehr_model_g(feats_ehr_0)\n",
    "            \n",
    "            return feats_ehr_0, feats_ehr_3, feats_img_0, feats_img_3\n",
    "            \n",
    "        else:\n",
    "            feats_img = self.cxr_model(img)\n",
    "            feats_img = self.cxr_model_g(feats_img)\n",
    "            feats_ehr = self.ehr_model(x, seq_lengths)\n",
    "            feats_ehr = self.ehr_model_g(feats_ehr)\n",
    "        \n",
    "            return feats_ehr, feats_img\n",
    "    \n",
    "    def forward_fused(self, x, seq_lengths=None, img=None, pairs=None ):\n",
    "        if ('lineareval' in self.args.fusion_type) & (not self.args.finetune):\n",
    "            ehr_feats = x\n",
    "            cxr_feats = img\n",
    "        else:\n",
    "            ehr_feats = self.ehr_model(x, seq_lengths) #ehr_preds , \n",
    "            cxr_feats = self.cxr_model(img) #cxr_preds, _ , \n",
    "#         projected = self.projection(cxr_feats)\n",
    "\n",
    "        feats = torch.cat([ehr_feats, cxr_feats], dim=1)\n",
    "        fused_preds = self.fused_cls(feats)\n",
    "\n",
    "        return {\n",
    "            'early': fused_preds, \n",
    "            'joint': fused_preds, \n",
    "            'lineareval': fused_preds,\n",
    "            'ehr_feats': ehr_feats,\n",
    "#             'cxr_feats': projected,\n",
    "            'unified': fused_preds\n",
    "            }\n",
    "    \n",
    "    def forward_uni_eval_cxr(self, img ):\n",
    "        if ('lineareval' in self.args.fusion_type) & (not self.args.finetune):\n",
    "            cxr_feats = img\n",
    "        else:\n",
    "            cxr_feats = self.cxr_model(img)\n",
    "        preds = self.fused_cls(cxr_feats)\n",
    "        return {\n",
    "            'lineareval_cxr': preds,\n",
    "            }\n",
    "    \n",
    "    def forward_uni_eval_ehr(self, x, seq_lengths=None):\n",
    "        if ('lineareval' in self.args.fusion_type) & (not self.args.finetune):\n",
    "            ehr_feats = x\n",
    "        else:\n",
    "            ehr_feats = self.ehr_model(x, seq_lengths)\n",
    "        preds = self.fused_cls(ehr_feats)\n",
    "        return {\n",
    "            'lineareval_ehr': preds,\n",
    "            }   \n",
    "                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5538696b-11b6-4aea-9a9e-015ca3cc0a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using [0] device...\n"
     ]
    }
   ],
   "source": [
    "# Import Pytorch \n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint, Callback, TQDMProgressBar\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import math\n",
    "\n",
    "# Import other useful libraries\n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import pickle\n",
    "from flash.core.optimizers import LARS\n",
    "import os\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Import custom libraries/functions\n",
    "import load_tasks as tasks\n",
    "\n",
    "gpu_num=os.environ['CUDA_VISIBLE_DEVICES']\n",
    "\n",
    "# Set cuda device\n",
    "if gpu_num=='0':\n",
    "    gpu=[0]\n",
    "elif gpu_num=='1':\n",
    "    gpu=[1]\n",
    "elif gpu_num=='2':\n",
    "    gpu=[2]\n",
    "elif gpu_num=='3':\n",
    "    gpu=[3]\n",
    "elif gpu_num=='4':\n",
    "    gpu=[4]\n",
    "elif gpu_num=='5':\n",
    "    gpu=[5]\n",
    "elif gpu_num=='6':\n",
    "    gpu=[6]\n",
    "elif gpu_num=='7':\n",
    "    gpu=[7]\n",
    "elif gpu_num=='8':\n",
    "    gpu=[8]\n",
    "else:\n",
    "    gpu=['None']\n",
    "print('Using {} device...'.format(gpu)) \n",
    "       \n",
    "class SimCLR(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, args, train_dl):\n",
    "        super().__init__()\n",
    "        assert args.temperature > 0.0, 'The temperature must be a positive float!'\n",
    "        self.warmup_epochs= 10 #int(0.05*max_epochs) (10 as in SimCLR)\n",
    "        self.automatic_optimization = False\n",
    "        \n",
    "        self.num_train_batches=len(train_dl)\n",
    "        self.batch_size=args.batch_size\n",
    "        hidden_dim=args.hidden_dim\n",
    "        self.args=args\n",
    "        self.LABEL_COLUMNS = tasks.load_labels(args.task)\n",
    "        self.task = args.task\n",
    "        \n",
    "        # Load the architecture based on args\n",
    "        self.model = Fusion(args)\n",
    "        self.load_weights()\n",
    "        self.freeze_weights()\n",
    "        \n",
    "        \n",
    "    def load_weights(self):\n",
    "        # loads both encoders for simclr\n",
    "        load_dir_simclr = self.args.save_dir\n",
    "\n",
    "        if self.args.load_state is not None: \n",
    "            # what is 'LC' and why this condition?\n",
    "            # if 'LC' not in self.args.load_state:\n",
    "            #     if 'mortality' in self.args.save_dir:\n",
    "            #         load_dir_simclr = load_dir_simclr.replace('mortality', 'phenotyping')\n",
    "                    \n",
    "            if 'epoch' in self.args.load_state:\n",
    "                model_dir='/'+self.args.load_state.split('_epoch')[0] + '/'\n",
    "                if self.args.tag == 'eval_epoch':\n",
    "                    checkpoint = torch.load(load_dir_simclr + model_dir + self.args.load_state+\".ckpt\", map_location=\"cpu\")\n",
    "                else:\n",
    "                    checkpoint = torch.load(load_dir_simclr + model_dir + self.args.load_state+\".ckpt\")    \n",
    "            else:\n",
    "                if self.args.tag == 'eval_epoch':\n",
    "                    checkpoint = torch.load(os.path.join(load_dir_simclr, self.args.load_state+\".ckpt\"), map_location=\"cpu\")\n",
    "                else:\n",
    "                    checkpoint = torch.load(os.path.join(load_dir_simclr, self.args.load_state+\".ckpt\"))\n",
    "            own_state = self.model.state_dict()\n",
    "            own_keys = list(own_state.keys())\n",
    "            checkpoint_keys = list(checkpoint['state_dict'].keys())\n",
    "            \n",
    "            print('Total number of checkpoint params = {}'.format(len(checkpoint_keys)))\n",
    "            print('Total number of current model params = {}'.format(len(own_keys)))\n",
    "\n",
    "            count = 0\n",
    "            changed = []\n",
    "            for name in own_keys:\n",
    "                if name not in checkpoint_keys:\n",
    "                    # print(name)\n",
    "                    # double check if name exists in a different format\n",
    "                    for x in checkpoint_keys:\n",
    "                        if name in x:\n",
    "                            param=checkpoint['state_dict'][x]\n",
    "                            if isinstance(param, torch.nn.Parameter):\n",
    "                                param=param.data\n",
    "                            own_state[name].copy_(param)\n",
    "                            count+=1\n",
    "                else:\n",
    "                    param=checkpoint['state_dict'][name]\n",
    "                    if isinstance(param, torch.nn.Parameter):\n",
    "                        param=param.data\n",
    "                    own_state[name].copy_(param)\n",
    "                    count+=1\n",
    "            print('Total number params loaded for model weights = {}'.format(count))\n",
    "        \n",
    "    def freeze_weights(self):\n",
    "        if self.args.finetune:\n",
    "            if 'ehr' not in self.args.fusion_type:\n",
    "                self.freeze(self.model.cxr_model_g)\n",
    "            if 'cxr' not in self.args.fusion_type:    \n",
    "                self.freeze(self.model.ehr_model_g)\n",
    "        else: \n",
    "            if 'lineareval' in self.args.fusion_type:\n",
    "                print('freezing encoders')\n",
    "                if 'ehr' not in self.args.fusion_type:\n",
    "                    self.freeze(self.model.cxr_model)\n",
    "                    self.freeze(self.model.cxr_model_g)\n",
    "                if 'cxr' not in self.args.fusion_type:\n",
    "                    self.freeze(self.model.ehr_model)\n",
    "                    self.freeze(self.model.ehr_model_g) \n",
    "        \n",
    "    def freeze(self, model):\n",
    "        for p in model.parameters():\n",
    "            p.requires_grad = False     \n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        \n",
    "        if self.args.fusion_type == 'None':\n",
    "            # Scaled learning rate in case of multiple GPUs\n",
    "            if self.args.num_gpu > 1:\n",
    "                effective_batchsize = self.args.batch_size*self.args.num_gpu\n",
    "                scaled_lr = self.args.lr*effective_batchsize/self.args.batch_size\n",
    "            else:\n",
    "                scaled_lr = self.args.lr \n",
    "                        \n",
    "            # Optimizer\n",
    "            optimizer = LARS(self.parameters(), lr=scaled_lr, momentum=0.9, weight_decay=self.args.weight_decay)\n",
    "            \n",
    "            # Note that the order of the below affects the initial starting learning rate, hence do not change.\n",
    "            # Main scheduler\n",
    "            mainscheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, 500, verbose=False)\n",
    "            # Learning rate warmup\n",
    "            lambda1= lambda epoch : (epoch+1)/self.warmup_epochs\n",
    "            warmupscheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lambda1, verbose=False)\n",
    "                         \n",
    "            return [optimizer], [mainscheduler, warmupscheduler]\n",
    "        \n",
    "        \n",
    "        else:\n",
    "            optimizer_adam = optim.AdamW(self.parameters(), lr=self.args.lr) #, weight_decay=self.args.weight_decay)\n",
    "            lr_scheduler_adam = optim.lr_scheduler.MultiStepLR(optimizer_adam,milestones=[int(self.args.epochs*0.6),\n",
    "                                                       int(self.args.epochs*0.8)],gamma=0.1)\n",
    "            return [optimizer_adam], [lr_scheduler_adam]\n",
    "                \n",
    "    def logging_status(self, mode):\n",
    "        if mode == 'train':\n",
    "            on_step=True\n",
    "            on_epoch=True\n",
    "        else:\n",
    "            on_step=False # Report for the sake of naming but it's not useful\n",
    "            on_epoch=True\n",
    "        return on_step, on_epoch\n",
    "    \n",
    "#     # TODO: Make this more efficient\n",
    "#     def accuracy_top_k(self, k, temp):\n",
    "#         temp = temp.argsort(dim=1, descending=True)[:, :k]\n",
    "#         batchsize=temp.shape[0]\n",
    "#         b_idx = np.arange(0,batchsize)\n",
    "#         tot=0\n",
    "#         for j in range(0, batchsize):\n",
    "#             tot+= b_idx[j] in temp[j]\n",
    "#         return tot*100/batchsize\n",
    "    \n",
    "    def bce_loss(self, preds, y, mode='train'):\n",
    "        \n",
    "        loss = nn.BCELoss()(preds, y)\n",
    "        \n",
    "        if torch.is_tensor(y):\n",
    "            y = y.detach().cpu().numpy()\n",
    "            \n",
    "        auroc = np.round(roc_auc_score(y, preds.detach().cpu()), 4)\n",
    "        auprc = np.round(average_precision_score(y, preds.detach().cpu()), 4)\n",
    "        \n",
    "        on_step=False\n",
    "        on_epoch=True\n",
    "        #self.log(mode + '_loss', loss, on_step=on_step, on_epoch=on_epoch, logger=True)\n",
    "        self.log(mode + '_auroc', auroc, on_step=on_step, on_epoch=on_epoch) #, logger=True)\n",
    "        self.log(mode + '_auprc', auprc, on_step=on_step, on_epoch=on_epoch) #, logger=True)\n",
    "        \n",
    "        return loss \n",
    "    \n",
    "    \n",
    "    def info_nce_loss(self, feats_ehr, feats_img, mode='train'):\n",
    "        # Calculate cosine similarity matrix\n",
    "        cos_sim = F.cosine_similarity(feats_img[:,None,:], feats_ehr[None,:,:], dim=-1)\n",
    "        #print(cos_sim.size())\n",
    "        cos_sim = cos_sim /  self.args.temperature\n",
    "        # double-check \n",
    "        self_mask = torch.eye(cos_sim.shape[0], dtype=torch.bool,  device=cos_sim.device)\n",
    "        #print(self_mask.size())\n",
    "        cos_sim_negative = torch.clone(cos_sim)\n",
    "        cos_sim_negative.masked_fill_(self_mask, -9e15)\n",
    "        \n",
    "        # Compute based on img->ehr\n",
    "        nll_1 = cos_sim[self_mask] - torch.logsumexp(cos_sim_negative, dim=1)\n",
    "        #temp_1=torch.reshape(cos_sim, (cos_sim.shape[0],cos_sim.shape[1]))\n",
    "        \n",
    "        # Compute based on ehr->img\n",
    "        nll_2 = cos_sim[self_mask] - torch.logsumexp(cos_sim_negative, dim=0) \n",
    "        #temp_2=torch.reshape(cos_sim_2, (cos_sim_2.shape[0],cos_sim_2.shape[1]))\n",
    "        \n",
    "        # Total loss \n",
    "        loss = -(nll_1 + nll_2).mean()\n",
    "            \n",
    "        # Logging ranking metrics\n",
    "        #self.log(mode+'_loss', loss, on_step=on_step, on_epoch=on_epoch, logger=True)\n",
    "        on_step, on_epoch = self.logging_status(mode)\n",
    "        #self.log(mode+'_acc_top1', self.accuracy_top_k(1, temp_1), on_step=on_step, on_epoch=on_epoch) #, logger=True)\n",
    "        #self.log(mode+'_acc_top5', self.accuracy_top_k(5, temp_1), on_step=on_step, on_epoch=on_epoch) #, logger=True)\n",
    "                     \n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    \n",
    "    def modified_info_nce_loss(self, feats_ehr, feats_img, time_diff, mode='train'):\n",
    "        # Calculate cosine similarity matrix\n",
    "        cos_sim = F.cosine_similarity(feats_img[:,None,:], feats_ehr[None,:,:], dim=-1)\n",
    "        cos_sim = cos_sim /  self.args.temperature\n",
    "        self_mask = torch.eye(cos_sim.shape[0], dtype=torch.bool,  device=cos_sim.device)\n",
    "        cos_sim_negative = torch.clone(cos_sim)\n",
    "        cos_sim_negative.masked_fill_(self_mask, -9e15)\n",
    "        \n",
    "        # Compute the values of beta\n",
    "        k = 1\n",
    "        time_diff = torch.FloatTensor(time_diff)\n",
    "        beta = torch.exp(-k*time_diff).to(self.device)\n",
    "        \n",
    "        # Compute based on img->ehr\n",
    "        nll_1 = cos_sim[self_mask] - torch.logsumexp(cos_sim_negative, dim=1)\n",
    "        nll_1 = beta*nll_1\n",
    "        \n",
    "        # Compute based on ehr->img\n",
    "        nll_2 = cos_sim[self_mask] - torch.logsumexp(cos_sim_negative, dim=0)\n",
    "        nll_2 = beta*nll_2\n",
    "        \n",
    "        # Total loss \n",
    "        loss = -(nll_1 + nll_2).mean()\n",
    "            \n",
    "        # Logging ranking metrics\n",
    "        #self.log(mode+'_loss', loss, on_step=on_step, on_epoch=on_epoch, logger=True)\n",
    "        on_step, on_epoch = self.logging_status(mode)\n",
    "       \n",
    "        return loss \n",
    "    \n",
    "    \n",
    "    def off_diagonal(self,x):\n",
    "        n, m = x.shape\n",
    "        assert n == m\n",
    "        return x.flatten()[:-1].view(n - 1, n + 1)[:, 1:].flatten()\n",
    "\n",
    "    \n",
    "    def vicreg_loss(self, feats_ehr, feats_img, mode='train'):\n",
    "        x = feats_ehr\n",
    "        y = feats_img\n",
    "        repr_loss = F.mse_loss(x, y)\n",
    "\n",
    "        #x = torch.cat(FullGatherLayer.apply(x), dim=0) #\n",
    "        #y = torch.cat(FullGatherLayer.apply(y), dim=0) #\n",
    "        \n",
    "        x = x - x.mean(dim=0)\n",
    "        y = y - y.mean(dim=0)\n",
    "\n",
    "        std_x = torch.sqrt(x.var(dim=0) + 0.0001)\n",
    "        std_y = torch.sqrt(y.var(dim=0) + 0.0001)\n",
    "        std_loss = torch.mean(F.relu(1 - std_x)) / 2 + torch.mean(F.relu(1 - std_y)) / 2\n",
    "                \n",
    "        cov_x = (x.T @ x) / (self.args.batch_size - 1)\n",
    "        cov_y = (y.T @ y) / (self.args.batch_size - 1)\n",
    "        \n",
    "        num_features = len(cov_x) #TODO as arg\n",
    "                \n",
    "        cov_loss = self.off_diagonal(cov_x).pow_(2).sum().div(num_features) + self.off_diagonal(cov_y).pow_(2).sum().div(num_features)\n",
    "\n",
    "        loss = (\n",
    "            self.args.sim_coeff * repr_loss\n",
    "            + self.args.std_coeff * std_loss\n",
    "            + self.args.cov_coeff * cov_loss\n",
    "        )\n",
    "        on_step, on_epoch = self.logging_status(mode)\n",
    "        \n",
    "        return loss\n",
    "        \n",
    "        \n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        opt = self.optimizers()\n",
    "        opt.zero_grad()\n",
    "        mode = 'train'\n",
    "        # Forward pass for SimCLR\n",
    "        if ((self.args.fusion_type=='None') & (self.args.beta_infonce == False) & (self.args.vicreg == False)):\n",
    "            ehr, imgs, y_ehr, y_cxr, seq_lengths, pairs = batch\n",
    "            print(\"ehr\", ehr.shape, \"seq_lengths\" , len(seq_lengths) , \"imgs\", imgs.shape)\n",
    "            ehr = torch.from_numpy(ehr).float()\n",
    "            ehr = ehr.to(self.device)\n",
    "            print(self.device)\n",
    "            feats_ehr, feats_img = self.model(ehr, seq_lengths, imgs)\n",
    "            # print(feats_ehr.shape, feats_img.shape)\n",
    "            # Compute and log infoNCE loss\n",
    "            loss = self.info_nce_loss(feats_ehr, feats_img, mode)\n",
    "            self.log(mode+'_loss', loss, on_step=True, on_epoch=True) #, logger=True)\n",
    "            \n",
    "            \n",
    "        elif ((self.args.fusion_type=='None') & (self.args.beta_infonce == True) & (self.args.vicreg == False)):\n",
    "            ehr, imgs, y_ehr, y_cxr, seq_lengths, pairs, time_diff = batch\n",
    "            ehr = torch.from_numpy(ehr).float()\n",
    "            ehr = ehr.to(self.device)\n",
    "            feats_ehr, feats_img = self.model(ehr, seq_lengths, imgs)\n",
    "            \n",
    "            # Compute and log infoNCE loss\n",
    "            loss = self.modified_info_nce_loss(feats_ehr, feats_img, time_diff, mode)\n",
    "            self.log(mode+'_loss', loss, on_step=True, on_epoch=True) #, logger=True)\n",
    "            \n",
    "        elif ((self.args.fusion_type=='None') & (self.args.beta_infonce == False) & (self.args.vicreg == True)):\n",
    "            ehr, imgs, y_ehr, y_cxr, seq_lengths, pairs = batch\n",
    "            ehr = torch.from_numpy(ehr).float()\n",
    "            ehr = ehr.to(self.device)\n",
    "            feats_ehr, feats_img = self.model(ehr, seq_lengths, imgs)\n",
    "            \n",
    "            # Compute and log vicreg loss\n",
    "            loss = self.vicreg_loss(feats_ehr, feats_img, mode)\n",
    "            self.log(mode+'_loss', loss, on_step=True, on_epoch=True) #, logger=True)\n",
    "            \n",
    "                        \n",
    "        else:\n",
    "            \n",
    "            if self.args.finetune:\n",
    "                ehr, imgs, y_ehr, y_cxr, seq_lengths, pairs = batch\n",
    "\n",
    "                ehr = torch.from_numpy(ehr).float()\n",
    "                ehr = ehr.to(self.device)\n",
    "                imgs = imgs.to(self.device)\n",
    "                output = self.model(x=ehr, seq_lengths=seq_lengths, img=imgs)\n",
    "                y_ehr = torch.from_numpy(y_ehr)\n",
    "        \n",
    "            else: # Features are already processed for linear classifier\n",
    "                seq_lengths=None\n",
    "                if 'ehr' in self.args.fusion_type:\n",
    "                    ehr, y_ehr = batch\n",
    "                    ehr = ehr.to(self.device)\n",
    "                    output = self.model(x=ehr,seq_lengths=seq_lengths)\n",
    "                elif 'cxr' in self.args.fusion_type:\n",
    "                    imgs, y_cxr, y_ehr = batch\n",
    "                    imgs = imgs.to(self.device)\n",
    "                    output = self.model(img=imgs)\n",
    "                else:\n",
    "                    ehr, imgs, y_ehr, y_cxr = batch\n",
    "                    ehr = ehr.to(self.device)\n",
    "                    imgs = imgs.to(self.device)\n",
    "                    output = self.model(x=ehr, seq_lengths=seq_lengths, img=imgs)\n",
    "\n",
    "\n",
    "            y = y_ehr.float()\n",
    "            y = y.to(self.device)\n",
    "\n",
    "            preds = output[self.args.fusion_type].squeeze()\n",
    "            # print(preds)\n",
    "\n",
    "            # Compute and log BCE loss\n",
    "            loss = self.bce_loss(preds, y, mode)\n",
    "            self.log(mode+'_loss', loss, on_step=True, on_epoch=True) #, logger=True)\n",
    "    \n",
    "        # import pdb; pdb.set_trace()\n",
    "        # print(\"loss\" , loss.shape)\n",
    "        # Backpropagate\n",
    "        self.manual_backward(loss)\n",
    "        # Optimizer step\n",
    "        opt.step()\n",
    "        # Learning rate step\n",
    "        if self.args.fusion_type=='None':\n",
    "            mainscheduler, warmupscheduler = self.lr_schedulers()\n",
    "            if (self.trainer.is_last_batch) and (self.trainer.current_epoch < self.warmup_epochs-1):\n",
    "                warmupscheduler.step()\n",
    "            elif (self.trainer.is_last_batch) and (self.trainer.current_epoch >= self.warmup_epochs-1):\n",
    "                mainscheduler.step()\n",
    "                \n",
    "#             if (batch_idx==self.num_train_batches-1) & (self.trainer.current_epoch < self.warmup_epochs-1):\n",
    "#                 warmupscheduler.step()\n",
    "#             elif (batch_idx==self.num_train_batches-1) & (self.trainer.current_epoch >= self.warmup_epochs-1):\n",
    "#                 mainscheduler.step()\n",
    "            \n",
    "\n",
    "            return {'loss': loss, 'feats_ehr': feats_ehr.detach().cpu(), 'feats_img': feats_img.detach().cpu(), 'y_ehr':y_ehr}\n",
    "        else:\n",
    "            return {'loss': loss}\n",
    "\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        mode='val'\n",
    "        # Forward pass for SimCLR\n",
    "        if ((self.args.fusion_type=='None') & (self.args.beta_infonce == False)):\n",
    "            ehr, imgs, y_ehr, y_cxr, seq_lengths, pairs = batch\n",
    "            ehr = torch.from_numpy(ehr).float()\n",
    "            ehr = ehr.to(self.device)\n",
    "            feats_ehr, feats_img = self.model(ehr, seq_lengths, imgs) \n",
    "            \n",
    "            # Compute and log infoNCE loss\n",
    "            loss = self.info_nce_loss(feats_ehr, feats_img, mode)\n",
    "            self.log(mode+'_loss_epoch', loss, on_step=False, on_epoch=True) #, logger=True)\n",
    "            \n",
    "            return {'loss': loss, 'feats_ehr': feats_ehr.detach().cpu(), 'feats_img': feats_img.detach().cpu(), 'y_ehr':y_ehr}\n",
    "        \n",
    "        elif ((self.args.fusion_type=='None') & (self.args.beta_infonce == True)):\n",
    "            ehr, imgs, y_ehr, y_cxr, seq_lengths, pairs, time_diff = batch\n",
    "            ehr = torch.from_numpy(ehr).float()\n",
    "            ehr = ehr.to(self.device)\n",
    "            feats_ehr, feats_img = self.model(ehr, seq_lengths, imgs)\n",
    "            \n",
    "            # Compute and log infoNCE loss\n",
    "            loss = self.modified_info_nce_loss(feats_ehr, feats_img, time_diff, mode)\n",
    "            self.log(mode+'_loss', loss, on_step=False, on_epoch=True) #, logger=True)\n",
    "            return {'loss': loss, 'feats_ehr': feats_ehr.detach().cpu(), 'feats_img': feats_img.detach().cpu(), 'y_ehr':y_ehr}\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            if self.args.finetune:\n",
    "                ehr, imgs, y_ehr, y_cxr, seq_lengths, pairs = batch\n",
    "\n",
    "                ehr = torch.from_numpy(ehr).float()\n",
    "                ehr = ehr.to(self.device)\n",
    "                imgs = imgs.to(self.device)\n",
    "                output = self.model(x=ehr, seq_lengths=seq_lengths, img=imgs)\n",
    "                y_ehr = torch.from_numpy(y_ehr)\n",
    "        \n",
    "            else: # Features are already processed for linear classifier\n",
    "                seq_lengths=None\n",
    "                if 'ehr' in self.args.fusion_type:\n",
    "                    ehr, y_ehr = batch\n",
    "                    ehr = ehr.to(self.device)\n",
    "                    output = self.model(x=ehr,seq_lengths=seq_lengths)\n",
    "                elif 'cxr' in self.args.fusion_type:\n",
    "                    imgs, y_cxr, y_ehr = batch\n",
    "                    imgs = imgs.to(self.device)\n",
    "                    output = self.model(img=imgs)\n",
    "                else:\n",
    "                    ehr, imgs, y_ehr, y_cxr = batch\n",
    "                    ehr = ehr.to(self.device)\n",
    "                    imgs = imgs.to(self.device)\n",
    "                    output = self.model(x=ehr, seq_lengths=seq_lengths, img=imgs)\n",
    "\n",
    "\n",
    "            y = y_ehr.float()\n",
    "            y = y.to(self.device)\n",
    "\n",
    "            preds = output[self.args.fusion_type].squeeze()\n",
    "            # print(preds)\n",
    "            # Compute and log BCE loss\n",
    "            loss = self.bce_loss(preds, y, mode)\n",
    "            # Compute and log BCE loss\n",
    "            #loss = self.bce_loss(batch, mode='val')\n",
    "            self.log(mode+'_loss_epoch', loss, on_step=False, on_epoch=True) #, logger=True)\n",
    "            \n",
    "            return {'loss': loss}\n",
    "        \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        mode='test'\n",
    "        \n",
    "        \n",
    "        # Forward pass for SimCLR\n",
    "        if ((self.args.fusion_type=='None') & (self.args.beta_infonce == False)):\n",
    "            if self.args.beta_infonce == True:\n",
    "                ehr, imgs, y_ehr, y_cxr, seq_lengths, pairs, time_diff = batch\n",
    "            else:\n",
    "                ehr, imgs, y_ehr, y_cxr, seq_lengths, pairs = batch\n",
    "            ehr = torch.from_numpy(ehr).float()\n",
    "            ehr = ehr.to(self.device)\n",
    "            \n",
    "            # At test time of SIMCLR, always return all the layer features\n",
    "            if self.args.mode == 'eval':\n",
    "                feats_ehr_0, feats_ehr_3, feats_img_0, feats_img_3 = self.model(ehr, seq_lengths, imgs) \n",
    "            \n",
    "                # Compute and log infoNCE loss\n",
    "                if self.args.beta_infonce == True:\n",
    "                    loss = self.modified_info_nce_loss(feats_ehr_3, feats_img_3, time_diff, mode)\n",
    "                else:\n",
    "                    loss = self.info_nce_loss(feats_ehr_3, feats_img_3, mode)\n",
    "                self.log(mode+'_loss_epoch', loss, on_step=False, on_epoch=True) #, logger=True)\n",
    "            \n",
    "                return {'loss': loss,   'feats_ehr_0': feats_ehr_0.detach().cpu(), \n",
    "                                        'feats_ehr_3': feats_ehr_3.detach().cpu(), \n",
    "                                        'feats_img_0': feats_img_0.detach().cpu(), \n",
    "                                        'feats_img_3': feats_img_3.detach().cpu(), \n",
    "                                        'y_ehr':y_ehr}        \n",
    "        \n",
    "        else:\n",
    "            if self.args.finetune:\n",
    "                ehr, imgs, y_ehr, y_cxr, seq_lengths, pairs = batch\n",
    "\n",
    "                ehr = torch.from_numpy(ehr).float()\n",
    "                ehr = ehr.to(self.device)\n",
    "                imgs = imgs.to(self.device)\n",
    "                output = self.model(x=ehr, seq_lengths=seq_lengths, img=imgs)\n",
    "                y_ehr = torch.from_numpy(y_ehr)\n",
    "        \n",
    "            else: # Features are already processed for linear classifier\n",
    "                seq_lengths=None\n",
    "                if 'ehr' in self.args.fusion_type:\n",
    "                    ehr, y_ehr = batch\n",
    "                    ehr = ehr.to(self.device)\n",
    "                    output = self.model(x=ehr,seq_lengths=seq_lengths)\n",
    "                elif 'cxr' in self.args.fusion_type:\n",
    "                    imgs, y_cxr, y_ehr = batch\n",
    "                    imgs = imgs.to(self.device)\n",
    "                    output = self.model(img=imgs)\n",
    "                else:\n",
    "                    ehr, imgs, y_ehr, y_cxr = batch\n",
    "                    ehr = ehr.to(self.device)\n",
    "                    imgs = imgs.to(self.device)\n",
    "                    output = self.model(x=ehr, seq_lengths=seq_lengths, img=imgs)\n",
    "\n",
    "\n",
    "            y = y_ehr.float()\n",
    "            y = y.to(self.device)\n",
    "\n",
    "            preds = output[self.args.fusion_type].squeeze()\n",
    "            \n",
    "            # print(y.shape, preds.shape)\n",
    "            # Compute and log BCE loss\n",
    "            loss = self.bce_loss(preds, y, mode)\n",
    "            #loss = self.bce_loss(batch, mode=mode)\n",
    "            return {'loss': loss, 'preds': preds, 'y_ehr': y}\n",
    "                \n",
    "    \n",
    "    def process_features(self, outputs, mode):\n",
    "        y = []\n",
    "        if self.args.mode=='eval':\n",
    "            feats_ehr_0=[]\n",
    "            feats_ehr_3=[]\n",
    "            feats_img_0=[]\n",
    "            feats_img_3=[]\n",
    "        elif mode == 'test':\n",
    "            preds = []\n",
    "        else:\n",
    "            feats_ehr = []\n",
    "            feats_img = []\n",
    "        # Iterate through batches and append\n",
    "        i=0\n",
    "        for output in outputs:\n",
    "            if i ==0:\n",
    "                if self.args.mode == 'eval':\n",
    "                    feats_ehr_0 = output['feats_ehr_0'].detach().cpu()\n",
    "                    feats_ehr_3 = output['feats_ehr_3'].detach().cpu()\n",
    "                    feats_img_0 = output['feats_img_0'].detach().cpu()\n",
    "                    feats_img_3 = output['feats_img_3'].detach().cpu()\n",
    "                elif mode == 'test':\n",
    "                    preds = output['preds'].detach().cpu()\n",
    "                else: \n",
    "                    feats_ehr = output['feats_ehr'].detach().cpu()\n",
    "                    feats_img = output['feats_img'].detach().cpu()\n",
    "                y = output['y_ehr'].tolist()\n",
    "                \n",
    "            else:\n",
    "                if self.args.mode == 'eval':\n",
    "                    feats_ehr_0 = torch.cat((feats_ehr_0, output['feats_ehr_0'].detach().cpu()))\n",
    "                    feats_ehr_3 = torch.cat((feats_ehr_3, output['feats_ehr_3'].detach().cpu()))\n",
    "                    feats_img_0 = torch.cat((feats_img_0, output['feats_img_0'].detach().cpu()))\n",
    "                    feats_img_3 = torch.cat((feats_img_3, output['feats_img_3'].detach().cpu()))\n",
    "                elif mode == 'test':\n",
    "                    preds = torch.cat((preds, output['preds'].detach().cpu()))\n",
    "                else:\n",
    "                    feats_ehr = torch.cat((feats_ehr, output['feats_ehr'].detach().cpu()))\n",
    "                    feats_img = torch.cat((feats_img, output['feats_img'].detach().cpu()))\n",
    "                y.extend(output['y_ehr'].tolist())\n",
    "            i+=1\n",
    "        if self.args.mode=='eval':\n",
    "            return feats_ehr_0, feats_ehr_3, feats_img_0, feats_img_3, y\n",
    "        elif mode =='test':\n",
    "            return y, preds\n",
    "        else:\n",
    "            return feats_ehr, feats_img, y\n",
    "    \n",
    "    def save_features(self, x, descrip, mode):\n",
    "        model_path = self.args.save_dir+'/simclr_lr/'+self.args.file_name\n",
    "        if not os.path.exists(model_path):\n",
    "          os.makedirs(model_path)\n",
    "        \n",
    "        torch.save(x, model_path+'/{}_{}_epoch_{}.pt'.format(mode, descrip, self.current_epoch))\n",
    "    \n",
    "    def training_epoch_end(self, outputs):\n",
    "        mode='train'\n",
    "        if ((self.args.fusion_type=='None') & (self.args.save_features == True)):\n",
    "            feats_ehr, feats_img, y = self.process_features(outputs, mode)\n",
    "            self.save_features(feats_ehr, 'feats_ehr', mode)\n",
    "            self.save_features(feats_img, 'feats_img', mode)      \n",
    "            self.save_features(y, 'y', mode)   \n",
    "        \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        mode='val'\n",
    "        if ((self.args.fusion_type=='None') & (self.args.save_features == True)):\n",
    "            feats_ehr, feats_img, y = self.process_features(outputs, mode)\n",
    "            self.save_features(feats_ehr, 'feats_ehr', mode)\n",
    "            self.save_features(feats_img, 'feats_img', mode)      \n",
    "            self.save_features(y, 'y', mode)\n",
    "            \n",
    "\n",
    "    def test_epoch_end(self, outputs):\n",
    "        if ((self.args.fusion_type=='None') & (self.args.save_features == True)):\n",
    "            mode = self.args.eval_set\n",
    "            feats_ehr_0, feats_ehr_3, feats_img_0, feats_img_3, y = self.process_features(outputs, mode)\n",
    "            self.save_features(feats_ehr_0, 'feats_ehr_0', mode)\n",
    "            self.save_features(feats_ehr_3, 'feats_ehr_3', mode)\n",
    "            self.save_features(feats_img_0, 'feats_img_0', mode)\n",
    "            self.save_features(feats_img_3, 'feats_img_3', mode)      \n",
    "            self.save_features(y, 'y', mode)\n",
    "        else:\n",
    "            if self.task =='phenotyping':\n",
    "                mode = 'test'\n",
    "                y, preds = self.process_features(outputs, mode)\n",
    "\n",
    "                auroc_per_label = np.round(roc_auc_score(y, preds, average=None), 4)\n",
    "                auprc_per_label = np.round(average_precision_score(y, preds, average=None), 4)\n",
    "\n",
    "\n",
    "                auroc_label={}\n",
    "                auprc_label={}\n",
    "                for i, name in enumerate(self.LABEL_COLUMNS):\n",
    "                    auroc_label[name]=auroc_per_label[i].item()\n",
    "                    auprc_label[name]=auprc_per_label[i].item()\n",
    "                    #print(name, auroc_per_label[i], auprc_per_label[i])\n",
    "\n",
    "                self.log('auroc_label', auroc_label)\n",
    "                self.log('auprc_label', auprc_label)\n",
    "            \n",
    "    def calculate_auroc_epoch(self, outputs, mode):\n",
    "        labels = []\n",
    "        predictions = []\n",
    "        auroc_label={}\n",
    "        outputs=outputs[self.args.fusion_type].squeeze()\n",
    "        \n",
    "        for output in outputs:\n",
    "            for out_labels in output[\"labels\"].detach().cpu():\n",
    "                labels.append(out_labels)\n",
    "            for out_predictions in output[\"predictions\"].detach().cpu():\n",
    "                predictions.append(out_predictions)\n",
    "\n",
    "        labels = torch.stack(labels).int()\n",
    "        predictions = torch.stack(predictions)\n",
    "        for i, name in enumerate(self.LABEL_COLUMNS):\n",
    "            class_roc_auc = roc_auc_score(labels[:, i], predictions[:, i])\n",
    "            auroc_label[name]=class_roc_auc\n",
    "            \n",
    "        auroc = roc_auc_score(labels, predictions)\n",
    "        auprc = average_precision_score(labels, predictions)\n",
    "        \n",
    "        return auroc, auprc\n",
    "       \n",
    "    \n",
    "def return_model_version(trainer):\n",
    "    filename = trainer.checkpoint_callback.filename\n",
    "    best_model_path = trainer.checkpoint_callback.best_model_path\n",
    "    return  filename+best_model_path.split('.ckpt')[0].split(filename)[1]\n",
    "\n",
    "\n",
    "\n",
    "def train(model, args, train_loader, val_loader, **kwargs): \n",
    "    filename = args.file_name+'_epoch_{epoch:02d}'\n",
    "    \n",
    "    model_path = args.save_dir+'/'+args.file_name\n",
    "    if not os.path.exists(model_path):\n",
    "        os.makedirs(model_path)\n",
    "    \n",
    "    # For model selection\n",
    "    if 'logger' not in kwargs:\n",
    "        checkpoint_callback = ModelCheckpoint(monitor=\"val_auroc\", mode='max', \n",
    "                                              filename=args.load_state+'_{epoch:02d}',\n",
    "                                              every_n_epochs=1,             \n",
    "                                              save_on_train_epoch_end=True) \n",
    "        \n",
    "        trainer = pl.Trainer(default_root_dir=os.path.join(model_path),\n",
    "                         accelerator=\"auto\",\n",
    "                         max_epochs=args.epochs, gpus=gpu,\n",
    "                         callbacks=[checkpoint_callback],\n",
    "                         enable_progress_bar=False,\n",
    "                         num_sanity_val_steps=0)\n",
    "                         \n",
    "        \n",
    "    # For SIMCLR training\n",
    "    else:\n",
    "        logger = kwargs['logger']\n",
    "        checkpoints = ModelCheckpoint(dirpath=model_path,\n",
    "                                  filename=filename,\n",
    "                                  save_weights_only=True, \n",
    "                                  save_top_k=-1,\n",
    "                                  auto_insert_metric_name=False, \n",
    "                                  every_n_epochs=1,             \n",
    "                                  save_on_train_epoch_end=True)\n",
    "        if args.num_gpu == 1:\n",
    "            strategy = None\n",
    "        else:\n",
    "            strategy = 'ddp'\n",
    "        \n",
    "        trainer = pl.Trainer(default_root_dir=os.path.join(model_path),\n",
    "                             max_epochs=args.epochs, #gpus=gpu,\n",
    "                             callbacks=[checkpoints, LearningRateMonitor('epoch')],\n",
    "                             logger=logger,  \n",
    "                             log_every_n_steps=5,  enable_progress_bar=True,\n",
    "                             num_sanity_val_steps=0,\n",
    "                            accelerator='gpu', devices=args.num_gpu, strategy=strategy)\n",
    "\n",
    "   \n",
    "    num_batches_train = len(train_loader)   \n",
    "    # import pdb; pdb.set_trace()\n",
    "    trainer.fit(model, train_loader, val_loader)\n",
    "        \n",
    "    return trainer\n",
    "    \n",
    "\n",
    "# Call this function if doing test without training\n",
    "def test(model, args, test_loader, **kwargs):\n",
    "    if 'logger' not in kwargs:\n",
    "        trainer = pl.Trainer(default_root_dir=os.path.join(args.save_dir))\n",
    "        \n",
    "    else:\n",
    "        logger = kwargs['logger']\n",
    "        trainer = pl.Trainer(default_root_dir=os.path.join(args.save_dir), logger=logger)\n",
    "    \n",
    "    trainer.test(model, test_loader)\n",
    "    \n",
    "    return trainer\n",
    "\n",
    "    \n",
    "# Prepare data features for downstream tasks \n",
    "@torch.no_grad()\n",
    "def prepare_data_features(device, model, data_loader, bs, fusion_layer, fusion_type):\n",
    "    #print(fusion_layer)\n",
    "    # Prepare model\n",
    "    network = deepcopy(model)\n",
    "    if 'ehr' not in fusion_type:\n",
    "        network.model.cxr_model.vision_backbone.fc = nn.Identity() # Removing projection head g(.) \n",
    "     \n",
    "    if 'cxr' not in fusion_type:\n",
    "        network.model.ehr_model.dense_layer = nn.Identity() # Removing projection head g(.)\n",
    "    \n",
    "    network.eval()\n",
    "    network.to(device)\n",
    "\n",
    "    # Encode all images\n",
    "    feats_ehr, feats_imgs, labels_ehr, labels_imgs = [], [], [], []\n",
    "    \n",
    "    for batch_ehr, batch_imgs, batch_ehr_labels, batch_cxr_labels, seq_lengths, pairs in data_loader:\n",
    "        labels_ehr.append(torch.from_numpy(batch_ehr_labels).detach())\n",
    "        #time_diff.append(torch.from_numpy(np.array(batch_time)).detach())\n",
    "        \n",
    "        if 'cxr' not in fusion_type:\n",
    "            batch_ehr = torch.from_numpy(batch_ehr).float().to(device)\n",
    "            #batch_ehr = batch_ehr.to(device)\n",
    "            batch_ehr_feats = network.model.ehr_model(batch_ehr, seq_lengths)\n",
    "            if fusion_layer == 3:\n",
    "                batch_ehr_feats = network.model.ehr_model_g(batch_ehr_feats)\n",
    "                \n",
    "            #print('ehr batch shape', np.shape(batch_ehr_feats))\n",
    "            #batch_ehr_feats=torch.reshape(batch_ehr_feats, (1, np.shape(batch_ehr_feats)[0])) #TODO need this for other code\n",
    "            feats_ehr.append(batch_ehr_feats.detach().cpu()) \n",
    "\n",
    "        if 'ehr' not in fusion_type:\n",
    "            batch_imgs = batch_imgs.to(device)\n",
    "            batch_imgs_feats = network.model.cxr_model(batch_imgs)\n",
    "            if fusion_layer == 3:\n",
    "                batch_imgs_feats = network.model.cxr_model_g(batch_imgs_feats)\n",
    "                \n",
    "            #print('cxr batch shape', np.shape(batch_imgs_feats))\n",
    "            feats_imgs.append(batch_imgs_feats.detach().cpu())\n",
    "            labels_imgs.append(batch_cxr_labels)\n",
    "    \n",
    "    labels_ehr = torch.cat(labels_ehr, dim=0)\n",
    "    #time_diff = torch.cat(time_diff, dim=0)\n",
    "    \n",
    "#     print('shape ehr', np.shape(feats_ehr))\n",
    "#     print('shape imgs', np.shape(feats_imgs))\n",
    "    \n",
    "#     print('type ehr', type(feats_ehr))\n",
    "#     print('type cxr', type(feats_imgs))\n",
    "    \n",
    "#     print(type(feats_ehr[0]))\n",
    "#     print(type(feats_ehr[0][0]))\n",
    "#     print(feats_ehr[0][0])\n",
    "    \n",
    "#     print(type(feats_imgs[0]))\n",
    "#     print(type(feats_imgs[0][0]))\n",
    "#     print(feats_imgs[0][0])\n",
    "    \n",
    "    if 'cxr' not in fusion_type:\n",
    "        #if len(feats_ehr) == len(labels_ehr):\n",
    "        #    feats_ehr=torch.as_tensor(feats_ehr)\n",
    "        feats_ehr = torch.cat(feats_ehr, dim=0)\n",
    "        \n",
    "\n",
    "    if 'ehr' not in fusion_type:\n",
    "        feats_imgs = torch.cat(feats_imgs, dim=0)\n",
    "        labels_imgs = torch.cat(labels_imgs, dim=0)\n",
    "\n",
    "    if 'cxr' in fusion_type:\n",
    "        return data.DataLoader(data.TensorDataset(feats_imgs, labels_imgs, labels_ehr), batch_size=bs, shuffle=False, drop_last=False)\n",
    "    elif 'ehr' in fusion_type:\n",
    "        return data.DataLoader(data.TensorDataset(feats_ehr, labels_ehr), batch_size=bs, shuffle=False, drop_last=False)\n",
    "    else:\n",
    "        return data.DataLoader(data.TensorDataset(feats_ehr, feats_imgs, labels_ehr, labels_imgs), batch_size=bs, shuffle=False, drop_last=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f82c7a25-9703-4c80-8bac-86f95262970b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load run_gpu.py\n",
    "# Copyright 2022 Farah E. Shamout\n",
    "#\n",
    "# TODO: licsense\n",
    "# ==============================================================================\n",
    "\"\"\"This script defines the SimCLR model and performs training and evaluation.\"\"\"\n",
    "\n",
    "\n",
    "data_dir = '/scratch/fs999/shamoutlab/data/mimic-iv-extracted/'\n",
    "img_dir = '/scratch/fs999/shamoutlab/data/physionet.org/files//mimic-cxr-jpg/2.0.0'\n",
    "code_dir = '/scratch/se1525/mml-ssl'\n",
    "task = 'phenotyping'\n",
    "\n",
    "# Import libraries\n",
    "import sys\n",
    "#sys.path.append(f'{code_dir_medfuse}')\n",
    "import numpy as np\n",
    "import argparse\n",
    "import os\n",
    "import importlib as imp\n",
    "import re\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import neptune.new as neptune\n",
    "from pathlib import Path\n",
    "\n",
    "# ## Visualization\n",
    "# import matplotlib.pyplot as plt\n",
    "# from matplotlib.pyplot import figure\n",
    "# from tqdm.notebook import tqdm\n",
    "# import matplotlib\n",
    "# matplotlib.use('Agg')\n",
    "\n",
    "# Import Pytorch \n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "\n",
    "\n",
    "## Performance metrics\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "# Import custom functions\n",
    "import custom_parser as par\n",
    "import data_utils as prep\n",
    "# from fusion_trainer_farah import FusionTrainer\n",
    "# from mmtm_trainer import MMTMTrainer\n",
    "# from daft_trainer import DAFTTrainer\n",
    "\n",
    "\n",
    "# Import functions from MedFuse\n",
    "import datasets.ehr_dataset \n",
    "from datasets.ehr_dataset import get_datasets\n",
    "from datasets.ehr_dataset import EHRdataset\n",
    "from datasets.cxr_dataset import get_cxr_datasets\n",
    "import datasets.fusion\n",
    "from datasets.fusion import load_cxr_ehr\n",
    "from ehr_preprocess import ehr_funcs\n",
    "import load_tasks as tasks\n",
    "\n",
    "#sys.path.append('/home/shamoutlab/.local/bin')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import importlib\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf7bcaaf-14fc-4c08-8d1e-341b989cd29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ca9519a-667b-493b-8d85-5eda196e6431",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initiate_logger(tags):\n",
    "    logger = pl_loggers.NeptuneLogger(project=\"shaza-workspace/mml-ssl\",\n",
    "    api_key=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiI4NDU3ZDlmMi01OGEyLTQzMTAtODJmYS01Mjc5N2U4ZjgyMTAifQ==\", tags=tags, log_model_checkpoints=False)\n",
    "    return logger\n",
    "\n",
    "seed = 1002\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28ba8847-547f-4c98-8bfc-510359417f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse arguments\n",
    "parser = par.initiate_parsing()\n",
    "args = parser.parse_args([ '--device' , '$CUDA_VISIBLE_DEVICES',\n",
    "'--vision-backbone', 'resnet34' ,\n",
    "'--resize', '256' , \n",
    "'--job_number' , '${SLURM_JOBID}',\n",
    "'--file_name' , 'SIMCLR-${SLURM_JOBID}' ,\n",
    "'--epochs' , '2' , '--transforms_cxr' , 'simclrv2' , '--temperature' , '0.01' ,\n",
    "'--batch_size' , '30' , '--lr' , '0.8' ,\n",
    "'--num_gpu' , '1' ,\n",
    "'--pretrain_type' , 'simclr' ,\n",
    "'--mode' , 'train' ,\n",
    "'--fusion_type' , 'None' ,\n",
    "'--save_dir' , '/scratch/se1525/mml-ssl/checkpoints/phenotyping/models' ,\n",
    "'--tag' , 'simclr_train'])\n",
    "job_number = args.job_number\n",
    "\n",
    "path = Path(args.save_dir)\n",
    "path.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76d7783a-1b73-4345-991d-9782a38d174c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using None device...\n"
     ]
    }
   ],
   "source": [
    "# Set cuda device\n",
    "if args.device=='0':\n",
    "    device = 'cuda:0' if torch.cuda.is_available() else 'cpu'  \n",
    "elif args.device=='1':\n",
    "    device = 'cuda:1' if torch.cuda.is_available() else 'cpu'   \n",
    "elif args.device=='2':\n",
    "    device = 'cuda:2' if torch.cuda.is_available() else 'cpu'\n",
    "elif args.device=='3':\n",
    "    device = 'cuda:3' if torch.cuda.is_available() else 'cpu'\n",
    "elif args.device=='4':\n",
    "    device = 'cuda:4' if torch.cuda.is_available() else 'cpu'\n",
    "elif args.device=='5':\n",
    "    device = 'cuda:5' if torch.cuda.is_available() else 'cpu'\n",
    "elif args.device=='6':\n",
    "    device = 'cuda:6' if torch.cuda.is_available() else 'cpu'\n",
    "elif args.device=='7':\n",
    "    device = 'cuda:7' if torch.cuda.is_available() else 'cpu'\n",
    "elif args.device=='8':\n",
    "    device = 'cuda:8' if torch.cuda.is_available() else 'cpu'\n",
    "else:\n",
    "    device = 'None'\n",
    "print('Using {} device...'.format(device))        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12666062-2d94-4ba9-bec2-bfa4ce06130f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "Appling SimCLR image transforms...\n",
      "Number of CXR images= 377110\n",
      "Number of ICU stays= 59372\n",
      "Number of CXR associated with ICU stay based on subject ID= 368350\n",
      "Number of unique CXR dicoms= 181195\n",
      "Number of unique CXR study id= 122087\n",
      "Mean time cxr - intime=  68.3979428307123\n",
      "Minimum time = 0.009\n",
      "Maximum time = 2368.942\n",
      "Excluding CXR with missing radiology reports =  7756\n",
      "7756\n",
      "882\n",
      "2166\n"
     ]
    }
   ],
   "source": [
    "# Load datasets and initiate dataloaders\n",
    "importlib.reload(datasets.fusion)\n",
    "print('Loading datasets...')\n",
    "discretizer, normalizer = ehr_funcs(args)\n",
    "ehr_train_ds, ehr_val_ds, ehr_test_ds = get_datasets(discretizer, normalizer, args)\n",
    "cxr_train_ds, cxr_val_ds, cxr_test_ds = get_cxr_datasets(args)\n",
    "train_dl, val_dl, test_dl = load_cxr_ehr(args, ehr_train_ds, ehr_val_ds, cxr_train_ds, cxr_val_ds, ehr_test_ds, cxr_test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3fea0f9e-1fc5-4c4d-a2f3-73b8d40ccfa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Storing arguments...\n",
      "  device                                  : $CUDA_VISIBLE_DEVICES\n",
      "  num_gpu                                 : 1\n",
      "  epochs                                  : 2\n",
      "  lr                                      : 0.8\n",
      "  save_dir                                : /scratch/se1525/mml-ssl/checkpoints/phenotyping/models\n",
      "  labels_set                              : pheno\n",
      "  task                                    : phenotyping\n",
      "  data_pairs                              : paired\n",
      "  mode                                    : train\n",
      "  tag                                     : simclr_train\n",
      "  pretrain_type                           : simclr\n",
      "  file_name                               : SIMCLR-${SLURM_JOBID}\n",
      "  load_state                              : None\n",
      "  eval_set                                : val\n",
      "  job_number                              : ${SLURM_JOBID}\n",
      "  eval_epoch                              : 0\n",
      "  load_state_ehr                          : None\n",
      "  num_classes                             : 25\n",
      "  rec_dropout                             : 0.0\n",
      "  timestep                                : 1.0\n",
      "  imputation                              : previous\n",
      "  ehr_data_root                           : /scratch/fs999/shamoutlab/data/mimic-iv-extracted\n",
      "  layers                                  : 1\n",
      "  dim                                     : 256\n",
      "  load_state_cxr                          : None\n",
      "  cxr_data_root                           : /scratch/fs999/shamoutlab/data/physionet.org/files/mimic-cxr-jpg/2.0.0\n",
      "  vision_backbone                         : resnet34\n",
      "  pretrained                              : False\n",
      "  layer_after                             : 4\n",
      "  vision_num_classes                      : 14\n",
      "  resize                                  : 256\n",
      "  crop                                    : 224\n",
      "  dropout                                 : 0.0\n",
      "  hidden_dim                              : 128\n",
      "  load_state_simclr                       : None\n",
      "  batch_size                              : 30\n",
      "  transforms_cxr                          : simclrv2\n",
      "  temperature                             : 0.01\n",
      "  weight_decay                            : 0.0001\n",
      "  finetune                                : False\n",
      "  dataset                                 : evaluation_task\n",
      "  width                                   : 1\n",
      "  save_features                           : False\n",
      "  beta_infonce                            : False\n",
      "  linearclassify                          : False\n",
      "  load_state_lc                           : None\n",
      "  lr_linearclassify                       : 0.0001\n",
      "  epochs_linearclassify                   : 100\n",
      "  overwrite_classifier                    : False\n",
      "  fusion_type                             : None\n",
      "  data_ratio                              : 1.0\n",
      "  mmtm_ratio                              : 4\n",
      "  fusion_layer                            : 0\n",
      "  beta_1                                  : 0.9\n",
      "  normalizer_state                        : None\n",
      "  sim_coeff                               : 25\n",
      "  std_coeff                               : 25\n",
      "  cov_coeff                               : 1\n",
      "  vicreg                                  : False\n"
     ]
    }
   ],
   "source": [
    "# Store arguments after loading datasets\n",
    "# line added by shaza\n",
    "os.makedirs(os.path.dirname(f\"{args.save_dir}/args/args_{job_number}.txt\"), exist_ok=True)\n",
    "with open(f\"{args.save_dir}/args/args_{job_number}.txt\", 'w') as results_file:\n",
    "    print(\"Storing arguments...\")\n",
    "    for arg in vars(args): \n",
    "        print(f\"  {arg:<40}: {getattr(args, arg)}\") \n",
    "        results_file.write(f\"  {arg:<40}: {getattr(args, arg)}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4cfe785e-3aca-4492-9a7c-730174b37929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/shaza-workspace/mml-ssl/e/MMLSSL-433\n",
      "Remember to stop your run once you’ve finished logging your metadata (https://docs.neptune.ai/api-reference/run#.stop). It will be stopped automatically only when the notebook kernel/interactive console is terminated.\n"
     ]
    }
   ],
   "source": [
    "# Initiate logger\n",
    "neptune_logger = initiate_logger([args.tag, args.job_number])  \n",
    "neptune_logger.experiment[\"args\"] = vars(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae52a9b0-48e0-4d58-8847-d9f81edd00cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: string series 'monitoring/stdout' value was longer than 1000 characters and was truncated. This warning is printed only once per series.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identity()\n",
      "Printing model architecture...\n",
      "SimCLR(\n",
      "  (model): Fusion(\n",
      "    (cxr_model): CXRModels(\n",
      "      (vision_backbone): ResNet(\n",
      "        (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "        (layer1): Sequential(\n",
      "          (0): BasicBlock(\n",
      "            (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (1): BasicBlock(\n",
      "            (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (2): BasicBlock(\n",
      "            (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (layer2): Sequential(\n",
      "          (0): BasicBlock(\n",
      "            (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (downsample): Sequential(\n",
      "              (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "              (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (1): BasicBlock(\n",
      "            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (2): BasicBlock(\n",
      "            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (3): BasicBlock(\n",
      "            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (layer3): Sequential(\n",
      "          (0): BasicBlock(\n",
      "            (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (downsample): Sequential(\n",
      "              (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "              (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (1): BasicBlock(\n",
      "            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (2): BasicBlock(\n",
      "            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (3): BasicBlock(\n",
      "            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (4): BasicBlock(\n",
      "            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (5): BasicBlock(\n",
      "            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (layer4): Sequential(\n",
      "          (0): BasicBlock(\n",
      "            (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (downsample): Sequential(\n",
      "              (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "              (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (1): BasicBlock(\n",
      "            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (2): BasicBlock(\n",
      "            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (avgpool): AvgPool2d(kernel_size=7, stride=1, padding=0)\n",
      "        (fc): Identity()\n",
      "      )\n",
      "      (bce_loss): BCELoss()\n",
      "    )\n",
      "    (cxr_model_g): Sequential(\n",
      "      (0): Identity()\n",
      "      (1): Linear(in_features=512, out_features=128, bias=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (4): ReLU(inplace=True)\n",
      "      (5): Linear(in_features=128, out_features=128, bias=False)\n",
      "    )\n",
      "    (ehr_model): LSTM(\n",
      "      (layer0): LSTM(76, 128, batch_first=True)\n",
      "      (do): Dropout(p=0.0, inplace=False)\n",
      "      (dense_layer): Identity()\n",
      "    )\n",
      "    (ehr_model_g): Sequential(\n",
      "      (0): Identity()\n",
      "      (1): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (4): ReLU(inplace=True)\n",
      "      (5): Linear(in_features=128, out_features=128, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Load the model, weights (if any), and freeze layers (if any)\n",
    "print(\"Loading model...\")\n",
    "if args.pretrain_type == 'simclr':\n",
    "    model = SimCLR(args, train_dl)\n",
    "print('Printing model architecture...')\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "226bc190-7ed4-4e50-9ad0-548a8b913ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 382, 76)\n",
      "30\n",
      "[True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'device'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-9e0f9dbb4eaf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m \u001b[0mvision_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mehr_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mehr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Output Shape of vision_backbone\"\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mvision_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/mml-ssl/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-972cebf09015>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, seq_lengths)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_padded_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menforce_sorted\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m              \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mht\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'layer{layer}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/mml-ssl/lib/python3.6/site-packages/torch/nn/utils/rnn.py\u001b[0m in \u001b[0;36mpack_padded_sequence\u001b[0;34m(input, lengths, batch_first, enforce_sorted)\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0mlengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m         \u001b[0msorted_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m         \u001b[0mbatch_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mbatch_first\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_select\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'device'"
     ]
    }
   ],
   "source": [
    "# # EHR input \n",
    "# ehr_input = torch.randn(30, 76, 128, 512)\n",
    "# # Vision input \n",
    "# vision_input = torch.randn(30, 3, 512, 615)\n",
    "\n",
    "# print(model.model.cxr_model.feats_dim)\n",
    "# print(model)\n",
    "\n",
    "# data_iter = iter(train_dl)\n",
    "# batch = next(data_iter)\n",
    "# ehr, imgs, y_ehr, y_cxr, seq_lengths, pairs = batch\n",
    "\n",
    "# print(ehr.shape, len(seq_lengths), imgs.shape)\n",
    "\n",
    "# vision_output = model.model.cxr_model.vision_backbone(imgs)\n",
    "# print(vision_output.shape)\n",
    "\n",
    "# print(model.model.cxr_model.vision_backbone)\n",
    "\n",
    "# Print vision backbone arch \n",
    "# print(model.model.cxr_model.vision_backbone)\n",
    " # Pass through vision backbone\n",
    "# imgs = imgs.to(\"cuda:0\")\n",
    "# vision_output = model.model.cxr_model.vision_backbone(imgs)\n",
    "# print(\"Output Shape of vision_backbone\" , vision_output.shape)\n",
    "\n",
    "# print(\" ---------------------- \")\n",
    "# # Pass through the layers one by one and examine the output\n",
    "# input_data = imgs\n",
    "# layer_outputs = []\n",
    "# for name, layer in model.model.cxr_model.vision_backbone.named_children():\n",
    "#     input_data = layer(input_data)\n",
    "#     layer_outputs.append((name, input_data.clone()))\n",
    "\n",
    "# for name, output in layer_outputs:\n",
    "#     print(f\"Layer: {name}, Output Shape: {output.shape}\")\n",
    "    \n",
    "    \n",
    "#  # dim 512 x 128\n",
    "#  # self.cxr_model.vision_backbone.fc: Identity() \n",
    "#  # nn.Linear(512, w*hidden_dim, bias=True)\n",
    "# cxr_proj = model.model.cxr_model_g(output)\n",
    "\n",
    "    \n",
    "\n",
    "# ehr = torch.from_numpy(ehr).float().to(\"cuda:0\")\n",
    "# imgs = imgs.to(\"cuda:0\")\n",
    "# feats_img = model.model.cxr_model(imgs)\n",
    "# print(feats_img.shape)\n",
    "# feats_img = model.model.cxr_model_g(feats_img)\n",
    "# print(feats_img.shape)\n",
    "# feats_ehr = model.model.ehr_model(ehr, seq_lengths)\n",
    "# print(feats_ehr.shape)\n",
    "# feats_ehr = model.model.ehr_model_g(feats_ehr)\n",
    "# print(feats_ehr.shape)\n",
    "\n",
    "# print(feats_ehr.shape, feats_img.shape)\n",
    "\n",
    "# # # Pass through EHR model\n",
    "# ehr = torch.from_numpy(ehr)\n",
    "# ehr = ehr.float()\n",
    "# ehr_output = model.model.ehr_model(ehr, seq_lengths)\n",
    "# final_output = model.model.ehr_model_g(ehr_output)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Pass through CXR model\n",
    "# cxr_output = model.model.cxr_model(imgs)\n",
    "# print(cxr_output.shape)\n",
    "\n",
    "# ehr (30, 628, 76) seq_lengths 30 imgs torch.Size([30, 3, 256, 256])\n",
    "\n",
    "data_iter = iter(train_dl)\n",
    "batch = next(data_iter)\n",
    "ehr, imgs, y_ehr, y_cxr, seq_lengths, pairs = batch\n",
    "print(ehr.shape)\n",
    "print(len(seq_lengths))\n",
    "print(pairs)\n",
    "test = torch.randn(30,3,224,224)\n",
    "\n",
    "vision_output = model.model.ehr_model(ehr, seq_lengths)\n",
    "print(\"Output Shape of vision_backbone\" , vision_output.shape)\n",
    "\n",
    "print(\" ---------------------- \")\n",
    "# Pass through the layers one by one and examine the output\n",
    "input_data = imgs\n",
    "layer_outputs = []\n",
    "for name, layer in model.model.ehr_model.named_children():\n",
    "    input_data = layer(input_data)\n",
    "    layer_outputs.append((name, input_data.clone()))\n",
    "\n",
    "for name, output in layer_outputs:\n",
    "    print(f\"Layer: {name}, Output Shape: {output.shape}\")\n",
    "    \n",
    "    \n",
    " # self.cxr_model.vision_backbone.fc: Identity() \n",
    " # nn.Linear(512, w*hidden_dim, bias=True) :  512 x 128\n",
    "# cxr_proj = model.model.cxr_model_g(output)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bc40858b-4c78-48bc-b46a-0e15bbab6830",
   "metadata": {},
   "source": [
    "# from fusion_models import Fusion\n",
    "\n",
    "# outputs_at_each_layer = []\n",
    "# for layer_name, layer in model.named_children():\n",
    "#     hooks = layer.register_forward_hook(lambda module, input, output: outputs_at_each_layer.append(output))\n",
    "    \n",
    "# data_iter = iter(train_dl)\n",
    "# batch = next(data_iter)\n",
    "# ehr, imgs, y_ehr, y_cxr, seq_lengths, pairs = batch\n",
    "\n",
    "# input_tensor = torch.tensor(imgs)\n",
    "# ehr = torch.from_numpy(ehr).float()\n",
    "\n",
    "# model_fusion = Fusion (args)\n",
    "                \n",
    "# # Forward pass\n",
    "# with torch.no_grad():\n",
    "#     model_fusion.eval()\n",
    "#     model_fusion(ehr, seq_lengths, imgs)\n",
    "\n",
    "# # # Remove hooks\n",
    "# # for hook in hooks:\n",
    "# #     hook.remove()\n",
    "\n",
    "# # 'outputs_at_each_layer' contains the outputs at each layer of the model\n",
    "# for layer_output in outputs_at_each_layer:\n",
    "#     print(layer_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "08dc38ec-4e85-4054-a48f-6d98cc47633c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> training\n",
      "258\n"
     ]
    }
   ],
   "source": [
    "print('==> training')        \n",
    "filename = args.file_name+'_epoch_{epoch:02d}'\n",
    "    \n",
    "model_path = args.save_dir+'/'+args.file_name\n",
    "if not os.path.exists(model_path):\n",
    "        os.makedirs(model_path)\n",
    "        \n",
    "logger = neptune_logger\n",
    "checkpoints = ModelCheckpoint(dirpath=model_path,\n",
    "                                  filename=filename,\n",
    "                                  save_weights_only=True, \n",
    "                                  save_top_k=-1,\n",
    "                                  auto_insert_metric_name=False, \n",
    "                                  every_n_epochs=1,             \n",
    "                                  save_on_train_epoch_end=True)\n",
    "\n",
    "strategy = None\n",
    "trainer = pl.Trainer(default_root_dir=os.path.join(model_path),\n",
    "                             max_epochs=args.epochs, #gpus=gpu,\n",
    "                             callbacks=[checkpoints, LearningRateMonitor('epoch')],\n",
    "                             logger=logger,  \n",
    "                             log_every_n_steps=5,  enable_progress_bar=True,\n",
    "                             num_sanity_val_steps=0,\n",
    "                            accelerator='gpu', devices=args.num_gpu, strategy=strategy)\n",
    "\n",
    "train_loader=train_dl\n",
    "val_loader=val_dl\n",
    "num_batches_train = len(train_loader)\n",
    "print(num_batches_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a21919fb-3c19-4eb5-a9c5-98f5a00fea1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Set SLURM handle signals.\n",
      "\n",
      "  | Name  | Type   | Params\n",
      "---------------------------------\n",
      "0 | model | Fusion | 21.5 M\n",
      "---------------------------------\n",
      "21.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "21.5 M    Total params\n",
      "86.152    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaf54aa5b2064b558ef64e3a2e05e71c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ehr (30, 959, 76) seq_lengths 30 imgs torch.Size([30, 3, 224, 224])\n",
      "cuda:0\n",
      "ehr (30, 745, 76) seq_lengths 30 imgs torch.Size([30, 3, 224, 224])\n",
      "cuda:0\n",
      "ehr (30, 555, 76) seq_lengths 30 imgs torch.Size([30, 3, 224, 224])\n",
      "cuda:0\n",
      "ehr (30, 425, 76) seq_lengths 30 imgs torch.Size([30, 3, 224, 224])\n",
      "cuda:0\n",
      "ehr (30, 648, 76) seq_lengths 30 imgs torch.Size([30, 3, 224, 224])\n",
      "cuda:0\n",
      "ehr (30, 539, 76) seq_lengths 30 imgs torch.Size([30, 3, 224, 224])\n",
      "cuda:0\n",
      "ehr (30, 406, 76) seq_lengths 30 imgs torch.Size([30, 3, 224, 224])\n",
      "cuda:0\n",
      "ehr (30, 557, 76) seq_lengths 30 imgs torch.Size([30, 3, 224, 224])\n",
      "cuda:0\n",
      "ehr (30, 331, 76) seq_lengths 30 imgs torch.Size([30, 3, 224, 224])\n",
      "cuda:0\n",
      "ehr (30, 577, 76) seq_lengths 30 imgs torch.Size([30, 3, 224, 224])\n",
      "cuda:0\n",
      "ehr (30, 279, 76) seq_lengths 30 imgs torch.Size([30, 3, 224, 224])\n",
      "cuda:0\n",
      "ehr (30, 512, 76) seq_lengths 30 imgs torch.Size([30, 3, 224, 224])\n",
      "cuda:0\n",
      "ehr (30, 302, 76) seq_lengths 30 imgs torch.Size([30, 3, 224, 224])\n",
      "cuda:0\n",
      "ehr (30, 359, 76) seq_lengths 30 imgs torch.Size([30, 3, 224, 224])\n",
      "cuda:0\n",
      "ehr (30, 736, 76) seq_lengths 30 imgs torch.Size([30, 3, 224, 224])\n",
      "cuda:0\n",
      "ehr (30, 400, 76) seq_lengths 30 imgs torch.Size([30, 3, 224, 224])\n",
      "cuda:0\n",
      "ehr (30, 434, 76) seq_lengths 30 imgs torch.Size([30, 3, 224, 224])\n",
      "cuda:0\n",
      "ehr (30, 378, 76) seq_lengths 30 imgs torch.Size([30, 3, 224, 224])\n",
      "cuda:0\n",
      "ehr (30, 549, 76) seq_lengths 30 imgs torch.Size([30, 3, 224, 224])\n",
      "cuda:0\n",
      "ehr (30, 386, 76) seq_lengths 30 imgs torch.Size([30, 3, 224, 224])\n",
      "cuda:0\n",
      "ehr (30, 442, 76) seq_lengths 30 imgs torch.Size([30, 3, 224, 224])\n",
      "cuda:0\n",
      "ehr (30, 1081, 76) seq_lengths 30 imgs torch.Size([30, 3, 224, 224])\n",
      "cuda:0\n",
      "ehr (30, 518, 76) seq_lengths 30 imgs torch.Size([30, 3, 224, 224])\n",
      "cuda:0\n",
      "ehr (30, 540, 76) seq_lengths 30 imgs torch.Size([30, 3, 224, 224])\n",
      "cuda:0\n",
      "ehr (30, 333, 76) seq_lengths 30 imgs torch.Size([30, 3, 224, 224])\n",
      "cuda:0\n",
      "ehr (30, 1195, 76) seq_lengths 30 imgs torch.Size([30, 3, 224, 224])\n",
      "cuda:0\n",
      "ehr (30, 583, 76) seq_lengths 30 imgs torch.Size([30, 3, 224, 224])\n",
      "cuda:0\n",
      "ehr (30, 548, 76) seq_lengths 30 imgs torch.Size([30, 3, 224, 224])\n",
      "cuda:0\n",
      "ehr (30, 1390, 76) seq_lengths 30 imgs torch.Size([30, 3, 224, 224])\n",
      "cuda:0\n",
      "ehr (30, 342, 76) seq_lengths 30 imgs torch.Size([30, 3, 224, 224])\n",
      "cuda:0\n",
      "ehr (30, 416, 76) seq_lengths 30 imgs torch.Size([30, 3, 224, 224])\n",
      "cuda:0\n",
      "ehr (30, 329, 76) seq_lengths 30 imgs torch.Size([30, 3, 224, 224])\n",
      "cuda:0\n",
      "ehr (30, 279, 76) seq_lengths 30 imgs torch.Size([30, 3, 224, 224])\n",
      "cuda:0\n",
      "ehr (30, 525, 76) seq_lengths 30 imgs torch.Size([30, 3, 224, 224])\n",
      "cuda:0\n",
      "ehr (30, 946, 76) seq_lengths 30 imgs torch.Size([30, 3, 224, 224])\n",
      "cuda:0\n",
      "ehr (30, 378, 76) seq_lengths 30 imgs torch.Size([30, 3, 224, 224])\n",
      "cuda:0\n",
      "ehr (30, 321, 76) seq_lengths 30 imgs torch.Size([30, 3, 224, 224])\n",
      "cuda:0\n",
      "ehr (30, 607, 76) seq_lengths 30 imgs torch.Size([30, 3, 224, 224])\n",
      "cuda:0\n",
      "ehr (30, 148, 76) seq_lengths 30 imgs torch.Size([30, 3, 224, 224])\n",
      "cuda:0\n",
      "ehr (30, 359, 76) seq_lengths 30 imgs torch.Size([30, 3, 224, 224])\n",
      "cuda:0\n",
      "ehr (30, 516, 76) seq_lengths 30 imgs torch.Size([30, 3, 224, 224])\n",
      "cuda:0\n",
      "ehr (30, 503, 76) seq_lengths 30 imgs torch.Size([30, 3, 224, 224])\n",
      "cuda:0\n",
      "ehr (30, 1395, 76) seq_lengths 30 imgs torch.Size([30, 3, 224, 224])\n",
      "cuda:0\n",
      "ehr (30, 568, 76) seq_lengths 30 imgs torch.Size([30, 3, 224, 224])\n",
      "cuda:0\n",
      "ehr (30, 509, 76) seq_lengths 30 imgs torch.Size([30, 3, 224, 224])\n",
      "cuda:0\n",
      "ehr (30, 493, 76) seq_lengths 30 imgs torch.Size([30, 3, 224, 224])\n",
      "cuda:0\n",
      "ehr (30, 172, 76) seq_lengths 30 imgs torch.Size([30, 3, 224, 224])\n",
      "cuda:0\n",
      "ehr (30, 572, 76) seq_lengths 30 imgs torch.Size([30, 3, 224, 224])\n",
      "cuda:0\n",
      "ehr (30, 390, 76) seq_lengths 30 imgs torch.Size([30, 3, 224, 224])\n",
      "cuda:0\n",
      "ehr (30, 498, 76) seq_lengths 30 imgs torch.Size([30, 3, 224, 224])\n",
      "cuda:0\n",
      "ehr (30, 720, 76) seq_lengths 30 imgs torch.Size([30, 3, 224, 224])\n",
      "cuda:0\n",
      "ehr (30, 523, 76) seq_lengths 30 imgs torch.Size([30, 3, 224, 224])\n",
      "cuda:0\n",
      "ehr (30, 345, 76) seq_lengths 30 imgs torch.Size([30, 3, 224, 224])\n",
      "cuda:0\n",
      "ehr (30, 1086, 76) seq_lengths 30 imgs torch.Size([30, 3, 224, 224])\n",
      "cuda:0\n",
      "ehr (30, 461, 76) seq_lengths 30 imgs torch.Size([30, 3, 224, 224])\n",
      "cuda:0\n",
      "ehr (30, 696, 76) seq_lengths 30 imgs torch.Size([30, 3, 224, 224])\n",
      "cuda:0\n",
      "ehr (30, 738, 76) seq_lengths 30 imgs torch.Size([30, 3, 224, 224])\n",
      "cuda:0\n",
      "ehr (30, 601, 76) seq_lengths 30 imgs torch.Size([30, 3, 224, 224])\n",
      "cuda:0\n",
      "ehr (30, 462, 76) seq_lengths 30 imgs torch.Size([30, 3, 224, 224])\n",
      "cuda:0\n",
      "ehr (30, 577, 76) seq_lengths 30 imgs torch.Size([30, 3, 224, 224])\n",
      "cuda:0\n",
      "ehr (30, 352, 76) seq_lengths 30 imgs torch.Size([30, 3, 224, 224])\n",
      "cuda:0\n",
      "ehr (30, 418, 76) seq_lengths 30 imgs torch.Size([30, 3, 224, 224])\n",
      "cuda:0\n",
      "ehr (30, 861, 76) seq_lengths 30 imgs torch.Size([30, 3, 224, 224])\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "783d4094-1474-4f32-916e-c6667a5ffb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # For efficiency, change data loaders for lineareval (not finetune)\n",
    "# if ('lineareval' in args.fusion_type) & (not args.finetune):\n",
    "#     print(\"Processing features for linear evaluation...\")\n",
    "#     train_dl = prepare_data_features(device, model, train_dl, args.batch_size, args.fusion_layer, args.fusion_type) \n",
    "#     val_dl = prepare_data_features(device, model, val_dl, args.batch_size, args.fusion_layer, args.fusion_type)\n",
    "#     test_dl = prepare_data_features(device, model, test_dl, args.batch_size, args.fusion_layer, args.fusion_type)\n",
    "\n",
    "# if args.mode == 'train':\n",
    "#     print('==> training')        \n",
    "#     print(len(train_dl))\n",
    "#     train(model, args, train_dl, val_dl,\n",
    "#           logger=neptune_logger,\n",
    "#           load_state_prefix=args.load_state_simclr)\n",
    "\n",
    "# elif args.mode == 'eval':\n",
    "#     print('==> evaluating on the '+args.eval_set)\n",
    "#     if args.eval_set=='val':\n",
    "#         test_dl=val_dl\n",
    "#     elif args.eval_set=='train':\n",
    "#         test_dl=train_dl\n",
    "#     print(len(test_dl))\n",
    "#     test(model, args, test_dl, logger=neptune_logger)\n",
    "#     #trainer.eval()\n",
    "\n",
    "# else:\n",
    "#     raise ValueError(\"Incorrect value for args.mode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac6f10c-64fb-490c-ba3f-5d071e11bab9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MML SSL",
   "language": "python",
   "name": "mml-ssl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
